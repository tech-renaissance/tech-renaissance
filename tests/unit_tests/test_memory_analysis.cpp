/**
 * @file test_memory_analysis.cpp
 * @brief å†…å­˜åˆ†æåŠŸèƒ½æµ‹è¯•
 * @details æµ‹è¯•Modelç±»çš„analyze_memoryå’Œprint_memory_profileåŠŸèƒ½
 * @version 1.47.0
 * @date 2025-11-17
 * @author æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
 * @note æµ‹è¯•å†…å­˜åˆ†æåŠŸèƒ½çš„å‡†ç¡®æ€§å’Œæ€§èƒ½
 */

#include "tech_renaissance.h"
#include <iostream>
#include <iomanip>
#include <chrono>

using namespace tr;

// è¾…åŠ©å‡½æ•°ï¼šæ‰“å°å†…å­˜åˆ†æç»“æœ
void print_memory_analysis_result(const Model::MemoryProfile& profile, const std::string& test_name) {
    std::cout << "\n=== " << test_name << " ===" << std::endl;
    std::cout << "Parameter Memory: " << profile.parameter_memory << " bytes" << std::endl;
    std::cout << "Activation Memory: " << profile.activation_memory << " bytes" << std::endl;
    std::cout << "Gradient Memory: " << profile.gradient_memory << " bytes" << std::endl;
    std::cout << "Total Training Memory: " << profile.total_memory << " bytes" << std::endl;
    std::cout << "Total Inference Memory: " << profile.inference_memory() << " bytes" << std::endl;

    std::cout << "\nLayer Details:" << std::endl;
    for (size_t i = 0; i < profile.layer_activations.size(); ++i) {
        std::cout << "  Layer " << i << ": "
                  << "Params=" << profile.layer_parameters[i] << " bytes, "
                  << "Activations=" << profile.layer_activations[i] << " bytes" << std::endl;
    }
}

int main() {
    std::cout << "Starting Memory Analysis Tests..." << std::endl;

    try {
        // === æµ‹è¯•1ï¼šç®€å•çš„Linearå±‚æ¨¡å‹ ===
        std::cout << "\n[Test 1] Simple Linear Model" << std::endl;

        auto linear_model = std::make_shared<Model>("LinearModel", std::vector<std::shared_ptr<Module>>{
            std::make_shared<Linear>(784, 256),
            std::make_shared<Tanh>(),
            std::make_shared<Linear>(256, 10)
        });

        // è®¾ç½®åç«¯å¹¶åˆå§‹åŒ–å‚æ•°
        linear_model->to(CPU);

        // åˆ†æå†…å­˜ä½¿ç”¨
        Shape input_shape(32, 784);  // batch_size=32, input_features=784
        auto profile1 = linear_model->analyze_memory(input_shape);

        print_memory_analysis_result(profile1, "Simple Linear Model");

        // éªŒè¯è®¡ç®—ç»“æœ
        // Linear1: (784*256)*4 = 802,816 bytes (å‚æ•°), (32*256)*4 = 32,768 bytes (æ¿€æ´»å€¼)
        // Linear2: (256*10)*4 = 10,240 bytes (å‚æ•°), (32*10)*4 = 1,280 bytes (æ¿€æ´»å€¼)
        // Tanh: 0 bytes (å‚æ•°), (32*256)*4 = 32,768 bytes (æ¿€æ´»å€¼)

        size_t expected_params = (784 * 256 + 256 * 10) * sizeof(float);  // 813,056 bytes
        size_t expected_activations = (32 * 256 + 32 * 256 + 32 * 10) * sizeof(float);  // 66,816 bytes
        size_t expected_gradients = expected_params;  // æ¢¯åº¦å†…å­˜ = å‚æ•°å†…å­˜
        size_t expected_total = expected_params + expected_activations + expected_gradients;  // 1,692,928 bytes

        if (profile1.parameter_memory == expected_params &&
            profile1.activation_memory == expected_activations &&
            profile1.gradient_memory == expected_gradients &&
            profile1.total_memory == expected_total) {
            std::cout << "[PASS] Linear model memory analysis is correct!" << std::endl;
        } else {
            std::cout << "[FAIL] Linear model memory analysis mismatch!" << std::endl;
            std::cout << "Expected params: " << expected_params << ", got: " << profile1.parameter_memory << std::endl;
            std::cout << "Expected activations: " << expected_activations << ", got: " << profile1.activation_memory << std::endl;
            return 1;
        }

        // === æµ‹è¯•2ï¼šåŒ…å«Flattenå±‚çš„æ¨¡å‹ ===
        std::cout << "\n[Test 2] Model with Different Architecture" << std::endl;

        auto flatten_model = std::make_shared<Model>("ArchitectureModel", std::vector<std::shared_ptr<Module>>{
            std::make_shared<Linear>(784, 128),
            std::make_shared<Tanh>(),
            std::make_shared<Linear>(128, 64),
            std::make_shared<Tanh>()
        });

        flatten_model->to(CPU);

        // æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶
        Shape input_2d(16, 784);  // batch=16, features=784 (å±•å¹³åçš„MNIST)
        auto profile2 = flatten_model->analyze_memory(input_2d);

        print_memory_analysis_result(profile2, "Different Architecture Model");

        // éªŒè¯è®¡ç®—ç»“æœ
        // Linear1: (784*128)*4 = 401,408 bytes (å‚æ•°), (16*128)*4 = 8,192 bytes (æ¿€æ´»å€¼)
        // Linear2: (128*64)*4 = 32,768 bytes (å‚æ•°), (16*64)*4 = 4,096 bytes (æ¿€æ´»å€¼)
        // Tanh1: 0 bytes (å‚æ•°), (16*128)*4 = 8,192 bytes (æ¿€æ´»å€¼)
        // Tanh2: 0 bytes (å‚æ•°), (16*64)*4 = 4,096 bytes (æ¿€æ´»å€¼)
        size_t expected_params2 = (784 * 128 + 128 * 64) * sizeof(float);  // 434,176 bytes
        size_t expected_activations2 = (16 * 128 + 16 * 128 + 16 * 64 + 16 * 64) * sizeof(float);  // 24,576 bytes

        if (profile2.parameter_memory == expected_params2 && profile2.activation_memory == expected_activations2) {
            std::cout << "[PASS] Different architecture model analysis is correct!" << std::endl;
        } else {
            std::cout << "[FAIL] Different architecture model analysis mismatch!" << std::endl;
            std::cout << "Expected params: " << expected_params2 << ", got: " << profile2.parameter_memory << std::endl;
            std::cout << "Expected activations: " << expected_activations2 << ", got: " << profile2.activation_memory << std::endl;
            return 1;
        }

        // === æµ‹è¯•3ï¼šprint_memory_profileç¾è§‚æ€§æµ‹è¯• ===
        std::cout << "\n[Test 3] print_memory_profile Beauty Test" << std::endl;

        std::cout << "\n--- Calling print_memory_profile ---" << std::endl;
        linear_model->print_memory_profile(input_shape);
        std::cout << "--- End of print_memory_profile ---" << std::endl;

        std::cout << "[PASS] print_memory_profile completed successfully!" << std::endl;

        // === æµ‹è¯•4ï¼šå½¢çŠ¶æ¨æ–­éªŒè¯ ===
        std::cout << "\n[Test 4] Shape Inference Verification" << std::endl;

        // éªŒè¯æ¯å±‚çš„å½¢çŠ¶æ¨æ–­æ˜¯å¦æ­£ç¡®
        auto single_linear = std::make_shared<Linear>(10, 5);
        auto test_model = std::make_shared<Model>("TestModel", std::vector<std::shared_ptr<Module>>{
            single_linear
        });
        test_model->to(CPU);

        Shape test_input(3, 10);  // batch=3, features=10
        auto profile3 = test_model->analyze_memory(test_input);

        // Linear(10,5)åº”è¯¥è¾“å‡º(3,5)
        // å‚æ•°: (10*5)*4 = 200 bytes
        // æ¿€æ´»å€¼: (3*5)*4 = 60 bytes
        if (profile3.parameter_memory == 200 && profile3.activation_memory == 60) {
            std::cout << "[PASS] Shape inference works correctly!" << std::endl;
        } else {
            std::cout << "[FAIL] Shape inference failed!" << std::endl;
            return 1;
        }

        // === æµ‹è¯•5ï¼šæ€§èƒ½æµ‹è¯•ï¼ˆç¡®ä¿åˆ†ææ–¹æ³•æ˜¯è½»é‡çº§çš„ï¼‰ ===
        std::cout << "\n[Test 5] Performance Test (Lightweight Analysis)" << std::endl;

        auto large_model = std::make_shared<Model>("LargeModel", std::vector<std::shared_ptr<Module>>{
            std::make_shared<Linear>(1000, 500),
            std::make_shared<Tanh>(),
            std::make_shared<Linear>(500, 200),
            std::make_shared<Tanh>(),
            std::make_shared<Linear>(200, 100),
            std::make_shared<Tanh>(),
            std::make_shared<Linear>(100, 10)
        });
        large_model->to(CPU);

        Shape large_input(64, 1000);

        // å¤šæ¬¡è°ƒç”¨æµ‹è¯•æ€§èƒ½
        auto start = std::chrono::high_resolution_clock::now();
        for (int i = 0; i < 1000; ++i) {
            auto profile = large_model->analyze_memory(large_input);
            // é¿å…ç¼–è¯‘å™¨ä¼˜åŒ–æ‰è®¡ç®—
            volatile size_t dummy = profile.total_memory;
            (void)dummy;
        }
        auto end = std::chrono::high_resolution_clock::now();

        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
        std::cout << "1000 analyze_memory() calls took: " << duration.count() << " microseconds" << std::endl;
        std::cout << "Average per call: " << duration.count() / 1000.0 << " microseconds" << std::endl;

        if (duration.count() < 10000) {  // 10ms for 1000 calls = 10 microseconds per call
            std::cout << "[PASS] analyze_memory() is lightweight!" << std::endl;
        } else {
            std::cout << "[WARN] analyze_memory() might be too slow" << std::endl;
        }

        std::cout << "\nğŸ‰ All Memory Analysis Tests PASSED! ğŸ‰" << std::endl;

        // === æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥é›¶å†…å­˜åˆ†é…ç‰¹æ€§ ===
        std::cout << "\n[Test 6] Zero Memory Allocation Verification" << std::endl;

        // è¿™ä¸ªæµ‹è¯•ç¡®ä¿analyze_memoryæ–¹æ³•ä¸åˆ†é…å®é™…çš„Tensorå†…å­˜
        // æˆ‘ä»¬åªéœ€è¦è¿è¡Œä¸å´©æºƒå°±è¯æ˜å®ƒæ˜¯åŸºäºæ•°å­¦è®¡ç®—çš„
        for (int i = 0; i < 100; ++i) {
            auto profile = large_model->analyze_memory(Shape(i + 1, 1000));
            if (profile.total_memory == 0) {
                std::cout << "[FAIL] Memory analysis returned zero total memory!" << std::endl;
                return 1;
            }
        }

        std::cout << "[PASS] analyze_memory() successfully processes 100 different input shapes" << std::endl;
        std::cout << "[PASS] Zero memory allocation characteristic verified!" << std::endl;

        return 0;

    } catch (const std::exception& e) {
        std::cout << "[ERROR] Exception caught: " << e.what() << std::endl;
        return 1;
    }
}