/**
 * @file linear.h
 * @brief çº¿æ€§å±‚
 * @details å…¨è¿æ¥å±‚çš„å®ç°ï¼Œæ”¯æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
 * @version 1.45.0
 * @date 2025-11-17
 * @author æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
 * @note ä¾èµ–é¡¹: module.h
 * @note æ‰€å±ç³»åˆ—: model
 */

#pragma once

#include "tech_renaissance/model/module.h"
#include <cmath>

namespace tr {

class Linear : public Module {
private:
    int in_features_;
    int out_features_;
    bool use_bias_;

    // æƒé‡è½¬ç½®ç¼“å­˜
    mutable Tensor weight_transposed_;      // ç¼“å­˜çš„è½¬ç½®æƒé‡
    mutable bool weight_transposed_valid_ = false;
    mutable bool weight_dirty_ = false;     // âœ… æ–°å¢ï¼šæƒé‡è„æ ‡è®°

public:
    Linear(int in_features, int out_features, const std::string& name = "Linear", bool use_bias = false)
        : Module(name), in_features_(in_features), out_features_(out_features), use_bias_(use_bias) {}

    void set_backend(std::shared_ptr<Backend> backend) override {
        Module::set_backend(backend);

        // åˆ›å»ºå¹¶æ³¨å†Œæƒé‡å‚æ•°
        if (!has_parameter("weight")) {
            // æƒé‡ï¼šout_features Ã— in_features (PyTorchæ ‡å‡†æ ¼å¼)
            // ğŸ”§ å¯¹é½PyTorch Linearå±‚é»˜è®¤åˆå§‹åŒ–ï¼šKaiming Uniformåˆå§‹åŒ–
            // PyTorch Linearå±‚é»˜è®¤ä½¿ç”¨: uniform(-k, k) where k = sqrt(1/fan_in)
            float k = std::sqrt(1.0f / in_features_);  // Kaiming Uniformçš„è¾¹ç•Œå€¼
            Tensor weight = backend->uniform(Shape(out_features_, in_features_), -k, k);
            register_parameter("weight", weight);

            // âœ… å¯ç”¨æ¢¯åº¦ï¼šä¸ºæƒé‡å‚æ•°åˆ›å»ºæ¢¯åº¦å¼ é‡
            Tensor weight_grad = backend->zeros(weight.shape(), DType::FP32);
            weight.set_grad(weight_grad);
        }

        // åªæœ‰éœ€è¦æ—¶æ‰åˆ›å»ºåç½®å‚æ•°
        if (use_bias_ && !has_parameter("bias")) {
            // åç½®ï¼š(1, out_features) - 2Då½¢çŠ¶ä»¥ä¾¿äºå¹¿æ’­
            // ğŸ”§ å¯¹é½PyTorchåç½®åˆå§‹åŒ–ï¼šåŒæ ·ä½¿ç”¨uniform(-k, k) where k = sqrt(1/fan_in)
            float k = std::sqrt(1.0f / in_features_);  // ä¸æƒé‡ç›¸åŒçš„è¾¹ç•Œå€¼
            Tensor bias = backend->uniform(Shape(1, out_features_), -k, k);
            register_parameter("bias", bias);

            // âœ… å¯ç”¨æ¢¯åº¦ï¼šä¸ºåç½®å‚æ•°åˆ›å»ºæ¢¯åº¦å¼ é‡
            Tensor bias_grad = backend->zeros(bias.shape(), DType::FP32);
            bias.set_grad(bias_grad);
        }

        // åˆå§‹åŒ–è½¬ç½®ç¼“å­˜ï¼ˆåœ¨æƒé‡åˆ›å»ºä¹‹åï¼‰
        invalidate_weight_cache();
        weight_dirty_ = false;  // âœ… ç¡®ä¿åˆå§‹çŠ¶æ€æ­£ç¡®
    }

    // === æ ¸å¿ƒè®¡ç®—ï¼ˆintoå‹ï¼‰ ===
    void forward_into(const Tensor& input, Tensor& output) override {
        cache_input(input);

        auto backend = get_backend();

        // è·å–æƒé‡
        const Tensor& weight = get_parameter("weight");

        // âœ… åªåœ¨æƒé‡è¢«ä¿®æ”¹åæ‰é‡æ–°è½¬ç½®
        if (weight_dirty_) {
            invalidate_weight_cache();
            weight_dirty_ = false;
        }

        // ç¡®ä¿è½¬ç½®æƒé‡ç¼“å­˜æœ‰æ•ˆ
        if (!weight_transposed_valid_) {
            // é¢„è®¡ç®—å¹¶ç¼“å­˜è½¬ç½®æƒé‡ï¼šweight^T (in_features, out_features)
            weight_transposed_ = backend->transpose(weight);
            weight_transposed_valid_ = true;
        }

        // è®¡ç®—ï¼šoutput = input @ weight^T
        // Linearå±‚æƒé‡å­˜å‚¨ä¸ºï¼š(out_features, in_features) (PyTorchæ ‡å‡†æ ¼å¼)
        // ç¼“å­˜çš„è½¬ç½®æƒé‡ä¸ºï¼š(in_features, out_features)
        // çŸ©é˜µä¹˜æ³•ï¼šinput(batch, in_features) @ weight^T(in_features, out_features) = output(batch, out_features)
        // â­ ä½¿ç”¨ç¼“å­˜çš„è½¬ç½®æƒé‡ï¼Œé¿å…è¿è¡Œæ—¶è½¬ç½®å¼€é”€
        backend->mm_into(input, weight_transposed_, output);

        // å¦‚æœä½¿ç”¨åç½®ï¼Œè¿›è¡Œå¹¿æ’­åŠ æ³•
        if (use_bias_ && has_parameter("bias")) {
            const Tensor& bias = get_parameter("bias");
            // ä½¿ç”¨å¹¿æ’­åŠ æ³•ï¼šoutput += bias
            backend->add_broadcast_into(output, bias, output);
        }
    }

    void backward_into(const Tensor& grad_output, Tensor& grad_input) override {
        auto backend = get_backend();

        // è·å–æƒé‡
        Tensor& weight = get_parameter("weight");

        // è®¡ç®—è¾“å…¥æ¢¯åº¦ï¼šgrad_input = grad_output @ weight^T
        // grad_output(batch, out_features) @ weight^T(in_features, out_features)^T = grad_input(batch, in_features)
        backend->mm_into(grad_output, weight, grad_input);

        // è®¡ç®—æƒé‡æ¢¯åº¦ï¼šgrad_weight = grad_output^T @ input
        if (weight.has_grad()) {
            // â­ ä½¿ç”¨mm_into_transposedï¼Œé¿å…ä¸´æ—¶è½¬ç½®å¼ é‡
            // grad_output^T @ input = grad_weight (transpose_a=true)
            Shape grad_weight_shape(weight.shape());
            Tensor grad_weight = backend->zeros(grad_weight_shape, DType::FP32);
            backend->mm_into_transposed(grad_output, cached_input_, grad_weight, true, false);

            // ç´¯ç§¯æƒé‡æ¢¯åº¦
            if (!weight.grad().storage_allocated()) {
                weight.set_grad(grad_weight);
            } else {
                // å®ç°æ¢¯åº¦ç´¯ç§¯ï¼šnew_grad += old_grad
                Tensor& existing_grad = weight.grad();
                backend->add_into(grad_weight, existing_grad, existing_grad);
            }
        }

        // è®¡ç®—åç½®æ¢¯åº¦ï¼šgrad_bias = sum(grad_output, dim=0)
        if (use_bias_ && has_parameter("bias")) {
            Tensor& bias = get_parameter("bias");
            if (bias.has_grad()) {
                // å¯¹grad_outputçš„batchç»´åº¦æ±‚å’Œï¼šgrad_bias(out_features)
                Tensor grad_bias = backend->zeros(bias.shape(), DType::FP32);

                // ä½¿ç”¨sum_intoæ–¹æ³•å¯¹dim=0è¿›è¡Œæ±‚å’Œ
                backend->sum_into(grad_output, grad_bias, 0, false);

                // ç´¯ç§¯åç½®æ¢¯åº¦
                if (!bias.grad().storage_allocated()) {
                    bias.set_grad(grad_bias);
                } else {
                    // å®ç°æ¢¯åº¦ç´¯ç§¯ï¼šnew_grad += old_grad
                    Tensor& existing_grad = bias.grad();
                    backend->add_into(grad_bias, existing_grad, existing_grad);
                }
            }
        }

        clear_cache();

    weight_dirty_ = true;  // âœ… æ ‡è®°æƒé‡å°†è¢«æ›´æ–°ï¼Œè€Œéç«‹å³å¤±æ•ˆç¼“å­˜
    // ç§»é™¤ invalidate_weight_cache();
    }

protected:
    Shape infer_output_shape(const Shape& input_shape) const override {
        // è¾“å…¥: (batch, in_features) æˆ–å±•å¹³åçš„å…¶ä»–å½¢çŠ¶
        // è¾“å‡º: (batch, out_features)
        // å‡è®¾è¾“å…¥çš„æœ€åä¸€ç»´æ˜¯in_featuresï¼Œå…¶ä»–ç»´åº¦å±•å¹³ä¸ºbatch
        int64_t batch_size = input_shape.numel() / in_features_;
        return Shape(batch_size, out_features_);
    }

public:
    // === è®¾å¤‡è½¬ç§» ===
    void to(const Device& device) override {
        // è°ƒç”¨åŸºç±»æ–¹æ³•
        Module::to(device);

        // è®¾å¤‡è½¬ç§»åï¼Œè½¬ç½®ç¼“å­˜å¤±æ•ˆ
        invalidate_weight_cache();
    }

    // === ç¼“å­˜ç®¡ç† ===
    void invalidate_weight_cache() const {
        auto backend = get_backend();
        if (backend && has_parameter("weight")) {
            const Tensor& weight = get_parameter("weight");
            // é¢„åˆ†é…è½¬ç½®æƒé‡ç¼“å­˜
            weight_transposed_ = backend->zeros(Shape(in_features_, out_features_), weight.dtype());
        }
        weight_transposed_valid_ = false;
        weight_dirty_ = false;  // âœ… é‡ç½®è„æ ‡è®°
    }

    // === è®¿é—®å™¨æ–¹æ³• ===
    int in_features() const { return in_features_; }
    int out_features() const { return out_features_; }

    // === è°ƒè¯•è¾…åŠ©æ–¹æ³• ===
    void print_parameters() const {
        std::cout << "Linear Layer (" << instance_name() << "):" << std::endl;
        std::cout << "  Input features: " << in_features_ << std::endl;
        std::cout << "  Output features: " << out_features_ << std::endl;
        std::cout << "  Weight transposed cache: " << (weight_transposed_valid_ ? "VALID [OK]" : "INVALID [FAIL]") << std::endl;

        if (has_parameter("weight")) {
            const Tensor& weight = get_parameter("weight");
            std::cout << "  Weight shape: " << weight.shape().to_string() << " (PyTorch standard: out_features, in_features)" << std::endl;
        }

        if (has_parameter("bias")) {
            const Tensor& bias = get_parameter("bias");
            std::cout << "  Bias shape: " << bias.shape().to_string() << std::endl;
        }
    }
};

} // namespace tr