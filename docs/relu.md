# ReLUæ¿€æ´»å‡½æ•°å±‚

## æ¦‚è¿°

ReLUï¼ˆRectified Linear Unitï¼Œä¿®æ­£çº¿æ€§å•å…ƒï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨å’Œæœ€é‡è¦çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ã€‚å®ƒå…·æœ‰è®¡ç®—ç®€å•ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ç­‰ä¼˜ç‚¹ï¼Œå·²æˆä¸ºç°ä»£ç¥ç»ç½‘ç»œçš„æ ‡å‡†ç»„ä»¶ã€‚

## æ•°å­¦å®šä¹‰

ReLUå‡½æ•°çš„æ•°å­¦å®šä¹‰ä¸ºï¼š

```
ReLU(x) = max(0, x)
```

ReLUå¯¼æ•°çš„æ•°å­¦å®šä¹‰ä¸ºï¼š

```
dReLU(x) = {
    1,  if x > 0
    0,  if x <= 0
}
```

## ç±»å®šä¹‰

### å¤´æ–‡ä»¶ä½ç½®
```cpp
#include "tech_renaissance/model/relu.h"
```

### ç±»å£°æ˜
```cpp
class ReLU : public Module {
public:
    ReLU(const std::string& name = "ReLU");
    ~ReLU() = default;

    // æ ¸å¿ƒè®¡ç®—æ–¹æ³•
    void forward_into(const Tensor& input, Tensor& output) override;
    void backward_into(const Tensor& grad_output, Tensor& grad_input) override;

protected:
    Shape infer_output_shape(const Shape& input_shape) const override;
};
```

## æ ¸å¿ƒåŠŸèƒ½

### 1. å‰å‘ä¼ æ’­ï¼ˆforward_intoï¼‰
- å®ç°ReLUå‡½æ•°ï¼š`output = max(0, input)`
- è‡ªåŠ¨ç¼“å­˜è¾“å…¥æ•°æ®ç”¨äºåå‘ä¼ æ’­
- æ”¯æŒEigenå‘é‡åŒ–ä¼˜åŒ–

### 2. åå‘ä¼ æ’­ï¼ˆbackward_intoï¼‰
- ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼š`grad_input = grad_output * dReLU(cached_input)`
- å…¶ä¸­ `dReLU(x) = 1 if x > 0 else 0`
- è‡ªåŠ¨æ¸…ç†ç¼“å­˜æ•°æ®

### 3. å½¢çŠ¶æ¨æ–­
- ReLUå±‚ä¸æ”¹å˜å¼ é‡å½¢çŠ¶
- è¾“å…¥å½¢çŠ¶ = è¾“å‡ºå½¢çŠ¶

## æŠ€æœ¯ç‰¹æ€§

### ğŸš€ æ€§èƒ½ä¼˜åŒ–
- **Eigenå‘é‡åŒ–**ï¼šä½¿ç”¨Eigenåº“çš„SIMDæŒ‡ä»¤ä¼˜åŒ–
- **é›¶æ‹·è´è®¾è®¡**ï¼šintoå‹æ–¹æ³•é¿å…ä¸å¿…è¦çš„å†…å­˜åˆ†é…
- **å†…å­˜é«˜æ•ˆ**ï¼šåªç¼“å­˜å¿…è¦çš„è¾“å…¥æ•°æ®ç”¨äºæ¢¯åº¦è®¡ç®—

### ğŸ’¡ è®¾è®¡ä¼˜åŠ¿
- **æ— å‚æ•°è®¾è®¡**ï¼šReLUå±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°
- **æ•°å€¼ç¨³å®š**ï¼šé¿å…äº†sigmoid/tanhçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **ç¨€ç–æ¿€æ´»**ï¼šå¤©ç„¶äº§ç”Ÿç¨€ç–è¡¨ç¤ºï¼Œæœ‰åŠ©äºè®¡ç®—æ•ˆç‡

### ğŸ”§ ç±»å‹æ”¯æŒ
- **æ•°æ®ç±»å‹**ï¼šä»…æ”¯æŒFP32ç±»å‹å¼ é‡
- **è®¾å¤‡æ”¯æŒ**ï¼šCPUåç«¯ï¼ˆæœªæ¥å¯æ‰©å±•åˆ°GPUï¼‰
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸æ£€æŸ¥å’Œé”™è¯¯æç¤º

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨
```cpp
#include "tech_renaissance.h"

using namespace tr;

// åˆ›å»ºReLUå±‚
auto relu = std::make_shared<ReLU>("relu1");
relu->set_backend(BackendManager::get_cpu_backend());

// å‰å‘ä¼ æ’­
Tensor input = backend->uniform(Shape(10, 20), -1.0f, 1.0f);
Tensor output = relu->forward(input);

// åå‘ä¼ æ’­
Tensor grad_output = backend->ones(output.shape(), DType::FP32);
Tensor grad_input = relu->backward(grad_output);
```

### åœ¨MLPä¸­ä½¿ç”¨
```cpp
// æ„å»ºåŒ…å«ReLUçš„ç¥ç»ç½‘ç»œ
auto model = Model::create("MLP",
    std::make_shared<Flatten>(),
    std::make_shared<Linear>(784, 512),
    std::make_shared<ReLU>(),  // ReLUæ¿€æ´»å‡½æ•°
    std::make_shared<Linear>(512, 256),
    std::make_shared<ReLU>(),  // ç¬¬äºŒä¸ªReLU
    std::make_shared<Linear>(256, 10)
);
```

### è®­ç»ƒé›†æˆ
```cpp
// åœ¨Taskè®­ç»ƒä¸­ä½¿ç”¨ReLU
auto model = Model::create("MLP_with_ReLU",
    std::make_shared<Flatten>(),
    std::make_shared<Linear>(784, 512),
    std::make_shared<ReLU>(),
    std::make_shared<Linear>(512, 10)
);

auto trainer = Trainer(model, loss_fn, optimizer, scheduler);
auto task = Task(model, dataset, trainer);
task.run();  // è‡ªåŠ¨æ‰§è¡ŒåŒ…å«ReLUçš„å‰å‘å’Œåå‘ä¼ æ’­
```

## å®ç°ç»†èŠ‚

### Eigenä¼˜åŒ–ç‰ˆæœ¬
```cpp
// ReLUå‰å‘ä¼ æ’­
result_vec = input_vec.cwiseMax(0.0f);

// ReLUåå‘ä¼ æ’­
result_vec = (input_vec.array() > 0.0f).select(
    MatrixType::Ones(num_elements),
    MatrixType::Zero(num_elements)
);
```

### æœ´ç´ å®ç°ç‰ˆæœ¬
```cpp
// ReLUå‰å‘ä¼ æ’­
for (size_t i = 0; i < num_elements; ++i) {
    result_data[i] = (input_data[i] > 0.0f) ? input_data[i] : 0.0f;
}

// ReLUåå‘ä¼ æ’­
for (size_t i = 0; i < num_elements; ++i) {
    result_data[i] = (input_data[i] > 0.0f) ? 1.0f : 0.0f;
}
```

## ä¼˜åŠ¿ä¸ç‰¹ç‚¹

### âœ… ç›¸æ¯”å…¶ä»–æ¿€æ´»å‡½æ•°çš„ä¼˜åŠ¿

| ç‰¹æ€§ | ReLU | Sigmoid | Tanh |
|------|------|---------|------|
| **è®¡ç®—å¤æ‚åº¦** | O(1) | O(exp) | O(exp) |
| **æ¢¯åº¦æ¶ˆå¤±** | âŒ ä¸å­˜åœ¨ | âœ… ä¸¥é‡ | âœ… ä¸¥é‡ |
| **æ¢¯åº¦çˆ†ç‚¸** | âš ï¸ å¯èƒ½ | âŒ ä¸å­˜åœ¨ | âŒ ä¸å­˜åœ¨ |
| **ç¨€ç–æ€§** | âœ… å¤©ç„¶ç¨€ç– | âŒ å¯†é›† | âŒ å¯†é›† |
| **è¾“å‡ºèŒƒå›´** | [0, +âˆ) | [0, 1] | [-1, 1] |

### ğŸ¯ é€‚ç”¨åœºæ™¯
- **æ·±åº¦ç½‘ç»œ**ï¼šç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **è®¡ç®—æœºè§†è§‰**ï¼šCNNä¸­çš„æ ‡å‡†æ¿€æ´»å‡½æ•°
- **å¤§è§„æ¨¡æ¨¡å‹**ï¼šè®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²
- **å®æ—¶åº”ç”¨**ï¼šè®¡ç®—ç®€å•ï¼Œé€‚åˆæ¨ç†åŠ é€Ÿ

## ç‰ˆæœ¬ä¿¡æ¯
- **ç‰ˆæœ¬å·**: V1.45.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-11-25
- **ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
- **æ‰€å±ç³»åˆ—**: modelæ¨¡å—

## ç›¸å…³æ–‡æ¡£
- [Modelæ¨¡å—è®¾è®¡](model_trainer_system.md)
- [æ¿€æ´»å‡½æ•°å¯¹æ¯”](../examples/activation_comparison.cpp)
- [å¼ é‡åç«¯ç³»ç»Ÿ](tensor_backend_system.md)
- [ä»»åŠ¡è®­ç»ƒç³»ç»Ÿ](task_system.md)

---

**ReLU = ç®€å•é«˜æ•ˆ + æ¢¯åº¦å‹å¥½ + ç¨€ç–æ¿€æ´»ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ‡å‡†é€‰æ‹©ï¼** ğŸš€