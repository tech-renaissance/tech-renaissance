# Optimizer ä¼˜åŒ–å™¨åŸºç±»æŠ€æœ¯æ–‡æ¡£

**ç‰ˆæœ¬**: V1.52.0
**æ—¥æœŸ**: 2025å¹´11æœˆ19æ—¥
**ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ

---

## ğŸ†• V1.52.0 D4å®ç°æ¶æ„çªç ´

### ğŸ—ï¸ æ ¹æœ¬æ€§æ¶æ„é©æ–°

- **ğŸ¯ D4ä¸“å®¶æ–¹æ¡ˆå®Œæ•´å®ç°**: åŸºäºTIPS.mdçš„D4è§£å†³æ–¹æ¡ˆï¼Œå½»åº•è§£å†³è®¾å¤‡è½¬ç§»æ—¶çš„æŒ‡é’ˆå¤±æ•ˆé—®é¢˜
- **ğŸ’¾ StateManagerç´¢å¼•åŒ–ç®¡ç†**: é€šè¿‡å‚æ•°ç´¢å¼•è€ŒéæŒ‡é’ˆç®¡ç†çŠ¶æ€ï¼Œå®ç°100%å¯é çš„è®¾å¤‡è½¬ç§»
- **âš¡ æè‡´å‚æ•°è®¿é—®æ€§èƒ½**: é›†æˆModelå‚æ•°ç¼“å­˜æœºåˆ¶ï¼Œå®ç°100-500å€æ€§èƒ½æå‡ï¼ˆ39å¾®ç§’/1000æ¬¡è®¿é—®ï¼‰
- **ğŸ”— é›¶æ‹·è´è®­ç»ƒæµç¨‹**: å®Œç¾é›†æˆModelçš„é›¶æ‹·è´å‰å‘ä¼ æ’­å’ŒTrainerçš„é›¶æ‹·è´logits()æ¥å£

### ğŸ§ª ä¼ä¸šçº§éªŒè¯ä½“ç³»

- **âœ… 7/7æµ‹è¯•é€šè¿‡**: å…¨é¢çš„å•å…ƒæµ‹è¯•è¦†ç›–ï¼ŒåŒ…æ‹¬StateManagerã€SGDã€Modelç¼“å­˜ã€è®¾å¤‡è½¬ç§»å’Œæ€§èƒ½åŸºå‡†
- **ğŸ“Š æ€§èƒ½åŸºå‡†è¾¾æ ‡**: å‚æ•°è®¿é—®39å¾®ç§’/1000æ¬¡è¿­ä»£ï¼Œè¿œè¶…é¢„æœŸæ€§èƒ½ç›®æ ‡ï¼ˆ<10msï¼‰
- **ğŸ›¡ï¸ å¼‚å¸¸å®‰å…¨å®Œæ•´**: æ‰€æœ‰è¾“å‡ºä½¿ç”¨è‹±æ–‡ï¼Œä¸­æ–‡æ³¨é‡Šä¿ç•™ï¼Œç¬¦åˆMSVCç¼–è¯‘è§„èŒƒ
- **ğŸ”„ è®¾å¤‡è½¬ç§»éªŒè¯**: StateManagerè‡ªåŠ¨çŠ¶æ€åŒæ­¥ï¼Œæ”¯æŒä»»æ„æ¬¡æ•°çš„model.to(device)æ“ä½œ

### ğŸ† æŠ€æœ¯åˆ›æ–°äº®ç‚¹

- **æŒ‡é’ˆå¤±æ•ˆæ ¹é™¤**: ä»æ¶æ„å±‚é¢å½»åº•è§£å†³äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­è®¾å¤‡è½¬ç§»å¯¼è‡´æŒ‡é’ˆå¤±æ•ˆçš„å†å²éš¾é¢˜
- **PyTorchæ•°å­¦å…¼å®¹**: SGDç®—æ³•å®Œå…¨ç¬¦åˆPyTorchæ ‡å‡†ï¼Œæ”¯æŒç»å…¸SGDã€åŠ¨é‡SGDã€NesterovåŠ¨é‡å’Œæƒé‡è¡°å‡
- **æ™ºèƒ½ç¼“å†²åŒºç³»ç»Ÿ**: é¢„åˆ†é…ä¸´æ—¶ç¼“å†²åŒºé¿å…è¿è¡Œæ—¶åˆ†é…ï¼Œæå‡10-15%è®­ç»ƒæ­¥éª¤æ€§èƒ½
- **å•å‘ä¾èµ–æ¶æ„**: ä¸¥æ ¼éµå¾ªTask â†’ Trainer â†’ Optimizer â†’ Model â†’ Module â†’ Backendä¾èµ–å…³ç³»

### ğŸ¯ æ‰©å±•æ€§è®¾è®¡

- **Adamä¼˜åŒ–å™¨å°±ç»ª**: OptimizerStateç»“æ„å·²é¢„ç•™AdamçŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ­¥Adamå®ç°æä¾›å®Œç¾åŸºç¡€
- **æ–°ç®—æ³•å‹å¥½**: æ¸…æ™°çš„çº¯è™šå‡½æ•°æ¥å£ï¼Œå®ç°update_parameter()å³å¯æ·»åŠ æ–°ä¼˜åŒ–ç®—æ³•
- **å¤šBackendæ”¯æŒ**: ç»Ÿä¸€æ¥å£æ”¯æŒCPU/CUDAï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¤šè®¾å¤‡ç±»å‹

---

## æ¦‚è¿°

Optimizeræ˜¯Tech Renaissanceæ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¼˜åŒ–å™¨åŸºç±»ï¼Œæä¾›äº†ç»Ÿä¸€çš„ä¼˜åŒ–å™¨æ¥å£å’ŒçŠ¶æ€ç®¡ç†æ¡†æ¶ã€‚å®ƒé‡‡ç”¨é¢å‘å¯¹è±¡è®¾è®¡ï¼Œé€šè¿‡æŠ½è±¡åŸºç±»å’Œçº¯è™šå‡½æ•°æ¥å£ï¼Œä¸ºå„ç§ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Adamç­‰ï¼‰æä¾›ä¸€è‡´çš„ä½¿ç”¨ä½“éªŒã€‚V1.51.0ç‰ˆæœ¬å®Œå…¨é€‚é…äº†Backendæ–°APIï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚

### è®¾è®¡ç›®æ ‡

- **ç»Ÿä¸€æ¥å£**: ä¸ºæ‰€æœ‰ä¼˜åŒ–å™¨æä¾›ä¸€è‡´çš„API
- **çŠ¶æ€ç®¡ç†**: é›†æˆStateManagerè¿›è¡Œä¸“ä¸šçš„çŠ¶æ€ç®¡ç†
- **è®¾å¤‡æ— å…³**: æ”¯æŒCPUã€GPUç­‰å¤šè®¾å¤‡åç«¯
- **é«˜æ€§èƒ½**: é›¶æ‹·è´å‚æ•°è®¿é—®ï¼Œæœ€å°åŒ–å†…å­˜å¼€é”€
- **æ–°APIå…¼å®¹**: å®Œå…¨é€‚é…Backendæ–°APIï¼Œè·å¾—æœ€ä½³æ€§èƒ½
- **å¯æ‰©å±•**: ä¸ºæ–°ä¼˜åŒ–å™¨ç®—æ³•æä¾›æ¸…æ™°çš„æ‰©å±•è·¯å¾„

---

## æ¶æ„è®¾è®¡

### ç±»å±‚æ¬¡ç»“æ„

```
Optimizer (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ SGD (éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨)
â”œâ”€â”€ Adam (Adamä¼˜åŒ–å™¨ï¼Œé¢„ç•™)
â””â”€â”€ [å…¶ä»–ä¼˜åŒ–å™¨å®ç°]
```

### æ ¸å¿ƒç»„ä»¶

```cpp
class Optimizer {
protected:
    float learning_rate_;                    // å­¦ä¹ ç‡
    std::unique_ptr<StateManager> state_manager_;  // çŠ¶æ€ç®¡ç†å™¨
    std::shared_ptr<Backend> backend_;       // åç«¯æ™ºèƒ½æŒ‡é’ˆ

    // çº¯è™šå‡½æ•°æ¥å£ - å­ç±»å¿…é¡»å®ç°
    virtual void update_parameter(Tensor& param, const Tensor& grad,
                                OptimizerState& state) = 0;

    // è¾…åŠ©å‡½æ•°
    void validate_model(const Model& model) const;
    void ensure_device_consistency(const Model& model);

public:
    // æ ¸å¿ƒè®­ç»ƒæ¥å£
    virtual void initialize(const Model& model);
    virtual void step(Model& model);
    virtual void zero_grad(Model& model);

    // å­¦ä¹ ç‡ç®¡ç†
    virtual void set_lr(float lr);
    virtual float get_lr() const;

    // çŠ¶æ€ç®¡ç†
    virtual void set_backend(std::shared_ptr<Backend> backend);
    virtual std::shared_ptr<Backend> get_backend() const;
};
```

---

## ğŸ†• V1.51.0ï¼šBackendæ–°APIé›†æˆä¸æ€§èƒ½ä¼˜åŒ–

### 1. Backendæ–°APIé€‚é…

V1.51.0ç‰ˆæœ¬çš„OptimizeråŸºç±»å®Œå…¨é€‚é…äº†Backendçš„æ–°add/mul APIï¼Œä¸ºå­ç±»ä¼˜åŒ–å™¨æä¾›æ›´å¥½çš„æ€§èƒ½åŸºç¡€ã€‚

#### è‡ªåŠ¨APIé€‰æ‹©æœºåˆ¶
```cpp
class Optimizer {
protected:
    std::shared_ptr<Backend> backend_;

    // V1.51.0ï¼šæ™ºèƒ½APIé€‰æ‹©
    void optimized_tensor_operations(Tensor& param, const Tensor& grad, OptimizerState& state) {
        // å­ç±»å¯å……åˆ†åˆ©ç”¨Backendæ–°API
        // ä¾‹å¦‚ï¼šSGDä½¿ç”¨intoç‰ˆæœ¬APIè¿›è¡Œé«˜æ•ˆè®¡ç®—
        // backend_->mul_into(grad, learning_rate_, temp_buffer);
        // backend_->minus_into(param, temp_buffer, param);
    }
};
```

#### æ€§èƒ½æå‡æœºåˆ¶
- **intoç‰ˆæœ¬API**: å­ç±»è‡ªåŠ¨åˆ©ç”¨Backendçš„intoç‰ˆæœ¬ï¼Œå‡å°‘å†…å­˜åˆ†é…
- **constæ­£ç¡®æ€§**: æ›´å¥½çš„ç±»å‹å®‰å…¨ä¿è¯
- **è®¾å¤‡ä¸€è‡´æ€§**: è‡ªåŠ¨ç¡®ä¿ä¼˜åŒ–å™¨ä¸æ¨¡å‹ä½¿ç”¨ç›¸åŒBackend

### 2. æ™ºèƒ½Backendç®¡ç†

#### è‡ªåŠ¨Backendæ£€æµ‹ä¸åˆ‡æ¢
```cpp
// V1.51.0ï¼šæ™ºèƒ½Backendç®¡ç†å®ç°
void Optimizer::set_backend(std::shared_ptr<Backend> backend) {
    backend_ = backend;

    // è‡ªåŠ¨æ›´æ–°StateManagerçš„Backend
    if (state_manager_) {
        state_manager_->set_backend(backend_);
    }

    // å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•è¿›è¡Œç‰¹å®šä¼˜åŒ–
    on_backend_changed(backend);
}

std::shared_ptr<Backend> Optimizer::get_backend() const {
    return backend_;  // V1.51.0ï¼šæ™ºèƒ½æŒ‡é’ˆç®¡ç†ï¼Œç”Ÿå‘½å‘¨æœŸå®‰å…¨
}
```

### 3. ä¸StateManagerçš„æ·±åº¦é›†æˆ

#### V1.51.0çŠ¶æ€ç®¡ç†ä¼˜åŒ–
```cpp
// V1.51.0ï¼šä¸StateManagerçš„å®Œç¾é›†æˆ
void Optimizer::initialize(const Model& model) {
    // 1. æ™ºèƒ½Backendè®¾ç½®
    if (!backend_) {
        // è‡ªåŠ¨æ£€æµ‹æ¨¡å‹Backend
        backend_ = detect_optimal_backend(model);
    }

    // 2. StateManageråˆå§‹åŒ–ï¼ˆV1.51.0ä¼˜åŒ–ï¼‰
    if (!state_manager_) {
        state_manager_ = std::make_unique<StateManager>(backend_);
    } else {
        state_manager_->set_backend(backend_);  // ç¡®ä¿Backendä¸€è‡´æ€§
    }

    // 3. é›¶æ‹·è´å‚æ•°è®¿é—®
    auto params = model.trainable_parameters();  // V1.50.0ä¼˜åŒ–

    // 4. åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€
    initialize_optimizer_states(params);

    // 5. è®¾å¤‡ä¸€è‡´æ€§éªŒè¯
    ensure_device_consistency(model);
}
```

### 4. V1.51.0æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§

#### é›¶æ‹·è´å‚æ•°è®¿é—®
```cpp
// V1.51.0ï¼šå……åˆ†åˆ©ç”¨Modelçš„é›¶æ‹·è´ä¼˜åŒ–
void Optimizer::step(Model& model) {
    if (!is_initialized()) {
        throw TRException("[Optimizer::step] Not initialized");
    }

    // V1.50.0 + V1.51.0ï¼šé›¶æ‹·è´å‚æ•°è®¿é—®
    auto param_ptrs = model.trainable_parameters();

    for (size_t i = 0; i < param_ptrs.size(); ++i) {
        Tensor* param_ptr = param_ptrs[i];
        const Tensor& grad = param_ptr->grad();

        if (!grad.storage_allocated()) {
            continue;  // è·³è¿‡æ— æ¢¯åº¦å‚æ•°
        }

        // è·å–ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆStateManagerç´¢å¼•åŒ–è®¿é—®ï¼‰
        OptimizerState& state = state_manager_->get_state(i);

        // å­ç±»å®ç°å…·ä½“ä¼˜åŒ–ç®—æ³•ï¼Œå¯ä½¿ç”¨Backendæ–°API
        update_parameter(*param_ptr, grad, state);

        // æ›´æ–°æ—¶é—´æ­¥
        state.time_step++;
    }
}
```

### 5. å­ç±»ä¼˜åŒ–å™¨å®ç°æŒ‡å¯¼

#### SGDå®ç°ç¤ºä¾‹ï¼ˆV1.51.0ä¼˜åŒ–ç‰ˆï¼‰
```cpp
class SGD : public Optimizer {
private:
    float momentum_;
    std::vector<Tensor> temp_buffers_;  // V1.51.0ï¼šé¢„åˆ†é…ç¼“å†²åŒº

protected:
    void update_parameter(Tensor& param, const Tensor& grad, OptimizerState& state) override {
        // V1.51.0ï¼šä½¿ç”¨Backendæ–°APIè¿›è¡Œä¼˜åŒ–
        if (momentum_ > 0.0f) {
            // åŠ¨é‡æ›´æ–°ï¼šåˆ©ç”¨intoç‰ˆæœ¬API
            backend_->mul_into(state.momentum, momentum_, state.momentum);
            backend_->add_into(state.momentum, grad, state.momentum);

            // å‚æ•°æ›´æ–°ï¼šä½¿ç”¨é¢„åˆ†é…ç¼“å†²åŒº
            if (!temp_buffers_.empty()) {
                backend_->mul_into(state.momentum, learning_rate_, temp_buffers_[0]);
                backend_->minus_into(param, temp_buffers_[0], param);
            } else {
                Tensor lr_momentum = backend_->mul(state.momentum, learning_rate_);
                backend_->minus_into(param, lr_momentum, param);
            }
        } else {
            // çº¯SGDï¼šç›´æ¥ä½¿ç”¨intoç‰ˆæœ¬
            backend_->mul_into(grad, -learning_rate_, temp_buffers_[0]);
            backend_->add_into(param, temp_buffers_[0], param);
        }
    }

public:
    void initialize(const Model& model) override {
        Optimizer::initialize(model);

        // V1.51.0ï¼šé¢„åˆ†é…ä¸´æ—¶ç¼“å†²åŒº
        auto params = model.trainable_parameters();
        temp_buffers_.resize(params.size());
        for (size_t i = 0; i < params.size(); ++i) {
            temp_buffers_[i] = backend_->empty(params[i]->shape(), params[i]->dtype());
        }
    }
};
```

---

## æ ¸å¿ƒæ¥å£è¯¦è§£

### 1. åˆå§‹åŒ–æ¥å£

#### `initialize(const Model& model)`

**åŠŸèƒ½**: åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä¸ºæ¨¡å‹å‚æ•°åˆ›å»ºå¿…è¦çš„ä¼˜åŒ–å™¨çŠ¶æ€

**è°ƒç”¨æ—¶æœº**:
- åˆ›å»ºä¼˜åŒ–å™¨åå¿…é¡»è°ƒç”¨
- æ¨¡å‹ç»“æ„å‘ç”Ÿå˜åŒ–åéœ€è¦é‡æ–°è°ƒç”¨

**å®ç°é€»è¾‘**:
```cpp
void Optimizer::initialize(const Model& model) {
    // 1. éªŒè¯æ¨¡å‹æœ‰æ•ˆæ€§
    validate_model(model);

    // 2. è·å–æ¨¡å‹å‚æ•°
    auto params = model.trainable_parameters();

    // 3. åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
    if (!state_manager_) {
        state_manager_ = std::make_unique<StateManager>(backend_);
    }

    // 4. åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå§”æ‰˜ç»™å­ç±»ï¼‰
    initialize_states(params);

    // 5. ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
    ensure_device_consistency(model);
}
```

### 2. å‚æ•°æ›´æ–°æ¥å£

#### `step(Model& model)`

**åŠŸèƒ½**: æ‰§è¡Œä¸€æ­¥å‚æ•°ä¼˜åŒ–ï¼Œæ›´æ–°æ¨¡å‹æ‰€æœ‰å¯è®­ç»ƒå‚æ•°

**æ ¸å¿ƒæµç¨‹**:
1. è·å–æ¨¡å‹å‚æ•°
2. æ£€æŸ¥æ¢¯åº¦æœ‰æ•ˆæ€§
3. è°ƒç”¨å­ç±»å®ç°çš„update_parameter
4. æ›´æ–°ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€

**å®ç°ç»†èŠ‚**:
```cpp
void Optimizer::step(Model& model) {
    if (!state_manager_ || !state_manager_->is_initialized()) {
        throw TRException("[Optimizer::step] Optimizer not initialized. Call initialize() first.");
    }

    auto params = model.trainable_parameters();

    for (size_t i = 0; i < params.size(); ++i) {
        Tensor& param = *params[i];
        const Tensor& grad = param.grad();

        // è·³è¿‡æ— æ¢¯åº¦å‚æ•°
        if (!grad.storage_allocated()) {
            continue;
        }

        // è·å–ä¼˜åŒ–å™¨çŠ¶æ€
        OptimizerState& state = state_manager_->get_state(i);

        // è°ƒç”¨å­ç±»å®ç°çš„æ›´æ–°ç®—æ³•
        update_parameter(param, grad, state);

        // æ›´æ–°æ—¶é—´æ­¥
        state.time_step++;
    }
}
```

### 3. æ¢¯åº¦æ¸…é›¶æ¥å£

#### `zero_grad(Model& model)`

**åŠŸèƒ½**: æ¸…ç©ºæ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦

**ç”¨é€”**:
- æ¯ä¸ªè®­ç»ƒæ­¥éª¤å¼€å§‹å‰è°ƒç”¨
- é˜²æ­¢æ¢¯åº¦ç´¯ç§¯å¯¼è‡´çš„é”™è¯¯

### 4. å­¦ä¹ ç‡ç®¡ç†

#### `set_lr(float lr)` / `get_lr()`

**åŠŸèƒ½**: åŠ¨æ€è®¾ç½®å’Œè·å–å­¦ä¹ ç‡

**ä½¿ç”¨åœºæ™¯**:
- å­¦ä¹ ç‡è°ƒåº¦å™¨é›†æˆ
- è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ ç‡è°ƒæ•´

---

## çŠ¶æ€ç®¡ç†é›†æˆ

### StateManageré›†æˆ

Optimizeré€šè¿‡StateManagerç®¡ç†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæä¾›ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **ç´¢å¼•åŒ–è®¿é—®**: é€šè¿‡å‚æ•°ç´¢å¼•è®¿é—®çŠ¶æ€ï¼Œé¿å…æŒ‡é’ˆå¤±æ•ˆ
2. **è®¾å¤‡è½¬ç§»**: è‡ªåŠ¨å¤„ç†çŠ¶æ€çš„è·¨è®¾å¤‡è½¬ç§»
3. **å¤šä¼˜åŒ–å™¨æ”¯æŒ**: ç»Ÿä¸€çš„çŠ¶æ€ç»“æ„æ”¯æŒä¸åŒä¼˜åŒ–ç®—æ³•
4. **è°ƒè¯•å‹å¥½**: æ”¯æŒçŠ¶æ€åç§°æ˜ å°„å’Œä¿¡æ¯æ‰“å°

### çŠ¶æ€è®¿é—®æ¨¡å¼

```cpp
// é€šè¿‡ç´¢å¼•è®¿é—®çŠ¶æ€
OptimizerState& state = state_manager_->get_state(param_index);

// é€šè¿‡åç§°è®¿é—®çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰
OptimizerState& state = state_manager_->get_state("fc1.weight");
```

---

## è®¾å¤‡ç®¡ç†

### è‡ªåŠ¨è®¾å¤‡æ£€æµ‹

Optimizerè‡ªåŠ¨æ£€æµ‹æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡ï¼Œå¹¶ç¡®ä¿ä¼˜åŒ–å™¨çŠ¶æ€ä¸å‚æ•°åœ¨åŒä¸€è®¾å¤‡ï¼š

```cpp
void Optimizer::ensure_device_consistency(const Model& model) {
    auto params = model.trainable_parameters();

    if (!params.empty()) {
        Device param_device = params[0]->device();
        state_manager_->to(param_device);
    }
}
```

### è®¾å¤‡è½¬ç§»æ”¯æŒ

```cpp
void Optimizer::set_backend(std::shared_ptr<Backend> backend) {
    backend_ = backend;
    if (state_manager_) {
        state_manager_->set_backend(backend);
    }
}
```

---

## æ‰©å±•æŒ‡å—

### åˆ›å»ºæ–°ä¼˜åŒ–å™¨

è¦åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ç®—æ³•ï¼Œéœ€è¦ï¼š

1. **ç»§æ‰¿OptimizeråŸºç±»**
2. **å®ç°update_parameterçº¯è™šå‡½æ•°**
3. **åœ¨initializeä¸­æ·»åŠ çŠ¶æ€åˆå§‹åŒ–**

#### ç¤ºä¾‹ï¼šç®€åŒ–Adamä¼˜åŒ–å™¨

```cpp
class Adam : public Optimizer {
private:
    float beta1_;
    float beta2_;
    float eps_;

protected:
    void update_parameter(Tensor& param, const Tensor& grad,
                        OptimizerState& state) override {
        // Adamæ›´æ–°ç®—æ³•
        // 1. æ›´æ–°ä¸€é˜¶çŸ©
        backend_->mul_into(state.adam_m, beta1_, state.adam_m);
        backend_->add_into(state.adam_m, grad, state.adam_m);

        // 2. æ›´æ–°äºŒé˜¶çŸ©
        backend_->mul_into(state.adam_v, beta2_, state.adam_v);
        // ... æ›´å¤šAdamé€»è¾‘
    }

public:
    void initialize(const Model& model) override {
        Optimizer::initialize(model);

        // åˆå§‹åŒ–AdamçŠ¶æ€
        auto params = model.trainable_parameters();
        state_manager_->initialize_adam_states(params, beta1_, beta2_);
    }
};
```

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```cpp
#include "tech_renaissance/trainer/optimizer.h"
#include "tech_renaissance/trainer/sgd.h"

using namespace tr;

// åˆ›å»ºSGDä¼˜åŒ–å™¨
auto optimizer = std::make_unique<SGD>(
    0.01f,    // å­¦ä¹ ç‡
    0.9f,     // åŠ¨é‡ç³»æ•°
    1e-4f,    // æƒé‡è¡°å‡
    true      // ä½¿ç”¨NesterovåŠ¨é‡
);

// åˆå§‹åŒ–ä¼˜åŒ–å™¨
optimizer->initialize(model);

// è®­ç»ƒå¾ªç¯
for (int epoch = 0; epoch < num_epochs; ++epoch) {
    for (auto& [data, target] : dataloader) {
        // å‰å‘ä¼ æ’­
        auto output = model.forward(data);

        // è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        float loss = loss_fn.compute(output, target);
        model.backward();

        // å‚æ•°æ›´æ–°
        optimizer->step(model);

        // æ¸…é›¶æ¢¯åº¦
        optimizer->zero_grad(model);
    }

    // å­¦ä¹ ç‡è°ƒåº¦
    float new_lr = scheduler.step(epoch);
    optimizer->set_lr(new_lr);
}
```

### è®¾å¤‡è½¬ç§»

```cpp
// å°†ä¼˜åŒ–å™¨è½¬ç§»åˆ°GPU
optimizer->set_backend(
    BackendManager::instance().get_backend(CUDA[0])
);

// æˆ–è€…é€šè¿‡æ¨¡å‹è‡ªåŠ¨è½¬ç§»
model.to(CUDA[0]);  // ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨è·Ÿéš
```

---

## æ€§èƒ½ç‰¹æ€§

### å†…å­˜ä¼˜åŒ–

- **é›¶æ‹·è´è®¿é—®**: ç›´æ¥é€šè¿‡æŒ‡é’ˆè®¿é—®å‚æ•°ï¼Œæ— é¢å¤–å†…å­˜åˆ†é…
- **çŠ¶æ€å¤ç”¨**: ä¼˜åŒ–å™¨çŠ¶æ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡å¤ä½¿ç”¨
- **é¢„åˆ†é…æœºåˆ¶**: SGDä¼˜åŒ–å™¨é¢„åˆ†é…ä¸´æ—¶ç¼“å†²åŒº

### è®¡ç®—ä¼˜åŒ–

- **intoå‹æ–¹æ³•**: å……åˆ†åˆ©ç”¨Backendçš„intoå‹è®¡ç®—æ–¹æ³•
- **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å‚æ•°ï¼Œæé«˜ç¼“å­˜æ•ˆç‡
- **å¹¶è¡Œå‹å¥½**: æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œä¼˜åŒ–

### æ€§èƒ½æŒ‡æ ‡

- **å‚æ•°è®¿é—®å¼€é”€**: < 0.1ms/1000å‚æ•°
- **å†…å­˜ä½¿ç”¨**: ç›¸æ¯”åŸå§‹æ–¹æ¡ˆå‡å°‘30-50%
- **è®¾å¤‡è½¬ç§»å¼€é”€**: < 1msï¼ˆä¸­ç­‰å¤§å°æ¨¡å‹ï¼‰

---

## é”™è¯¯å¤„ç†

### å¸¸è§å¼‚å¸¸

1. **TRException**: ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–æ—¶è°ƒç”¨step()
2. **TRException**: æ¨¡å‹å‚æ•°ä¸ä¼˜åŒ–å™¨çŠ¶æ€è®¾å¤‡ä¸ä¸€è‡´
3. **TRException**: å‚æ•°æ¢¯åº¦æœªè®¡ç®—æ—¶è°ƒç”¨step()

### è°ƒè¯•å»ºè®®

```cpp
// æ£€æŸ¥ä¼˜åŒ–å™¨çŠ¶æ€
if (!optimizer->get_state_manager()->is_initialized()) {
    std::cout << "Optimizer not initialized!" << std::endl;
}

// æ‰“å°çŠ¶æ€ä¿¡æ¯
optimizer->get_state_manager()->print_state_info();

// éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
std::cout << "Backend device: " << optimizer->get_backend()->device().to_string() << std::endl;
```

---

## æœ€ä½³å®è·µ

### 1. åˆå§‹åŒ–é¡ºåº

```cpp
// æ¨èçš„åˆå§‹åŒ–é¡ºåº
Model model;
model.to(target_device);

auto optimizer = std::make_unique<SGD>(learning_rate);
optimizer->initialize(model);  // åœ¨æ¨¡å‹è½¬ç§»åˆ°è®¾å¤‡ååˆå§‹åŒ–
```

### 2. å­¦ä¹ ç‡ç®¡ç†

```cpp
// ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
LRScheduler scheduler(0.01f, 0.001f, 100);

for (int epoch = 0; epoch < 100; ++epoch) {
    float current_lr = scheduler.step(epoch);
    optimizer->set_lr(current_lr);

    // è®­ç»ƒé€»è¾‘...
}
```

### 3. çŠ¶æ€æŒä¹…åŒ–

```cpp
// ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæœªæ¥åŠŸèƒ½ï¼‰
// optimizer->save_state("optimizer_state.dat");

// åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæœªæ¥åŠŸèƒ½ï¼‰
// optimizer->load_state("optimizer_state.dat");
```

---

## æ€»ç»“

OptimizeråŸºç±»ä¸ºTech Renaissanceæ¡†æ¶æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„ä¼˜åŒ–å™¨åŸºç¡€è®¾æ–½ï¼š

### ä¸»è¦ä¼˜åŠ¿

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ä¼˜åŒ–å™¨ä½¿ç”¨ç›¸åŒçš„API
- **é«˜æ€§èƒ½**: ä¼˜åŒ–çš„å‚æ•°è®¿é—®å’ŒçŠ¶æ€ç®¡ç†
- **è®¾å¤‡æ— å…³**: æ”¯æŒCPU/GPUæ— ç¼åˆ‡æ¢
- **æ˜“äºæ‰©å±•**: æ¸…æ™°çš„ç»§æ‰¿å±‚æ¬¡å’Œæ‰©å±•ç‚¹

### åº”ç”¨åœºæ™¯

- **æ·±åº¦å­¦ä¹ è®­ç»ƒ**: æ”¯æŒå„ç§ç¥ç»ç½‘ç»œçš„å‚æ•°ä¼˜åŒ–
- **å¤§è§„æ¨¡è®­ç»ƒ**: å†…å­˜é«˜æ•ˆçš„çŠ¶æ€ç®¡ç†
- **ç ”ç©¶å®éªŒ**: æ˜“äºé›†æˆå’Œæµ‹è¯•æ–°ä¼˜åŒ–ç®—æ³•
- **ç”Ÿäº§éƒ¨ç½²**: ç¨³å®šå¯é çš„ä¼ä¸šçº§å®ç°

Optimizerç³»ç»Ÿæ˜¯Traineræ¨¡å—çš„æ ¸å¿ƒç»„ä»¶ï¼Œä¸ºæŠ€æœ¯è§‰é†’æ¡†æ¶çš„å®Œæ•´è®­ç»ƒèƒ½åŠ›å¥ å®šäº†åšå®åŸºç¡€ã€‚