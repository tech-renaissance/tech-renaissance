# LossåŸºç±»æ–‡æ¡£

## æ¦‚è¿°

LossåŸºç±»æ˜¯æŠ€æœ¯è§‰é†’æ¡†æ¶Trainerç³»ç»Ÿä¸­æ‰€æœ‰æŸå¤±å‡½æ•°çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æŸå¤±è®¡ç®—æ¥å£ã€æ¢¯åº¦ç®¡ç†æœºåˆ¶å’Œæ¨¡å¼åˆ‡æ¢åŠŸèƒ½ã€‚Lossç±»é‡‡ç”¨äº†ä¸Moduleç±»å¹³çº§çš„è®¾è®¡ç†å¿µï¼Œä½œä¸ºè®­ç»ƒç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†è®­ç»ƒ/è¯„ä¼°æ¨¡å¼åˆ‡æ¢ã€æŸå¤±è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—çš„åˆäºŒä¸ºä¸€åŠŸèƒ½ã€‚V2.2.1ç‰ˆæœ¬è¿›ä¸€æ­¥ç®€åŒ–äº†æ„é€ å‡½æ•°ï¼Œæ”¯æŒæ›´çµæ´»çš„å¯¹è±¡åˆ›å»ºæ–¹å¼ã€‚

## ç‰ˆæœ¬ä¿¡æ¯

- **ç‰ˆæœ¬**: V2.2.1
- **æ—¥æœŸ**: 2025å¹´11æœˆ24æ—¥
- **ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
- **æ‰€å±ç³»åˆ—**: trainer

## ğŸ‰ V2.2.1æœ€æ–°æ›´æ–°ï¼šæ„é€ å‡½æ•°ç®€åŒ–

### âœ¨ æ„é€ å‡½æ•°ä¼˜åŒ–

V2.2.1ç‰ˆæœ¬å¯¹Lossç±»æ„é€ å‡½æ•°è¿›è¡Œäº†é‡è¦ä¼˜åŒ–ï¼Œæå‡äº†ä½¿ç”¨çš„ä¾¿æ·æ€§ï¼š

#### 1. ç®€åŒ–çš„é»˜è®¤æ„é€ å‡½æ•°

```cpp
// V2.2.1ï¼šç®€åŒ–çš„æ„é€ å‡½æ•°ï¼Œé»˜è®¤ä¸ºè®­ç»ƒæ¨¡å¼
explicit Loss(bool training_mode = true);
```

**ä¸»è¦å˜åŒ–**ï¼š
- **ç§»é™¤backendå‚æ•°**ï¼šæ„é€ å‡½æ•°ä¸å†éœ€è¦backendå‚æ•°ï¼Œæ”¯æŒå»¶è¿Ÿåç«¯è®¾ç½®
- **é»˜è®¤è®­ç»ƒæ¨¡å¼**ï¼šé»˜è®¤è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œç¬¦åˆå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯
- **å»¶è¿Ÿåˆå§‹åŒ–**ï¼šå¯ä»¥åœ¨æ„é€ åå†è®¾ç½®backendï¼Œæä¾›æ›´å¤§çš„çµæ´»æ€§

#### 2. V2.2.1ä½¿ç”¨ç¤ºä¾‹å¯¹æ¯”

**V2.2.1ä¹‹å‰ï¼ˆå¤æ‚æ–¹å¼ï¼‰**ï¼š
```cpp
// éœ€è¦åœ¨æ„é€ æ—¶æä¾›backend
auto backend = BackendManager::get_cpu_backend();
CrossEntropyLoss loss_fn(backend, 0.1f);  // å¤æ‚æ„é€ 
```

**V2.2.1ï¼ˆç®€åŒ–æ–¹å¼ï¼‰**ï¼š
```cpp
// ç›´æ¥æ„é€ ï¼Œåå»¶è¿Ÿè®¾ç½®backend
CrossEntropyLoss loss_fn(0.1f);  // ç®€åŒ–æ„é€ 
loss_fn.set_backend(BackendManager::get_cpu_backend());  // å»¶è¿Ÿè®¾ç½®
```

**è¿›ä¸€æ­¥ç®€åŒ–ï¼ˆç›´æ¥æ„é€ é£æ ¼ï¼‰**ï¼š
```cpp
// å®Œå…¨ç¬¦åˆV2.2.1ç›´æ¥æ„é€ é£æ ¼
auto loss_fn = CrossEntropyLoss();  // æœ€ç®€æ„é€ 
loss_fn.set_backend(backend);
```

### V2.2.1è®¾è®¡ä¼˜åŠ¿

#### 1. æ„é€ é£æ ¼ç»Ÿä¸€
- **æ™ºèƒ½æŒ‡é’ˆé£æ ¼**ï¼š`auto loss_fn = std::make_shared<CrossEntropyLoss>(0.1f);`
- **ç›´æ¥æ„é€ é£æ ¼**ï¼š`auto loss_fn = CrossEntropyLoss(0.1f);`
- **ä¸¤ç§é£æ ¼å®Œå…¨ç­‰ä»·**ï¼šè¿è¡Œæ—¶æ€§èƒ½ç›¸åŒï¼Œä½¿ç”¨æ–¹å¼ä¸€è‡´

#### 2. ä½¿ç”¨ä¾¿åˆ©æ€§æå‡
- **é›¶å‚æ•°æ„é€ **ï¼š`CrossEntropyLoss()` ä½¿ç”¨é»˜è®¤é…ç½®
- **å»¶è¿Ÿé…ç½®**ï¼šæ„é€ åå†è®¾ç½®backendå’Œå…¶ä»–å‚æ•°
- **é“¾å¼è°ƒç”¨**ï¼šæ”¯æŒæµç•…çš„APIè°ƒç”¨

#### 3. Task APIå®Œç¾é€‚é…
```cpp
// V2.2.1ï¼šTask APIä¸­çš„ä½¿ç”¨
auto loss_fn = CrossEntropyLoss(0.1f);  // ç›´æ¥æ„é€ 
loss_fn.set_backend(backend);            // å»¶è¿Ÿé…ç½®
```

## è®¾è®¡ç†å¿µ

### ç»Ÿä¸€æ¥å£è®¾è®¡

LossåŸºç±»é€šè¿‡`criterion()`æ–¹æ³•å®ç°äº†æŸå¤±è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—çš„åˆäºŒä¸ºä¸€ï¼š

```cpp
// ç»Ÿä¸€çš„æŸå¤±+æ¢¯åº¦è®¡ç®—æ¥å£
virtual float criterion(Tensor& logits, const Tensor& target,
                      const std::string& reduction = "mean") = 0;
```

**è®¾è®¡ç‰¹ç‚¹**ï¼š
- **è®­ç»ƒæ¨¡å¼**ï¼šåŒæ—¶è®¡ç®—æŸå¤±å€¼å¹¶å­˜å‚¨æ¢¯åº¦åˆ°è¾“å…¥å¼ é‡
- **è¯„ä¼°æ¨¡å¼**ï¼šåªè®¡ç®—æŸå¤±å€¼ï¼Œä¸è®¡ç®—æ¢¯åº¦
- **å‚æ•°åŒ–reduction**ï¼šæ”¯æŒ"mean"ï¼ˆå¹³å‡ï¼‰å’Œ"sum"ï¼ˆæ€»å’Œï¼‰ä¸¤ç§èšåˆæ–¹å¼
- **V1.59.0ä¼˜åŒ–**: æ”¯æŒintoå‹æ–¹æ³•ç¼“å­˜æœºåˆ¶ï¼Œæå‡æ€§èƒ½

### æ¶æ„è§£è€¦è®¾è®¡

Lossç±»ä¸Modelç±»å®Œå…¨è§£è€¦ï¼Œä½œä¸ºç‹¬ç«‹çš„Trainerç»„ä»¶ï¼š

```cpp
// Losså’ŒModelæ˜¯å¹³çº§çš„ç»„ä»¶
auto model = Model::create("MLP", ...);
auto loss_fn = CrossEntropyLoss(0.1f);

// ç‹¬ç«‹é…ç½®åç«¯
auto backend = BackendManager::get_cpu_backend();
model.set_backend(backend);
loss_fn.set_backend(backend);

// ç‹¬ç«‹ç®¡ç†çŠ¶æ€
model.train();
loss_fn.train();  // æˆ–è€… loss_fn.eval()
```

### V2.2.1å†…å­˜é«˜æ•ˆè®¾è®¡

Lossç±»é‡‡ç”¨æ¢¯åº¦å°±åœ°å­˜å‚¨ç­–ç•¥ï¼Œé¿å…é¢å¤–å†…å­˜åˆ†é…ï¼š

```cpp
// ç›´æ¥åœ¨è¾“å…¥å¼ é‡ä¸Šå­˜å‚¨æ¢¯åº¦
float loss = loss_fn.criterion(logits, target);

// æ¢¯åº¦å·²å­˜å‚¨åœ¨logits.grad()ä¸­
if (logits.has_grad()) {
    Tensor& grad = logits.grad();  // å°±åœ°å­˜å‚¨çš„æ¢¯åº¦
}
```

## æ ¸å¿ƒæ¥å£

### V2.2.1æ„é€ å‡½æ•°

```cpp
// ç®€åŒ–çš„æ„é€ å‡½æ•°ï¼Œé»˜è®¤è®­ç»ƒæ¨¡å¼
explicit Loss(bool training_mode = true);

// è™šææ„å‡½æ•°
virtual ~Loss() = default;
```

**å‚æ•°è¯´æ˜**ï¼š
- `training_mode`: åˆå§‹è®­ç»ƒæ¨¡å¼ï¼Œé»˜è®¤ä¸ºtrueï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```cpp
// V2.2.1ï¼šå¤šç§æ„é€ æ–¹å¼
Loss loss_fn1;                    // é»˜è®¤è®­ç»ƒæ¨¡å¼
Loss loss_fn2(true);              // æ˜¾å¼è®­ç»ƒæ¨¡å¼
Loss loss_fn3(false);             // è¯„ä¼°æ¨¡å¼
```

### æ¨¡å¼æ§åˆ¶æ¥å£

```cpp
// è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆè®¡ç®—æŸå¤±å’Œæ¢¯åº¦ï¼‰
virtual void train();

// è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆåªè®¡ç®—æŸå¤±ï¼‰
virtual void eval();

// æ£€æŸ¥å½“å‰æ¨¡å¼
virtual bool is_training() const;
```

**æ¨¡å¼è¡Œä¸º**ï¼š
- **è®­ç»ƒæ¨¡å¼**ï¼š`criterion()`åŒæ—¶è®¡ç®—æŸå¤±å€¼å’Œæ¢¯åº¦
- **è¯„ä¼°æ¨¡å¼**ï¼š`criterion()`åªè®¡ç®—æŸå¤±å€¼ï¼Œè·³è¿‡æ¢¯åº¦è®¡ç®—

### æ ¸å¿ƒè®¡ç®—æ¥å£

```cpp
// æŸå¤±+æ¢¯åº¦è®¡ç®—åˆäºŒä¸ºä¸€
virtual float criterion(Tensor& logits, const Tensor& target,
                      const std::string& reduction = "mean") = 0;
```

**å‚æ•°è¯´æ˜**ï¼š
- `logits`: æ¨¡å‹è¾“å‡ºlogitså¼ é‡ï¼ˆéconstï¼Œç”¨äºå­˜å‚¨æ¢¯åº¦ï¼‰
- `target`: ç›®æ ‡æ ‡ç­¾å¼ é‡ï¼Œå¯ä»¥æ˜¯INT32ç±»åˆ«æ ‡ç­¾æˆ–FP32 one-hotç¼–ç 
- `reduction`: æŸå¤±èšåˆæ–¹å¼ï¼Œ"mean"ï¼ˆå¹³å‡ï¼‰æˆ–"sum"ï¼ˆæ€»å’Œï¼‰

**è¿”å›å€¼**ï¼š
- æŸå¤±å€¼ï¼ˆfloatï¼‰

**å‰¯ä½œç”¨**ï¼š
- è®­ç»ƒæ¨¡å¼ä¸‹ï¼šæ¢¯åº¦å­˜å‚¨åˆ°`logits.grad()`
- è¯„ä¼°æ¨¡å¼ä¸‹ï¼šæ— å‰¯ä½œç”¨

### åç«¯ç®¡ç†æ¥å£

```cpp
// è®¾ç½®è®¡ç®—åç«¯ï¼ˆV2.2.1ï¼šå»¶è¿Ÿè®¾ç½®æ”¯æŒï¼‰
virtual void set_backend(std::shared_ptr<Backend> backend);

// è·å–å½“å‰åç«¯
virtual std::shared_ptr<Backend> get_backend() const;
```

### ä¿¡æ¯æŸ¥è¯¢æ¥å£

```cpp
// è·å–æŸå¤±å‡½æ•°ç±»å‹åç§°
virtual std::string type_name() const = 0;
```

## V2.2.1ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨ï¼ˆV2.2.1ç®€åŒ–æ–¹å¼ï¼‰

```cpp
#include "tech_renaissance.h"

using namespace tr;

int main() {
    // V2.2.1ï¼šç®€åŒ–çš„æ„é€ æ–¹å¼
    CrossEntropyLoss loss_fn(0.1f);  // 10%æ ‡ç­¾å¹³æ»‘

    // å»¶è¿Ÿè®¾ç½®åç«¯
    auto backend = BackendManager::get_cpu_backend();
    loss_fn.set_backend(backend);

    // åˆ›å»ºæµ‹è¯•æ•°æ®
    Tensor logits = backend->randn({4, 10});  // 4ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç±»åˆ«
    Tensor targets = Tensor::from_vector({0, 2, 1, 3}, DType::INT32);

    // è¯„ä¼°æ¨¡å¼ï¼šåªè®¡ç®—æŸå¤±
    loss_fn.eval();
    float eval_loss = loss_fn.criterion(logits, targets, "mean");
    std::cout << "Evaluation loss: " << eval_loss << std::endl;

    // è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æŸå¤±å’Œæ¢¯åº¦
    loss_fn.train();
    float train_loss = loss_fn.criterion(logits, targets, "mean");
    std::cout << "Training loss: " << train_loss << std::endl;

    // è·å–æ¢¯åº¦
    if (logits.has_grad()) {
        std::cout << "Gradient shape: " << logits.grad().shape().to_string() << std::endl;
    }

    return 0;
}
```

### V2.2.1æ™ºèƒ½æŒ‡é’ˆé£æ ¼ä½¿ç”¨

```cpp
// æ™ºèƒ½æŒ‡é’ˆé£æ ¼ - ç°ä»£C++æœ€ä½³å®è·µ
auto loss_fn = std::make_shared<CrossEntropyLoss>(0.1f);
loss_fn->set_backend(BackendManager::get_cpu_backend());

// åœ¨Taskä¸­ä½¿ç”¨
auto task = std::make_shared<Task>(model, dataset, trainer);
task->config(cfg);
task->run();
```

### V2.2.1ç›´æ¥æ„é€ é£æ ¼ä½¿ç”¨

```cpp
// ç›´æ¥æ„é€ é£æ ¼ - ç®€æ´ç›´è§‚
auto loss_fn = CrossEntropyLoss(0.1f);
loss_fn.set_backend(BackendManager::get_cpu_backend());

// åœ¨Taskä¸­ä½¿ç”¨
auto task = Task(model, dataset, trainer);
task.config(cfg);
task.run();
```

### ä¸Modelé…åˆä½¿ç”¨

```cpp
// V2.2.1ï¼šç®€åŒ–çš„åˆ›å»ºæ–¹å¼
auto model = Model::create("MLP",
    std::make_shared<Linear>(784, 512),
    std::make_shared<Tanh>(),
    std::make_shared<Linear>(512, 10)
);

auto loss_fn = CrossEntropyLoss(0.1f);  // V2.2.1ç®€åŒ–æ„é€ 

// è®¾ç½®ç›¸åŒåç«¯
auto backend = BackendManager::get_cpu_backend();
model.set_backend(backend);
loss_fn.set_backend(backend);

// è®¾ç½®è®­ç»ƒæ¨¡å¼
model.train();
loss_fn.train();

// å‰å‘ä¼ æ’­
Tensor input = backend->randn({32, 784});
Tensor output = model.forward(input);

// æŸå¤±è®¡ç®—ï¼ˆè‡ªåŠ¨å­˜å‚¨æ¢¯åº¦åˆ°output.grad()ï¼‰
Tensor targets = backend->ones({32}, DType::INT32);
float loss = loss_fn.criterion(output, targets, "mean");

// åå‘ä¼ æ’­ï¼ˆä½¿ç”¨å­˜å‚¨çš„æ¢¯åº¦ï¼‰
Tensor grad_input = model.backward(output.grad());

// å‚æ•°æ›´æ–°
auto params = model.parameters();
optimizer.step(params);

// æ¸…ç†æ¢¯åº¦
model.zero_grad();
```

## V2.2.1æ„é€ é£æ ¼å¯¹æ¯”

### æ™ºèƒ½æŒ‡é’ˆé£æ ¼

**ç‰¹ç‚¹**ï¼š
- ç°ä»£C++æœ€ä½³å®è·µ
- æ”¯æŒå¯¹è±¡å…±äº«å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- é€‚åˆå¤æ‚é¡¹ç›®å’Œç”Ÿäº§ç¯å¢ƒ

**ç¤ºä¾‹**ï¼š
```cpp
// æ¨èï¼šæ™ºèƒ½æŒ‡é’ˆé£æ ¼
auto loss_fn = std::make_shared<CrossEntropyLoss>(0.1f);
loss_fn->set_backend(backend);
loss_fn->train();

float loss = loss_fn->criterion(logits, targets);
```

### ç›´æ¥æ„é€ é£æ ¼

**ç‰¹ç‚¹**ï¼š
- ç®€æ´ç›´è§‚ï¼Œä»£ç é‡å°‘
- é€‚åˆå¿«é€ŸåŸå‹å¼€å‘
- è‡ªåŠ¨å†…å­˜ç®¡ç†

**ç¤ºä¾‹**ï¼š
```cpp
// æ¨èï¼šç›´æ¥æ„é€ é£æ ¼
auto loss_fn = CrossEntropyLoss(0.1f);
loss_fn.set_backend(backend);
loss_fn.train();

float loss = loss_fn.criterion(logits, targets);
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ™ºèƒ½æŒ‡é’ˆé£æ ¼ | ç›´æ¥æ„é€ é£æ ¼ | æ€§èƒ½æ¯” |
|------|-------------|-------------|--------|
| **æ„é€ æ—¶é—´** | åŸºå‡† | åŸºå‡† | 100% |
| **è¿è¡Œæ—¶æ€§èƒ½** | åŸºå‡† | åŸºå‡† | 100% |
| **å†…å­˜ä½¿ç”¨** | åŸºå‡† | åŸºå‡† | 100% |
| **ä»£ç ç®€æ´æ€§** | â­â­â­ | â­â­â­â­â­ | +67% |
| **å¼€å‘æ•ˆç‡** | â­â­â­â­ | â­â­â­â­â­ | +25% |

## ç»§æ‰¿æŒ‡å—

### V2.2.1æ´¾ç”Ÿç±»æ„é€ å‡½æ•°

```cpp
// V2.2.1ï¼šæ¨èæ„é€ å‡½æ•°æ¨¡å¼
class MyLoss : public Loss {
public:
    // ç®€åŒ–æ„é€ å‡½æ•°
    explicit MyLoss(float custom_param = 0.0f, bool training_mode = true)
        : Loss(training_mode), custom_param_(custom_param) {}

    // æˆ–è€…æ”¯æŒå»¶è¿Ÿæ„é€ çš„å·¥å‚æ–¹æ³•
    static std::shared_ptr<MyLoss> create(float custom_param = 0.0f) {
        return std::make_shared<MyLoss>(custom_param);
    }

    static MyLoss create_direct(float custom_param = 0.0f) {
        return MyLoss(custom_param);
    }

private:
    float custom_param_;
};
```

### å¿…é¡»å®ç°çš„æ–¹æ³•

æ´¾ç”Ÿç±»å¿…é¡»å®ç°ä»¥ä¸‹çº¯è™šå‡½æ•°ï¼š

```cpp
// æŸå¤±å‡½æ•°ç±»å‹åç§°
virtual std::string type_name() const override = 0;

// æ ¸å¿ƒï¼šæŸå¤±+æ¢¯åº¦è®¡ç®—åˆäºŒä¸ºä¸€
virtual float criterion(Tensor& logits, const Tensor& target,
                      const std::string& reduction = "mean") override = 0;
```

### V2.2.1å®ç°ç¤ºä¾‹

```cpp
class MSELoss : public Loss {
public:
    // V2.2.1ï¼šç®€åŒ–æ„é€ å‡½æ•°
    explicit MSELoss(float reduction_factor = 1.0f, bool training_mode = true)
        : Loss(training_mode), reduction_factor_(reduction_factor) {}

    // V2.2.1ï¼šå·¥å‚æ–¹æ³•æ”¯æŒ
    static std::shared_ptr<MSELoss> create_ptr(float reduction_factor = 1.0f) {
        return std::make_shared<MSELoss>(reduction_factor);
    }

    static MSELoss create(float reduction_factor = 1.0f) {
        return MSELoss(reduction_factor);
    }

    std::string type_name() const override {
        return "MSELoss";
    }

    float criterion(Tensor& logits, const Tensor& target,
                   const std::string& reduction = "mean") override {
        auto backend = get_backend();

        // è®¡ç®—å‡æ–¹è¯¯å·®
        Tensor diff = backend->subtract(logits, target);
        Tensor squared = backend->multiply(diff, diff);
        Tensor mse = backend->sum(squared, /*dim=*/{0, 1});

        float loss_value = mse.item<float>() * reduction_factor_;

        // æ ¹æ®reductionå¤„ç†
        if (reduction == "mean") {
            loss_value /= (logits.shape().numel() / logits.shape().dim(0));
        }

        // è®­ç»ƒæ¨¡å¼ä¸‹è®¡ç®—æ¢¯åº¦
        if (is_training()) {
            Tensor grad = backend->multiply(diff, 2.0f * reduction_factor_);
            if (reduction == "mean") {
                float scale = 1.0f / logits.shape().numel();
                backend->mul_inplace(grad, scale);
            }

            if (!logits.has_grad()) {
                logits.set_grad(backend->zeros_like(logits));
            }
            backend->copy_into(grad, logits.grad());
        }

        return loss_value;
    }

private:
    float reduction_factor_;
};
```

## V2.2.1æœ€ä½³å®è·µ

### 1. V2.2.1æ„é€ æ–¹å¼é€‰æ‹©

```cpp
// æ¨èï¼šæ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©æ„é€ é£æ ¼

// å¤§å‹ç”Ÿäº§é¡¹ç›® - æ™ºèƒ½æŒ‡é’ˆé£æ ¼
class ProductionTrainer {
private:
    std::shared_ptr<CrossEntropyLoss> loss_fn_;
public:
    ProductionTrainer() {
        loss_fn_ = std::make_shared<CrossEntropyLoss>(0.1f);
        loss_fn_->set_backend(BackendManager::get_cpu_backend());
    }
};

// å¿«é€ŸåŸå‹å¼€å‘ - ç›´æ¥æ„é€ é£æ ¼
void quick_experiment() {
    auto loss_fn = CrossEntropyLoss(0.1f);
    loss_fn.set_backend(BackendManager::get_cpu_backend());
    // ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€æ‰‹åŠ¨å†…å­˜ç®¡ç†
}
```

### 2. V2.2.1åç«¯ç®¡ç†

```cpp
// V2.2.1ï¼šæ¨èçš„åç«¯è®¾ç½®æ¨¡å¼
auto loss_fn = CrossEntropyLoss(0.1f);
auto backend = BackendManager::get_cpu_backend();
loss_fn.set_backend(backend);  // å»¶è¿Ÿè®¾ç½®ï¼Œæ›´åŠ çµæ´»

// ç¡®ä¿ä¸Modelä½¿ç”¨ç›¸åŒåç«¯
auto model = Model::create("MLP", modules...);
model.set_backend(backend);  // ç»Ÿä¸€åç«¯
```

### 3. V2.2.1æ¨¡å¼ç®¡ç†

```cpp
// V2.2.1ï¼šç®€åŒ–çš„æ¨¡å¼ç®¡ç†
auto loss_fn = CrossEntropyLoss();
loss_fn.set_backend(backend);

// æ˜ç¡®è®¾ç½®æ¨¡å¼
loss_fn.eval();   // æ¨ç†æ—¶
float val_loss = loss_fn.criterion(logits, targets);

loss_fn.train();  // è®­ç»ƒæ—¶
float train_loss = loss_fn.criterion(logits, targets);
```

### 4. V2.2.1Taské›†æˆ

```cpp
// V2.2.1ï¼šTask APIä¸­çš„å®Œç¾é›†æˆ

// æ™ºèƒ½æŒ‡é’ˆé£æ ¼
auto loss_fn_ptr = std::make_shared<CrossEntropyLoss>(0.1f);
loss_fn_ptr->set_backend(backend);
auto trainer_ptr = std::make_shared<Trainer>(model, loss_fn_ptr, optimizer, scheduler);
auto task = std::make_shared<Task>(model, dataset, trainer_ptr);

// ç›´æ¥æ„é€ é£æ ¼
auto loss_fn = CrossEntropyLoss(0.1f);
loss_fn.set_backend(backend);
auto trainer = Trainer(model, loss_fn, optimizer, scheduler);
auto task = Task(model, dataset, trainer);
```

## æ€§èƒ½ç‰¹æ€§

### å†…å­˜æ•ˆç‡

| ç‰¹æ€§ | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| å°±åœ°æ¢¯åº¦å­˜å‚¨ | ç›´æ¥åœ¨è¾“å…¥å¼ é‡ä¸Šå­˜å‚¨æ¢¯åº¦ | é¿å…é¢å¤–å†…å­˜åˆ†é… |
| æ¨¡å¼æ„ŸçŸ¥ | è¯„ä¼°æ¨¡å¼è·³è¿‡æ¢¯åº¦è®¡ç®— | èŠ‚çœè®¡ç®—èµ„æº |
| è®¡ç®—å¤ç”¨ | è®­ç»ƒæ¨¡å¼ä¸‹å¤ç”¨ä¸­é—´ç»“æœ | å‡å°‘é‡å¤è®¡ç®— |
| V2.2.1æ„é€ ä¼˜åŒ– | å»¶è¿Ÿbackendè®¾ç½®ï¼Œå‡å°‘æ„é€ å¼€é”€ | æå‡åˆå§‹åŒ–æ•ˆç‡ |

### è®¡ç®—å¤æ‚åº¦

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |
|------|------------|------------|
| æŸå¤±è®¡ç®— | O(N) | O(1) |
| æ¢¯åº¦è®¡ç®— | O(N) | O(1) |
| æ€»ä½“å¤æ‚åº¦ | O(N) | O(1) |

å…¶ä¸­Næ˜¯è¾“å…¥å¼ é‡çš„å…ƒç´ æ€»æ•°ã€‚

## é”™è¯¯å¤„ç†

### V2.2.1å¸¸è§å¼‚å¸¸

```cpp
try {
    // V2.2.1ï¼šç®€åŒ–çš„é”™è¯¯å¤„ç†
    CrossEntropyLoss loss_fn(0.1f);

    // é”™è¯¯ï¼šæœªè®¾ç½®åç«¯ï¼ˆV2.2.1åå¿…é¡»æ˜¾å¼è®¾ç½®ï¼‰
    // auto loss = loss_fn.criterion(logits, targets);  // TRException

    // V2.2.1ï¼šæ­£ç¡®çš„è®¾ç½®æ–¹å¼
    loss_fn.set_backend(BackendManager::get_cpu_backend());
    auto loss = loss_fn.criterion(logits, targets);  // æ­£å¸¸å·¥ä½œ

} catch (const TRException& e) {
    std::cerr << "Loss computation error: " << e.what() << std::endl;
}
```

### é”™è¯¯ç±»å‹

1. **åç«¯æœªè®¾ç½®**ï¼šV2.2.1åå¿…é¡»åœ¨è°ƒç”¨`criterion()`å‰è°ƒç”¨`set_backend()`
2. **å½¢çŠ¶ä¸åŒ¹é…**ï¼šlogitså’Œtargetsçš„batch_sizeå¿…é¡»ä¸€è‡´
3. **æ•°æ®ç±»å‹é”™è¯¯**ï¼štargetå¿…é¡»æ˜¯INT32ç±»åˆ«æ ‡ç­¾æˆ–FP32 one-hotç¼–ç 
4. **æ— æ•ˆå‚æ•°**ï¼šreductionå¿…é¡»æ˜¯"mean"æˆ–"sum"

## é™åˆ¶å’Œå½“å‰çŠ¶æ€

### å½“å‰é™åˆ¶

1. **åç«¯æ”¯æŒ**ï¼šç›®å‰ä»…æ”¯æŒCPUåç«¯ï¼ˆå¯æ‰©å±•è‡³CUDAï¼‰
2. **æ•°æ®ç±»å‹**ï¼šä¸»è¦æ”¯æŒFP32è®¡ç®—ï¼Œéƒ¨åˆ†æ”¯æŒINT8
3. **æ¢¯åº¦å­˜å‚¨**ï¼šæ¢¯åº¦å­˜å‚¨åœ¨è¾“å…¥å¼ é‡ä¸­ï¼Œå¯èƒ½å½±å“è¾“å…¥å¼ é‡ä½¿ç”¨

### æœªæ¥å¢å¼º

1. **å¤šåç«¯æ”¯æŒ**ï¼šæ‰©å±•è‡³CUDAå’Œå…¶ä»–ä¸“ç”¨åç«¯
2. **æ›´å¤šæŸå¤±å‡½æ•°**ï¼šå®ç°æ›´å¤šæ·±åº¦å­¦ä¹ å¸¸ç”¨æŸå¤±å‡½æ•°
3. **é«˜çº§ç‰¹æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰æƒé‡ã€æ©ç æŸå¤±ç­‰
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šSIMDæŒ‡ä»¤ä¼˜åŒ–ï¼Œå¤šçº¿ç¨‹å¹¶è¡Œ

## ç±»å®šä¹‰

```cpp
namespace tr {
class Loss {
public:
    // V2.2.1ï¼šç®€åŒ–æ„é€ å‡½æ•°
    explicit Loss(bool training_mode = true);
    virtual ~Loss() = default;

    // æ¨¡å¼æ§åˆ¶
    virtual void train();
    virtual void eval();
    virtual bool is_training() const;

    // æ ¸å¿ƒæ¥å£
    virtual float criterion(Tensor& logits, const Tensor& target,
                          const std::string& reduction = "mean") = 0;

    // åç«¯ç®¡ç†
    virtual void set_backend(std::shared_ptr<Backend> backend);
    virtual std::shared_ptr<Backend> get_backend() const;

    // ä¿¡æ¯æŸ¥è¯¢
    virtual std::string type_name() const = 0;

protected:
    // V2.2.1ï¼šæˆå‘˜å˜é‡
    std::shared_ptr<Backend> backend_;  // åç«¯æŒ‡é’ˆ
    bool training_mode_;                // è®­ç»ƒ/è¯„ä¼°æ¨¡å¼æ ‡å¿—
};
}
```

## æ–‡ä»¶

- **å¤´æ–‡ä»¶**ï¼š`include/tech_renaissance/trainer/loss.h`
- **å®ç°**ï¼š`src/trainer/loss.cpp`

## ç›¸å…³æ–‡æ¡£

- [å¯¹è±¡æ„é€ é£æ ¼æŒ‡å—](guide.md) - V2.2.1æ–°å¢ï¼šè¯¦ç»†è¯´æ˜ä¸¤ç§æ„é€ é£æ ¼
- [CrossEntropyLossæ–‡æ¡£](cross_entropy_loss.md) - V2.2.1æ›´æ–°ï¼šç®€åŒ–æ„é€ å‡½æ•°
- [Taské«˜çº§APIæ–‡æ¡£](task.md) - V2.2.1æ›´æ–°ï¼šæ”¯æŒåŒé‡æ„é€ é£æ ¼
- [ModuleåŸºç±»æ–‡æ¡£](model/module.md)
- [Linearå±‚æ–‡æ¡£](model/linear.md)
- [Backendæ–‡æ¡£](backend/backend.md)
- [Tensoræ–‡æ¡£](data/tensor.md)