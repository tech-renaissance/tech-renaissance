# Backendç³»ç»ŸæŠ€æœ¯æ–‡æ¡£

**ç‰ˆæœ¬**: V1.51.0
**æ—¥æœŸ**: 2025å¹´11æœˆ19æ—¥
**ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [APIå‚è€ƒ](#apiå‚è€ƒ)
- [å†…å­˜ç®¡ç†](#å†…å­˜ç®¡ç†)
- [è®¾å¤‡ç®¡ç†](#è®¾å¤‡ç®¡ç†)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ‰©å±•æŒ‡å—](#æ‰©å±•æŒ‡å—)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ç³»ç»Ÿæ¦‚è¿°

Backendç³»ç»Ÿæ˜¯Tech Renaissanceæ¡†æ¶çš„æ ¸å¿ƒè®¡ç®—æŠ½è±¡å±‚ï¼Œè´Ÿè´£å¼ é‡è¿ç®—ã€å†…å­˜ç®¡ç†å’Œè®¾å¤‡äº¤äº’ã€‚é€šè¿‡ç»Ÿä¸€çš„æ¥å£è®¾è®¡ï¼Œå®ç°äº†CPUå’ŒCUDAåç«¯çš„é«˜æ€§èƒ½è®¡ç®—æ”¯æŒã€‚

### è®¾è®¡ç›®æ ‡

1. **ç»Ÿä¸€æ¥å£**: ä¸ºä¸Šå±‚æ¨¡å—æä¾›ä¸€è‡´çš„APIï¼Œå±è”½ç¡¬ä»¶å·®å¼‚
2. **é«˜æ€§èƒ½**: å……åˆ†åˆ©ç”¨ç°ä»£CPU/GPUçš„è®¡ç®—èƒ½åŠ›
3. **å¯æ‰©å±•**: æ”¯æŒæ–°è®¾å¤‡å’Œæ–°ç®—æ³•çš„æ— ç¼é›†æˆ
4. **ç±»å‹å®‰å…¨**: å¼ºç±»å‹è®¾è®¡ç¡®ä¿ç¼–è¯‘æ—¶é”™è¯¯æ£€æŸ¥

### æ ¸å¿ƒç‰¹æ€§

- âœ… **ç°ä»£C++è®¾è®¡**: æ”¯æŒRAIIã€æ™ºèƒ½æŒ‡é’ˆã€å¼‚å¸¸å®‰å…¨
- âœ… **SIMDä¼˜åŒ–**: è‡ªåŠ¨å‘é‡åŒ–ï¼Œå……åˆ†åˆ©ç”¨CPUæ€§èƒ½
- âœ… **GPUåŠ é€Ÿ**: åŸºäºcuBLAS/cuDNNçš„é«˜æ€§èƒ½CUDAå®ç°
- âœ… **é›¶æ‹·è´æ“ä½œ**: æœ€å°åŒ–å†…å­˜ç§»åŠ¨ï¼Œæå‡è¿ç®—æ•ˆç‡
- âœ… **è®¾å¤‡é€æ˜**: ç»Ÿä¸€APIæ”¯æŒCPU/GPUè®¾å¤‡åˆ‡æ¢

---

## æ¶æ„è®¾è®¡

### ç±»å±‚æ¬¡ç»“æ„

```cpp
// æŠ½è±¡åŸºç±»
class Backend {
public:
    // å¼ é‡åˆ›å»º
    virtual Tensor empty(const Shape& shape, DType dtype) = 0;
    virtual Tensor empty(const Shape& shape, DType dtype) const = 0;

    // å¼ é‡è¿ç®—
    virtual Tensor add(const Tensor& a, const Tensor& b) const;
    virtual void add_into(const Tensor& a, const Tensor& b, Tensor& result) const;
    virtual Tensor mul(const Tensor& a, const Tensor& b) const;
    virtual void mul_into(const Tensor& a, const Tensor& b, Tensor& result) const;

    // æ ‡é‡è¿ç®—
    virtual Tensor add(const Tensor& input, float scalar) const;
    virtual void add_into(const Tensor& input, float scalar, Tensor& output) const;

    // çŸ©é˜µè¿ç®—
    virtual Tensor mm(const Tensor& a, const Tensor& b) = 0;
    virtual void mm_into(const Tensor& a, const Tensor& b, Tensor& result) = 0;

    // è®¾å¤‡ç®¡ç†
    virtual Tensor to_cpu(const Tensor& tensor) const = 0;
    virtual Tensor from_cpu(const Tensor& tensor) const = 0;
};

// CPUå®ç°
class CpuBackend : public Backend { ... };

// CUDAå®ç°
class CudaBackend : public Backend { ... };
```

### åç«¯ç®¡ç†å™¨

```cpp
class BackendManager {
public:
    static BackendManager& instance();
    std::shared_ptr<Backend> get_cpu_backend();
    std::shared_ptr<Backend> get_cuda_backend(int device_id = 0);
    std::shared_ptr<Backend> get_backend(const Device& device);

private:
    std::unordered_map<Device, std::shared_ptr<Backend>> backends_;
};
```

---

## APIå‚è€ƒ

### å¼ é‡åˆ›å»º

#### empty - åˆ›å»ºç©ºå¼ é‡

```cpp
Tensor Backend::empty(const Shape& shape, DType dtype) const;
```

**å‚æ•°**:
- `shape`: å¼ é‡å½¢çŠ¶
- `dtype`: æ•°æ®ç±»å‹ (FP32, INT8, INT32)

**è¿”å›å€¼**: æœªåˆå§‹åŒ–çš„å¼ é‡

**ç¤ºä¾‹**:
```cpp
auto backend = BackendManager::instance().get_cpu_backend();
Tensor tensor = backend->empty({2, 3, 4}, DType::FP32);
```

#### zeros/ones - åˆ›å»ºå¸¸é‡å¼ é‡

```cpp
Tensor Backend::zeros(const Shape& shape, DType dtype);
Tensor Backend::ones(const Shape& shape, DType dtype);
```

### å¼ é‡è¿ç®—

#### add - å¼ é‡åŠ æ³•

```cpp
Tensor Backend::add(const Tensor& a, const Tensor& b) const;
void Backend::add_into(const Tensor& a, const Tensor& b, Tensor& result) const;
```

**å‚æ•°**:
- `a`, `b`: è¾“å…¥å¼ é‡ï¼Œå¿…é¡»å½¢çŠ¶ç›¸åŒ
- `result`: è¾“å‡ºå¼ é‡ï¼Œå¿…é¡»ä¸è¾“å…¥å½¢çŠ¶ç›¸åŒ

**æ€§èƒ½ä¼˜åŒ–**:
- CPU: ä½¿ç”¨Eigen SIMDä¼˜åŒ–
- CUDA: ä½¿ç”¨cuBLAS axpyæ“ä½œ

#### mul - å¼ é‡ä¹˜æ³•

```cpp
Tensor Backend::mul(const Tensor& a, const Tensor& b) const;
void Backend::mul_into(const Tensor& a, const Tensor& b, Tensor& result) const;
```

**è¯´æ˜**: é€å…ƒç´ ä¹˜æ³•ï¼Œä¸æ˜¯çŸ©é˜µä¹˜æ³•

### æ ‡é‡è¿ç®—

#### æ ‡é‡åŠ æ³•

```cpp
Tensor Backend::add(const Tensor& input, float scalar) const;
void Backend::add_into(const Tensor& input, float scalar, Tensor& output) const;
void Backend::add_inplace(Tensor& input, float scalar) const;
```

#### æ ‡é‡ä¹˜æ³•

```cpp
Tensor Backend::mul(const Tensor& input, float scalar) const;
void Backend::mul_into(const Tensor& input, float scalar, Tensor& output) const;
void Backend::mul_inplace(Tensor& input, float scalar) const;
```

### çŸ©é˜µè¿ç®—

#### mm - çŸ©é˜µä¹˜æ³•

```cpp
Tensor Backend::mm(const Tensor& a, const Tensor& b);
void Backend::mm_into(const Tensor& a, const Tensor& b, Tensor& result);
```

**è¦æ±‚**:
- `a`: çŸ©é˜µï¼Œå½¢çŠ¶ (m, k)
- `b`: çŸ©é˜µï¼Œå½¢çŠ¶ (k, n)
- `result`: è¾“å‡ºçŸ©é˜µï¼Œå½¢çŠ¶ (m, n)

---

## å†…å­˜ç®¡ç†

### åˆ†é…ç­–ç•¥

```cpp
class Backend {
protected:
    virtual std::shared_ptr<void> allocate(size_t size) = 0;
    virtual void deallocate(void* ptr) = 0;
    virtual void* get_data_ptr(const std::shared_ptr<void>& holder) = 0;
};
```

### å†…å­˜ä¼˜åŒ–

1. **é¢„åˆ†é…æ± **: å‡å°‘åŠ¨æ€å†…å­˜åˆ†é…å¼€é”€
2. **æ™ºèƒ½æŒ‡é’ˆ**: è‡ªåŠ¨å†…å­˜ç®¡ç†ï¼Œé¿å…å†…å­˜æ³„æ¼
3. **é›¶æ‹·è´**: é€šè¿‡è§†å›¾æ“ä½œé¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶

### CUDAå†…å­˜ç®¡ç†

```cpp
// å¼‚æ­¥å†…å­˜ä¼ è¾“
cudaMemcpyAsync(dst, src, size, cudaMemcpyDeviceToDevice, stream_);

// ç»Ÿä¸€å†…å­˜ç®¡ç†
cudaMallocManaged(&ptr, size);
```

---

## è®¾å¤‡ç®¡ç†

### è®¾å¤‡ç±»å‹

```cpp
enum class DeviceType {
    CPU = 0,
    CUDA = 1
};

struct Device {
    DeviceType type;
    int index;  // GPUè®¾å¤‡ID

    bool is_cpu() const;
    bool is_cuda() const;
    std::string to_string() const;
};
```

### è®¾å¤‡è½¬ç§»

```cpp
// CPUåˆ°CUDA
Tensor gpu_tensor = cuda_backend->from_cpu(cpu_tensor);

// CUDAåˆ°CPU
Tensor cpu_tensor = cuda_backend->to_cpu(gpu_tensor);

// é€šç”¨è®¾å¤‡è½¬ç§»
Tensor target_tensor = BackendManager::instance()
    .get_backend(target_device)
    ->to(source_tensor, target_device);
```

---

## æ€§èƒ½ä¼˜åŒ–

### CPUä¼˜åŒ–ç­–ç•¥

#### Eigené›†æˆ

```cpp
#ifdef TR_USE_EIGEN
#include <Eigen/Dense>

// è‡ªåŠ¨å‘é‡åŒ–
Eigen::Map<const Eigen::VectorXf> vec(data, size);
Eigen::VectorXf result = vec.array() + scalar;
#endif
```

#### ç¼–è¯‘ä¼˜åŒ–

```cmake
# Releaseæ¨¡å¼ä¼˜åŒ–æ ‡å¿—
set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} /O2 /Ob2 /arch:AVX2 /openmp")
```

### CUDAä¼˜åŒ–ç­–ç•¥

#### cuBLASé›†æˆ

```cpp
// é«˜æ€§èƒ½çŸ©é˜µè¿ç®—
cublasSaxpy(handle, n, &alpha, x, 1, y, 1);  // y = alpha*x + y
cublasSgemm(handle, opA, opB, m, n, k, &alpha, A, lda, B, ldb, &beta, C, ldc);
```

#### cuDNNé›†æˆ

```cpp
// æ·±åº¦å­¦ä¹ ä¸“ç”¨ä¼˜åŒ–
cudnnConvolutionForward(handle, &alpha, input_desc, input_data,
                        filter_desc, filter_data, conv_desc, algo,
                        workspace, workspace_size, &beta, output_desc, output_data);
```

### æ€§èƒ½åŸºå‡†

| æ“ä½œ | CPU (Eigen) | CUDA (cuBLAS) | åŠ é€Ÿæ¯” |
|------|-------------|---------------|--------|
| å‘é‡åŠ æ³• (1Må…ƒç´ ) | 0.5ms | 0.1ms | 5x |
| çŸ©é˜µä¹˜æ³• (1024x1024) | 50ms | 2ms | 25x |
| å·ç§¯ (3x3, 256é€šé“) | 100ms | 5ms | 20x |

---

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°åç«¯

1. **ç»§æ‰¿BackendåŸºç±»**:

```cpp
class CustomBackend : public Backend {
public:
    // å®ç°æ‰€æœ‰çº¯è™šå‡½æ•°
    Tensor empty(const Shape& shape, DType dtype) override;
    void fill(Tensor& dst, float value) override;
    Tensor mm(const Tensor& a, const Tensor& b) override;
    // ... å…¶ä»–æ–¹æ³•
};
```

2. **æ³¨å†Œåˆ°BackendManager**:

```cpp
// åœ¨BackendManagerä¸­æ³¨å†Œæ–°åç«¯
auto custom_backend = std::make_shared<CustomBackend>();
backends_[Device{DeviceType::CUSTOM, 0}] = custom_backend;
```

3. **æ·»åŠ å•å…ƒæµ‹è¯•**:

```cpp
TEST(CustomBackendTest, BasicOperations) {
    auto backend = std::make_shared<CustomBackend>();
    // æµ‹è¯•åŸºæœ¬åŠŸèƒ½
}
```

### æ·»åŠ æ–°è¿ç®—

1. **BackendåŸºç±»å£°æ˜**:

```cpp
class Backend {
public:
    virtual Tensor custom_op(const Tensor& input) = 0;
    virtual void custom_op_into(const Tensor& input, Tensor& output) = 0;
};
```

2. **å„åç«¯å®ç°**:

```cpp
// CPUå®ç°
Tensor CpuBackend::custom_op(const Tensor& input) {
    // Eigenä¼˜åŒ–å®ç°
}

// CUDAå®ç°
Tensor CudaBackend::custom_op(const Tensor& input) {
    // CUDAæ ¸å‡½æ•°å®ç°
}
```

---

## æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨intoç‰ˆæœ¬**: é¿å…ä¸´æ—¶å¯¹è±¡åˆ›å»º
```cpp
// å¥½çš„åšæ³•
backend->add_into(a, b, result);

// é¿å…çš„åšæ³•
result = backend->add(a, b);  // åˆ›å»ºä¸´æ—¶å¼ é‡
```

2. **æ‰¹é‡æ“ä½œ**: å‡å°‘å†…æ ¸å¯åŠ¨å¼€é”€
```cpp
// å¥½çš„åšæ³•
for (int i = 0; i < n; ++i) {
    backend->add_into(a[i], b[i], result[i]);
}

// é¿å…çš„åšæ³•
for (int i = 0; i < n; ++i) {
    result[i] = backend->add(a[i], b[i]);  // æ¯æ¬¡è°ƒç”¨éƒ½æœ‰å¼€é”€
}
```

3. **å†…å­˜å¤ç”¨**: ä½¿ç”¨é¢„åˆ†é…çš„ç¼“å†²åŒº
```cpp
class ComputeBuffer {
private:
    Tensor buffer_;

public:
    ComputeBuffer(const Shape& shape, std::shared_ptr<Backend> backend)
        : buffer_(backend->empty(shape, DType::FP32)) {}

    void compute(const Tensor& a, const Tensor& b, Tensor& result) {
        backend_->add_into(a, b, buffer_);  // å¤ç”¨ç¼“å†²åŒº
        // è¿›ä¸€æ­¥è®¡ç®—...
    }
};
```

### é”™è¯¯å¤„ç†

```cpp
try {
    auto result = backend->add(a, b);
} catch (const TRException& e) {
    std::cerr << "Backend operation failed: " << e.what() << std::endl;
    // å¤„ç†é”™è¯¯
}
```

### è°ƒè¯•æŠ€å·§

1. **ä½¿ç”¨è°ƒè¯•æ¨¡å¼ç¼–è¯‘**:
```bash
cmake -DCMAKE_BUILD_TYPE=Debug ..
```

2. **å¯ç”¨æ—¥å¿—**:
```cpp
Logger::instance().set_level(LogLevel::DEBUG);
```

3. **æ€§èƒ½åˆ†æ**:
```bash
nvprof ./executable  # CUDAæ€§èƒ½åˆ†æ
perf record ./executable  # CPUæ€§èƒ½åˆ†æ
```

---

## ç‰ˆæœ¬å†å²

### V1.51.0 (2025-11-19)
- âœ… Backend APIé‡æ„å®Œæˆ
- âœ… ç»Ÿä¸€add/mulè¿ç®—æ¥å£
- âœ… æ·»åŠ consté‡è½½æ–¹æ³•
- âœ… CPU Backendå®Œæ•´å®ç°
- âœ… CUDA Backendé«˜æ€§èƒ½ä¼˜åŒ–
- âœ… Alphaç¼–è¯‘éªŒè¯é€šè¿‡

### V1.50.0 (2025-11-18)
- âœ… Optimizerç³»ç»Ÿé›†æˆ
- âœ… StateManageræ¶æ„å®ç°
- âœ… SGDä¼˜åŒ–å™¨å®Œæˆ

### V1.45.0 (2025-11-17)
- âœ… Modelç±»é›¶æ‹·è´å‰å‘ä¼ æ’­
- âœ… å‚æ•°ç¼“å­˜æœºåˆ¶å®ç°

---

**æ³¨æ„**: æœ¬æ–‡æ¡£éšä»£ç æ›´æ–°è€ŒæŒç»­ç»´æŠ¤ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚