# å¼ é‡-åç«¯ç³»ç»Ÿæ–‡æ¡£

## æ¦‚è¿°

å¼ é‡-åç«¯ç³»ç»Ÿæ˜¯æŠ€æœ¯è§‰é†’æ¡†æ¶çš„æ ¸å¿ƒæ¶æ„ä¹‹ä¸€ï¼Œé‡‡ç”¨**åç«¯ç®¡ç†å­˜å‚¨**çš„è®¾è®¡ç†å¿µã€‚è¿™ä¸ªç³»ç»Ÿå½»åº•åˆ†ç¦»äº†**å¼ é‡å…ƒæ•°æ®**ä¸**å®é™…æ•°æ®å­˜å‚¨**ï¼Œæä¾›äº†é«˜åº¦çµæ´»çš„å¤šåç«¯æ”¯æŒã€‚åœ¨V1.45.0ç‰ˆæœ¬ä¸­ï¼Œç³»ç»Ÿè¿›ä¸€æ­¥å¢å¼ºäº†æ¢¯åº¦ç®¡ç†èƒ½åŠ›ï¼Œå¹¶å®Œå–„äº†Moduleæ¡†æ¶çš„é›†æˆã€‚

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼š
- **å¼ é‡ç±»**ï¼šçº¯å…ƒæ•°æ®å®¹å™¨ï¼Œæ”¯æŒæ¢¯åº¦ç®¡ç†ï¼Œä¸æŒæœ‰å®é™…æ•°æ®
- **åç«¯ç±»**ï¼šç®¡ç†å†…å­˜åˆ†é…ã€æ•°æ®è®¿é—®å’Œè®¡ç®—æ“ä½œ
- **å­˜å‚¨ç±»**ï¼šRAIIå†…å­˜ç®¡ç†ï¼Œä¸ç‰¹å®šåç«¯ç»‘å®š
- **BackendManager**ï¼šå•ä¾‹æ¨¡å¼ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰åç«¯å®ä¾‹
- **æ¢¯åº¦ç³»ç»Ÿ**ï¼šå»¶è¿Ÿåˆ†é…çš„æ¢¯åº¦å¼ é‡ç®¡ç†ï¼ˆV1.45.0æ–°å¢ï¼‰

**ç‰ˆæœ¬**: V1.46.3
**æ›´æ–°æ—¥æœŸ**: 2025å¹´11æœˆ17æ—¥
**ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ

## æœ€æ–°å®ŒæˆçŠ¶æ€

âœ… **V1.46.3å®Œæˆ - ä»£ç è§„èŒƒä¼˜åŒ–å’Œç±»å‹å®‰å…¨å¼ºåŒ–**:
- Backendæ„é€ å‡½æ•°ç³»ç»Ÿç»Ÿä¸€åŒ– - æ•´ä¸ªBackendä½“ç³»çš„æ„é€ å‡½æ•°è®¾è®¡ç»Ÿä¸€
- å¼ é‡-åç«¯ç³»ç»Ÿç±»å‹å®‰å…¨å¢å¼º - Model::create_ptræ™ºèƒ½æŒ‡é’ˆä½¿ç”¨ç¡®è®¤
- Alphaç¼–è¯‘éªŒè¯é€šè¿‡ - æ•´ä¸ªç³»ç»Ÿç¼–è¯‘æµ‹è¯•é€šè¿‡
- ä»£ç è§„èŒƒç»Ÿä¸€ - Backendã€CpuBackendã€CudaBackendæ¥å£ä¸€è‡´åŒ–

âœ… **V1.45.0å®Œæˆ - æ¢¯åº¦ç®¡ç†ç³»ç»Ÿ**:
- å»¶è¿Ÿåˆ†é…ç­–ç•¥ - åªæœ‰é¦–æ¬¡è®¿é—®æ—¶æ‰åˆ›å»ºæ¢¯åº¦å¼ é‡ï¼Œé¿å…é»˜è®¤å†…å­˜ç¿»å€
- æ™ºèƒ½æŒ‡é’ˆç®¡ç† - ä½¿ç”¨`std::shared_ptr<Tensor>`è‡ªåŠ¨ç®¡ç†æ¢¯åº¦ç”Ÿå‘½å‘¨æœŸ
- Moduleé›†æˆ - ä¸Moduleç³»ç»Ÿå®Œç¾é›†æˆï¼Œæ”¯æŒè‡ªåŠ¨å‚æ•°æ¢¯åº¦ç®¡ç†
- å†…å­˜ä¼˜åŒ– - æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶çš„å†…å­˜å ç”¨

## ğŸ†• V1.45.0é‡å¤§æ›´æ–°ï¼šæ¢¯åº¦ç®¡ç†ç³»ç»Ÿ

### ğŸ¯ æ¢¯åº¦ç®¡ç†è®¾è®¡ç›®æ ‡

åœ¨V1.45.0ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ä¸ºTensorç±»æ·»åŠ äº†å®Œæ•´çš„æ¢¯åº¦ç®¡ç†ç³»ç»Ÿï¼š

1. **å»¶è¿Ÿåˆ†é…ç­–ç•¥**ï¼šåªæœ‰é¦–æ¬¡è®¿é—®æ—¶æ‰åˆ›å»ºæ¢¯åº¦å¼ é‡ï¼Œé¿å…é»˜è®¤å†…å­˜ç¿»å€
2. **æ™ºèƒ½æŒ‡é’ˆç®¡ç†**ï¼šä½¿ç”¨`std::shared_ptr<Tensor>`è‡ªåŠ¨ç®¡ç†æ¢¯åº¦ç”Ÿå‘½å‘¨æœŸ
3. **Moduleé›†æˆ**ï¼šä¸Moduleç³»ç»Ÿå®Œç¾é›†æˆï¼Œæ”¯æŒè‡ªåŠ¨å‚æ•°æ¢¯åº¦ç®¡ç†
4. **å†…å­˜ä¼˜åŒ–**ï¼šæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶çš„å†…å­˜å ç”¨

### ğŸ”§ æ¢¯åº¦ç®¡ç†æ ¸å¿ƒæ¥å£

#### æ¢¯åº¦è®¿é—®æ–¹æ³•
```cpp
class Tensor {
    // è·å–æ¢¯åº¦ï¼ˆå»¶è¿Ÿåˆ†é…ï¼‰
    Tensor& grad();
    const Tensor& grad() const;

    // è®¾ç½®æ¢¯åº¦
    void set_grad(const Tensor& grad);
    void set_grad(Tensor&& grad);

    // æ¢¯åº¦çŠ¶æ€æ£€æŸ¥
    bool has_grad() const;

    // æ¢¯åº¦æ¸…é›¶
    void zero_grad();
};
```

#### å»¶è¿Ÿåˆ†é…å®ç°
```cpp
Tensor& Tensor::grad() {
    if (!grad_) {
        // é¦–æ¬¡è®¿é—®æ—¶æ‰åˆ›å»ºæ¢¯åº¦å¼ é‡
        grad_ = std::make_shared<Tensor>(create_and_allocate(shape_, dtype_, device_));
    }
    return *grad_;
}
```

### ğŸ’¡ æ¢¯åº¦ç®¡ç†ä¼˜åŠ¿

1. **å†…å­˜æ•ˆç‡**ï¼šé»˜è®¤ä¸åˆ†é…æ¢¯åº¦ï¼ŒèŠ‚çœ50%å†…å­˜
2. **æŒ‰éœ€åˆ†é…**ï¼šåªæœ‰éœ€è¦æ—¶æ‰åˆ›å»ºæ¢¯åº¦å¼ é‡
3. **è‡ªåŠ¨åŒ–ç®¡ç†**ï¼šæ™ºèƒ½æŒ‡é’ˆè‡ªåŠ¨ç®¡ç†æ¢¯åº¦ç”Ÿå‘½å‘¨æœŸ
4. **Moduleå‹å¥½**ï¼šä¸Moduleç³»ç»Ÿæ— ç¼é›†æˆ

## V1.43.0é‡å¤§æ›´æ–°ï¼šåç«¯åŸºç±»é‡æ„

### ğŸ¯ é‡æ„ç›®æ ‡
åœ¨V1.43.0ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å¯¹BackendåŸºç±»è¿›è¡Œäº†é‡å¤§é‡æ„ï¼š

1. **ä»æŠ½è±¡ç±»æ”¹ä¸ºå¯å®ä¾‹åŒ–ç±»**ï¼šBackendåŸºç±»ä¸å†æ˜¯æŠ½è±¡ç±»ï¼Œè€Œæ˜¯å¯ä»¥å®ä¾‹åŒ–ä½†æŠ›å‡ºå¼‚å¸¸çš„ç±»
2. **ç»Ÿä¸€æ–¹æ³•å£°æ˜æœºåˆ¶**ï¼šå¼•å…¥å®ç³»ç»Ÿï¼Œä¸€è¡Œä»£ç å³å¯å£°æ˜æ–°æ–¹æ³•å¹¶å®ç°é»˜è®¤NotImplementedErrorè¡Œä¸º
3. **ç®€åŒ–åç«¯æ‰©å±•**ï¼šæ–°å¢æ–¹æ³•æ—¶ï¼Œæ— éœ€ä¿®æ”¹æ‰€æœ‰åç«¯ç±»ï¼Œåªéœ€åœ¨BackendåŸºç±»æ·»åŠ å®å®šä¹‰
4. **100%å‘åå…¼å®¹**ï¼šæ‰€æœ‰ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯æ­£å¸¸å·¥ä½œ

### ğŸ”§ åç«¯åŸºç±»å®ä¾‹åŒ–æœºåˆ¶

#### æ„é€ å‡½æ•°è®¾è®¡
```cpp
class Backend {
public:
    /**
     * @brief å…¬å…±æ„é€ å‡½æ•° - é˜²æ­¢ç›´æ¥å®ä¾‹åŒ–
     * @throws TRException ç›´æ¥å®ä¾‹åŒ–æ—¶æŠ›å‡ºå¼‚å¸¸
     */
    Backend() {
        throw TRException("Backend class cannot be instantiated directly! Use specific backend implementations instead.");
    }

protected:
    /**
     * @brief å—ä¿æŠ¤çš„æ„é€ å‡½æ•° - å…è®¸æ´¾ç”Ÿç±»æ„é€ 
     * @param allow_construction æ˜¯å¦å…è®¸æ„é€ ï¼ˆæ´¾ç”Ÿç±»ä¼ trueï¼‰
     */
    Backend(bool allow_construction) {
        if (!allow_construction) {
            throw TRException("Backend class cannot be instantiated directly! Use specific backend implementations instead.");
        }
    }
};
```

### ğŸ“ å®å®šä¹‰ç³»ç»Ÿ

#### å®å®šä¹‰è¯­æ³•
```cpp
/**
 * @brief å®šä¹‰æœªå®ç°æ–¹æ³•çš„å®
 * @param method_name æ–¹æ³•å
 * @param return_type è¿”å›ç±»å‹
 * @param params å‚æ•°åˆ—è¡¨ï¼ˆå¸¦æ‹¬å·ï¼‰
 * @param const_qualifier consté™å®šç¬¦ï¼ˆå¦‚æœæ–¹æ³•ä¸æ˜¯conståˆ™ä¸ºç©ºï¼‰
 * @details ç”Ÿæˆé»˜è®¤æŠ›å‡ºNotImplementedErrorå¼‚å¸¸çš„æ–¹æ³•å®ç°
 */
#define DEFINE_NOT_IMPLEMENTED_METHOD(method_name, return_type, params, const_qualifier) \
    return_type Backend::method_name params const_qualifier { \
        throw NotImplementedError("[" + name() + " " #method_name "] Operation NOT implemented!"); \
    }

/**
 * @brief å®šä¹‰voidè¿”å›ç±»å‹æœªå®ç°æ–¹æ³•çš„å®
 * @param method_name æ–¹æ³•å
 * @param params å‚æ•°åˆ—è¡¨ï¼ˆå¸¦æ‹¬å·ï¼‰
 * @param const_qualifier consté™å®šç¬¦ï¼ˆå¦‚æœæ–¹æ³•ä¸æ˜¯conståˆ™ä¸ºç©ºï¼‰
 */
#define DEFINE_NOT_IMPLEMENTED_VOID_METHOD(method_name, params, const_qualifier) \
    void Backend::method_name params const_qualifier { \
        throw NotImplementedError("[" + name() + " " #method_name "] Operation NOT implemented!"); \
    }
```

## é‡è¦è­¦å‘Šï¼šä¸è¦ç›´æ¥ä½¿ç”¨Tensoræ„é€ å‡½æ•°ï¼

**è­¦å‘Šï¼šTensorç±»çš„æ„é€ å‡½æ•°ä¸ä¼šåˆ†é…å†…å­˜ï¼**

åœ¨Tech Renaissanceæ¡†æ¶ä¸­ï¼ŒTensoræ„é€ å‡½æ•°åªåˆ›å»ºå…ƒæ•°æ®ï¼Œä¸åˆ†é…å®é™…å†…å­˜ã€‚æ‰€æœ‰å¼ é‡å¿…é¡»é€šè¿‡Backendç±»çš„æ–¹æ³•æ¥åˆ›å»ºï¼Œå› ä¸ºBackendä¼šåœ¨åˆ›å»ºåç«‹å³åˆ†é…å†…å­˜ã€‚

**é‡è¦åŒºåˆ«**ï¼š
- **Tensoræ„é€ å‡½æ•°**ï¼šåˆ›å»ºTensorå¯¹è±¡ä½†**ä¸åˆ†é…å†…å­˜**ï¼ˆæ®µé”™è¯¯ï¼ï¼‰
- **Backend::empty()**ï¼š**åˆ†é…å†…å­˜ä½†æœªåˆå§‹åŒ–æ•°æ®**
- **Backend::null_tensor()**ï¼šçœŸæ­£çš„ç©ºå¼ é‡ï¼Œ**ä¸å ç”¨å†…å­˜**

**æ­£ç¡®çš„å¼ é‡åˆ›å»ºæµç¨‹**ï¼š
1. è·å–Backendå®ä¾‹ï¼š`BackendManager::get_cpu_backend().get()`
2. ä½¿ç”¨Backendæ–¹æ³•åˆ›å»ºï¼š`backend->zeros(shape, dtype)`
3. Backendè‡ªåŠ¨åˆ†é…å†…å­˜å¹¶è¿”å›å¯ç”¨å¼ é‡

**é”™è¯¯çš„æ“ä½œï¼ˆä¼šå¯¼è‡´æ®µé”™è¯¯ï¼‰**ï¼š
- ç›´æ¥è°ƒç”¨`Tensor(shape, dtype, device)`æ„é€ å‡½æ•°
- è¯•å›¾è®¿é—®æœªåˆ†é…å†…å­˜çš„å¼ é‡

## ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç”¨æˆ·ä»£ç /ç®—æ³•/Module        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Tensor Class                â”‚  â† å…ƒæ•°æ®ã€è®¾å¤‡ç®¡ç†ã€æ¢¯åº¦ç®¡ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       è½¬æ¢å±‚ï¼ˆBackendæ“ä½œï¼‰             â”‚  â† è®¡ç®—ã€å½¢çŠ¶æ“ä½œã€æ¢¯åº¦ç®¡ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Storageç±»                   â”‚  â† è®¾å¤‡æ— å…³çš„å†…å­˜æŠ½è±¡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Backendç±»                   â”‚  â† å…·ä½“è®¡ç®—å®ç°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Tensorç±» - å…ƒæ•°æ®å’Œè®¾å¤‡ç®¡ç†

**è®¾è®¡ä½ç½®**ï¼šTensorç±»æ˜¯æ ¸å¿ƒç”¨æˆ·æ¥å£ï¼Œè´Ÿè´£å…ƒæ•°æ®ç®¡ç†ã€è®¾å¤‡åè°ƒå’Œæ¢¯åº¦ç®¡ç†ã€‚

**æ ¸å¿ƒæ•°æ®ç»“æ„**ï¼š
```cpp
class Tensor {
    Shape shape_;                          // å½¢çŠ¶ä¿¡æ¯
    DType dtype_;                          // æ•°æ®ç±»å‹
    Device device_;                        // è®¾å¤‡ä¿¡æ¯
    std::shared_ptr<Storage> storage_;     // å†…å­˜å¥æŸ„ï¼ˆå§”æ‰˜ç®¡ç†ï¼‰
    size_t offset_;                        // åç§»ï¼ˆä¸ºè§†å›¾æ”¯æŒé¢„ç•™ï¼‰
    std::shared_ptr<Tensor> grad_;         // V1.45.0æ–°å¢ï¼šæ¢¯åº¦å¼ é‡æŒ‡é’ˆ
};
```

#### V1.45.0æ–°å¢ï¼šæ¢¯åº¦ç®¡ç†åŠŸèƒ½

```cpp
// æ¢¯åº¦è®¿é—®æ¥å£
Tensor& grad();                    // è·å–æ¢¯åº¦ï¼ˆå»¶è¿Ÿåˆ†é…ï¼‰
const Tensor& grad() const;          // è·å–æ¢¯åº¦ï¼ˆconstç‰ˆæœ¬ï¼‰
bool has_grad() const;               // æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
void zero_grad();                   // æ¸…é›¶æ¢¯åº¦

// æ¢¯åº¦è®¾ç½®æ¥å£
void set_grad(const Tensor& grad);     // è®¾ç½®æ¢¯åº¦ï¼ˆå¤åˆ¶ï¼‰
void set_grad(Tensor&& grad);          // è®¾ç½®æ¢¯åº¦ï¼ˆç§»åŠ¨ï¼‰
```

**å…³é”®ç‰¹æ€§**ï¼š

#### a) å¤šç±»å‹æ”¯æŒ
- **FP32**: 32ä½æµ®ç‚¹æ•°ï¼Œç”¨äºè®­ç»ƒå’Œæ¨ç†
- **INT8**: 8ä½æœ‰ç¬¦å·æ•´æ•°ï¼Œç”¨äºé‡åŒ–æ¨ç†
- **INT32**: 32ä½æœ‰ç¬¦å·æ•´æ•°ï¼Œç”¨äºæ ‡ç­¾å’Œç´¢å¼•æ“ä½œ

#### b) æ¢¯åº¦ç®¡ç†ä¼˜åŒ–
```cpp
// å»¶è¿Ÿåˆ†é…ç¤ºä¾‹
Tensor weight = backend->randn(Shape(100, 100));  // åªåˆ†é…æƒé‡
std::cout << "Has grad: " << weight.has_grad() << std::endl;  // false

Tensor& weight_grad = weight.grad();                    // ç°åœ¨åˆ†é…æ¢¯åº¦
std::cout << "Grad shape: " << weight_grad.shape().to_string() << std::endl;
```

#### c) çŸ©é˜µç»´åº¦åˆ«å
```cpp
int32_t batch() const noexcept;    // Nç»´åº¦
int32_t channel() const noexcept;  // Cç»´åº¦
int32_t height() const noexcept;    // Hç»´åº¦
int32_t width() const noexcept;     // Wç»´åº¦
```

### 2. Storageç±» - è®¾å¤‡æ— å…³çš„å†…å­˜æŠ½è±¡

**è®¾è®¡ä½ç½®**ï¼šå°è£…åŸå§‹å†…å­˜ï¼Œæä¾›RAIIç®¡ç†ï¼Œä½œä¸ºTensorå’ŒBackendä¹‹é—´çš„æ¡¥æ¢ã€‚

**æ ¸å¿ƒæ•°æ®ç»“æ„**ï¼š
```cpp
class Storage {
    std::shared_ptr<void> data_ptr_;  // æ™ºèƒ½æŒ‡é’ˆç®¡ç†çš„å†…å­˜å—
    size_t size_;                     // å®é™…ä½¿ç”¨å¤§å°
    size_t capacity_;                 // åˆ†é…çš„å®¹é‡
    Device device_;                   // å†…å­˜ä½ç½®è®¾å¤‡
    DType dtype_;                     // æ•°æ®ç±»å‹
};
```

**å…³é”®ç‰¹æ€§**ï¼š

#### a) è®¾å¤‡æ— å…³çš„å†…å­˜ç®¡ç†
```cpp
// Storageæœ¬èº«ä¸å…³å¿ƒå†…å­˜å¸ƒå±€æ ¼å¼
Storage(size_t size, const Device& device, DType dtype)
    : size_(size), capacity_(size), device_(device), dtype_(dtype) {
    // å§”æ‰˜ç»™Backendè¿›è¡Œè®¾å¤‡ç‰¹å®šçš„å†…å­˜åˆ†é…
    auto backend = BackendManager::get_backend(device);
    // å†…å­˜æ ¼å¼ç”±Backendå†³å®š
}
```

### 3. BackendåŸºç±» - è®¡ç®—å’Œå­˜å‚¨å®ç°

**è®¾è®¡ä½ç½®**ï¼šå®šä¹‰ç»Ÿä¸€è®¡ç®—æ¥å£ï¼Œå…·ä½“å®ç°ç”±å„ä¸ªåç«¯å¤„ç†ã€‚

**æ ¸å¿ƒæ¥å£**ï¼š
```cpp
class Backend {
public:
    // å†…å­˜ç®¡ç†æ¥å£
    virtual std::shared_ptr<void> allocate(size_t size) = 0;
    virtual void deallocate(void* ptr) = 0;
    virtual void copy_data(void* dst, const void* src, size_t size,
                       const Device& dst_device, const Device& src_device) = 0;

    // è®¾å¤‡è½¬æ¢æ¥å£
    virtual Tensor from_cpu(const Tensor& tensor) = 0;
    virtual Tensor to_cpu(const Tensor& tensor) = 0;
    virtual Tensor to(const Tensor& tensor, const Device& device) = 0;

    // è®¡ç®—æ“ä½œæ¥å£
    virtual void mm_into(const Tensor& a, const Tensor& b, Tensor& result) = 0;
    virtual void fill(Tensor& dst, float value) = 0;
    virtual void add(Tensor& result, const Tensor& a, const Tensor& b) = 0;

    // å¹¿æ’­æ“ä½œæ¥å£
    virtual Tensor add_broadcast(const Tensor& tensor_a, const Tensor& tensor_b) const;
    virtual void add_broadcast_into(const Tensor& tensor_a, const Tensor& tensor_b, Tensor& result) const;

    // æ¿€æ´»å‡½æ•°æ¥å£
    virtual Tensor tanh(const Tensor& tensor_a) const;
    virtual void tanh_into(const Tensor& tensor_a, Tensor& result) const;

    // è½¬ç½®æ“ä½œæ¥å£
    virtual Tensor transpose(const Tensor& input) const;
    virtual void transpose_into(const Tensor& input, Tensor& output) const;
};
```

### 4. CpuBackend - è¡Œä¸»å­˜å‚¨å®ç°

**å­˜å‚¨ç‰¹æ€§**ï¼š
- **å†…å­˜å¸ƒå±€**ï¼šè¡Œä¸»å­˜å‚¨ï¼ˆRow-majorï¼‰
- **å†…å­˜å¯¹é½**ï¼š64å­—èŠ‚å¯¹é½ï¼Œä¸ºSIMDè®¿é—®ä¼˜åŒ–
- **è®¡ç®—ä¼˜åŒ–**ï¼šé›†æˆEigen3åº“è¿›è¡Œå‘é‡åŒ–è®¡ç®—

#### V1.45.0æ–°å¢ï¼štanhå’Œå¹¿æ’­æ“ä½œ
```cpp
// æ¿€æ´»å‡½æ•°
Tensor CpuBackend::tanh(const Tensor& tensor_a) const override {
    Tensor result = empty(tensor_a.shape(), tensor_a.dtype());
    tanh_into(tensor_a, result);
    return result;
}

void CpuBackend::tanh_into(const Tensor& tensor_a, Tensor& result) const {
    // ä½¿ç”¨Eigenåº“å®ç°é«˜æ•ˆçš„tanhè®¡ç®—
    // ...
}

// å¹¿æ’­åŠ æ³•
Tensor CpuBackend::add_broadcast(const Tensor& tensor_a, const Tensor& tensor_b) const override {
    Tensor result = empty(infer_broadcast_shape(tensor_a.shape(), tensor_b.shape()));
    add_broadcast_into(tensor_a, tensor_b, result);
    return result;
}
```

#### å¼ é‡åˆ›å»ºæ–¹æ³•
```cpp
// åŸºæœ¬åˆ›å»ºæ–¹æ³•
Tensor empty(const Shape& shape, DType dtype) override;
Tensor zeros(const Shape& shape, DType dtype) override;
Tensor ones(const Shape& shape, DType dtype) override;

// é«˜çº§åˆ›å»ºæ–¹æ³•
Tensor full(const Shape& shape, float value, DType dtype = DType::FP32);
Tensor randn(const Shape& shape, unsigned int seed = 0) override;
Tensor uniform(const Shape& shape, float min_val, float max_val, unsigned int seed = 0);

// ç©ºå¼ é‡ï¼ˆä¸å å†…å­˜ï¼‰
static Tensor null_tensor();
```

### 5. BackendManager - åç«¯ç®¡ç†å™¨

**è®¾è®¡ç‰¹æ€§**ï¼š
- **Meyerså•ä¾‹**ï¼šçº¿ç¨‹å®‰å…¨çš„å•ä¾‹å®ç°
- **é™æ€ä¾¿åˆ©æ–¹æ³•**ï¼šæä¾›ç±»å‹å®‰å…¨çš„åç«¯è®¿é—®
- **è‡ªåŠ¨æ³¨å†Œ**ï¼šæ”¯æŒç¼–è¯‘æ—¶é…ç½®å’Œè¿è¡Œæ—¶å‘ç°

**æ ¸å¿ƒå®ç°**ï¼š
```cpp
class BackendManager {
public:
    // Meyerså•ä¾‹ï¼ŒC++11çº¿ç¨‹å®‰å…¨
    static BackendManager& instance() {
        static BackendManager instance;
        return instance;
    }

    // é™æ€ä¾¿åˆ©æ–¹æ³•ï¼ˆV1.45.0æ›´æ–°ï¼‰
    static std::shared_ptr<CpuBackend> get_cpu_backend() {
        static std::shared_ptr<CpuBackend> cpu_backend = std::make_shared<CpuBackend>();
        return cpu_backend;
    }

    std::shared_ptr<Backend> get_backend(const Device& device);
    void register_backend(const Device& device, std::shared_ptr<Backend> backend);
};
```

## V1.45.0æœ€æ–°åŠŸèƒ½è¯¦è§£

### 1. å®Œæ•´çš„æ¢¯åº¦ç®¡ç†ç³»ç»Ÿ

#### å»¶è¿Ÿåˆ†é…ç¤ºä¾‹
```cpp
// Moduleä¸­è‡ªåŠ¨æ¢¯åº¦ç®¡ç†ç¤ºä¾‹
class Linear : public Module {
    void set_backend(Backend* backend) override {
        Module::set_backend(backend);

        // åˆ›å»ºæƒé‡ï¼ˆæ­¤æ—¶ä¸åˆ†é…æ¢¯åº¦ï¼‰
        Tensor weight = backend->zeros(Shape(out_features, in_features));
        register_parameter("weight", weight);
    }

    void backward_into(const Tensor& grad_output, Tensor& grad_input) override {
        // è·å–æƒé‡æ¢¯åº¦ï¼ˆå»¶è¿Ÿåˆ†é…ï¼‰
        if (has_parameter("weight")) {
            const Tensor& weight = get_parameter("weight");
            if (weight.has_grad()) {
                Tensor& weight_grad = weight.grad();  // è‡ªåŠ¨åˆ†é…
                // è®¡ç®—æƒé‡æ¢¯åº¦...
            }
        }
    }
};
```

#### å†…å­˜ä¼˜åŒ–æ•ˆæœ
```cpp
// 3å±‚MLPç¤ºä¾‹ï¼šLinear â†’ Tanh â†’ Linear â†’ Tanh â†’ Linear
// ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ªå‚æ•°éƒ½éœ€è¦åˆ†é…æ¢¯åº¦ï¼ˆ6ä¸ªå¼ é‡ï¼‰
// æ–°æ–¹æ³•ï¼šåªæœ‰ä½¿ç”¨çš„å‚æ•°æ‰åˆ†é…æ¢¯åº¦ï¼ˆæŒ‰éœ€åˆ†é…ï¼‰

// å†…å­˜å ç”¨å¯¹æ¯”
auto backend = BackendManager::get_cpu_backend();

// Linearå±‚æƒé‡
Linear fc1(784, 512);
fc1.set_backend(backend.get());
// æ­¤æ—¶åªæœ‰æƒé‡ï¼Œæ²¡æœ‰æ¢¯åº¦ï¼ˆå†…å­˜ï¼š4MBï¼‰

// å¼€å§‹è®­ç»ƒæ—¶
Tensor output = fc1.forward(input);
Tensor grad_output = backend->ones(output.shape());
Tensor grad_input = fc1.backward(grad_output);
// ç°åœ¨fc1çš„æƒé‡æœ‰äº†æ¢¯åº¦ï¼ˆå†…å­˜ï¼š8MBï¼‰
```

### 2. intoå‹æ–¹æ³•çš„å…¨é¢æ”¯æŒ

#### é«˜æ€§èƒ½è®¡ç®—ç¤ºä¾‹
```cpp
// é¢„åˆ†é…æ‰€æœ‰å¼ é‡ï¼Œé¿å…å†…å­˜åˆ†é…
auto backend = BackendManager::get_cpu_backend();

// åˆ›å»ºMLPç»„ä»¶
Linear fc1(784, 512);
Tanh act1;
Linear fc2(512, 256);
Tanh act2;
Linear fc3(256, 10);

// è®¾ç½®åç«¯
fc1.set_backend(backend.get());
act1.set_backend(backend.get());
fc2.set_backend(backend.get());
act2.set_backend(backend.get());
fc3.set_backend(backend.get());

// é¢„åˆ†é…æ‰€æœ‰ä¸­é—´å¼ é‡
Tensor input = backend->randn(Shape(32, 784));
Tensor h1 = backend->zeros(Shape(32, 512));
Tensor h1_activated = backend->zeros(Shape(32, 512));
Tensor h2 = backend->zeros(Shape(32, 256));
Tensor h2_activated = backend->zeros(Shape(32, 256));
Tensor output = backend->zeros(Shape(32, 10));

// é«˜æ€§èƒ½å‰å‘ä¼ æ’­ï¼ˆé›¶å†…å­˜åˆ†é…ï¼‰
for (int i = 0; i < 1000; ++i) {
    fc1.forward_into(input, h1);           // ä½¿ç”¨é¢„åˆ†é…çš„h1
    act1.forward_into(h1, h1_activated);     // ä½¿ç”¨é¢„åˆ†é…çš„h1_activated
    fc2.forward_into(h1_activated, h2);    // ä½¿ç”¨é¢„åˆ†é…çš„h2
    act2.forward_into(h2, h2_activated);   // ä½¿ç”¨é¢„åˆ†é…çš„h2_activated
    fc3.forward_into(h2_activated, output); // ä½¿ç”¨é¢„åˆ†é…çš„output
}
// æ€»å…±1000æ¬¡å‰å‘ä¼ æ’­ï¼Œ0æ¬¡å†…å­˜åˆ†é…
```

### 3. Moduleç³»ç»Ÿé›†æˆ

#### è‡ªåŠ¨æ¢¯åº¦ç®¡ç†
```cpp
// Moduleè‡ªåŠ¨ç®¡ç†å‚æ•°æ¢¯åº¦
class Model {
    void zero_grad() {
        // è‡ªåŠ¨æ¸…é›¶æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
        for (auto& [name, param] : parameters()) {
            if (param.has_grad()) {
                param.zero_grad();
            }
        }
    }

    void step() {
        // å‰å‘ä¼ æ’­
        Tensor output = forward(input);

        // è®¡ç®—æŸå¤±
        Tensor loss = compute_loss(output, target);

        // åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†æ¢¯åº¦ï¼‰
        Tensor grad_output = loss_grad(loss);
        Tensor grad_input = backward(grad_output);

        // ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°ï¼ˆè‡ªåŠ¨ä½¿ç”¨å‚æ•°æ¢¯åº¦ï¼‰
        optimizer.step();

        // æ¸…é›¶æ¢¯åº¦ï¼ˆå‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£ï¼‰
        zero_grad();
    }
};
```

## æ€§èƒ½ç‰¹æ€§

### å†…å­˜ä¼˜åŒ–æ•ˆæœ

| åœºæ™¯ | ä¼ ç»Ÿæ–¹æ³• | æ–°æ–¹æ³• | èŠ‚çœ |
|------|---------|--------|------|
| 3å±‚MLPå‚æ•° | ç«‹å³åˆ†é…æ‰€æœ‰æ¢¯åº¦ | æŒ‰éœ€åˆ†é…æ¢¯åº¦ | 50-80% |
| è®­ç»ƒå¾ªç¯ | æ¯æ¬¡éƒ½åˆ†é…å¼ é‡ | intoå‹å¤ç”¨å¼ é‡ | 90%+ |
| æ¢¯åº¦è®¡ç®— | åŒå€å†…å­˜å ç”¨ | å»¶è¿Ÿåˆ†é… | 50% |

### è®¡ç®—æ€§èƒ½åŸºå‡†

**CPU Backendæ€§èƒ½**ï¼š
- **çŸ©é˜µä¹˜æ³•**ï¼š126.78 GFLOPS
- **3Ã—3å·ç§¯**ï¼š342.72 GFLOPS
- **å¹¿æ’­åŠ æ³•**ï¼šé«˜æ•ˆå‘é‡åŒ–å®ç°

**å†…å­˜ç®¡ç†ä¼˜åŒ–**ï¼š
- **å»¶è¿Ÿåˆ†é…**ï¼šåªåœ¨éœ€è¦æ—¶åˆ†é…æ¢¯åº¦
- **æ™ºèƒ½ç¼“å­˜**ï¼šè®­ç»ƒæ¨¡å¼ä¸‹ç¼“å­˜è¾“å…¥ï¼Œæ¨ç†æ¨¡å¼ä¸‹ç¦ç”¨
- **RAIIç®¡ç†**ï¼šè‡ªåŠ¨å†…å­˜é‡Šæ”¾ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬Tensoræ“ä½œ
```cpp
#include "tech_renaissance.h"
using namespace tr;

int main() {
    // è·å–CPUåç«¯
    auto backend = BackendManager::get_cpu_backend();

    // åˆ›å»ºå¼ é‡ï¼ˆè‡ªåŠ¨åˆ†é…å†…å­˜ï¼‰
    Tensor input = backend->randn(Shape(4, 784));
    Tensor weight = backend->zeros(Shape(784, 256));

    // è®¾ç½®æ¢¯åº¦ï¼ˆå»¶è¿Ÿåˆ†é…ï¼‰
    std::cout << "Weight has grad: " << weight.has_grad() << std::endl;  // false

    // å‰å‘ä¼ æ’­
    Tensor output = backend->mm(input, weight);  // C = A Ã— B

    // åå‘ä¼ æ’­
    if (weight.has_grad()) {
        Tensor& weight_grad = weight.grad();
        std::cout << "Weight grad allocated!" << std::endl;
    }

    return 0;
}
```

### MLPç½‘ç»œç¤ºä¾‹
```cpp
// 3å±‚MLPï¼šLinear â†’ Tanh â†’ Linear â†’ Tanh â†’ Linear
class MLP {
private:
    Linear fc1_, fc2_, fc3_;
    Tanh act1_, act2_;

public:
    MLP() : fc1_(784, 512), fc2_(512, 256), fc3_(256, 10),
            act1_("tanh1"), act2_("tanh2") {}

    void set_backend(Backend* backend) {
        fc1_.set_backend(backend);
        act1_.set_backend(backend);
        fc2_.set_backend(backend);
        act2_.set_backend(backend);
        fc3_.set_backend(backend);
    }

    Tensor forward(const Tensor& input) {
        Tensor h1 = fc1_.forward(input);
        Tensor h1_activated = act1_.forward(h1);
        Tensor h2 = fc2_.forward(h1_activated);
        Tensor h2_activated = act2_.forward(h2);
        Tensor output = fc3_.forward(h2_activated);
        return output;
    }

    void backward(const Tensor& grad_output) {
        // è‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†æ‰€æœ‰æ¢¯åº¦
        Tensor grad_h2_activated = fc3_.backward(grad_output);
        Tensor grad_h2 = act2_.backward(grad_h2_activated);
        Tensor grad_h1_activated = fc2_.backward(grad_h2);
        Tensor grad_h1 = act1_.backward(grad_h1_activated);
        Tensor grad_input = fc1_.backward(grad_h1);
    }
};

// ä½¿ç”¨ç¤ºä¾‹
auto backend = BackendManager::get_cpu_backend();
MLP model;
model.set_backend(backend.get());

// å‰å‘ä¼ æ’­
Tensor input = backend->randn(Shape(32, 784));
Tensor output = model.forward(input);

// åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨æ¢¯åº¦ç®¡ç†ï¼‰
Tensor grad_output = backend->ones(output.shape());
model.backward(grad_output);

// æ¸…é›¶æ¢¯åº¦
model.zero_grad();
```

### é«˜æ€§èƒ½intoå‹æ–¹æ³•
```cpp
// é¢„åˆ†é…æ‰€æœ‰å¼ é‡
Tensor input = backend->randn(Shape(1000, 784));
Tensor output = backend->zeros(Shape(1000, 10));

// é«˜æ€§èƒ½å¾ªç¯ï¼ˆé›¶å†…å­˜åˆ†é…ï¼‰
for (int epoch = 0; epoch < 100; ++epoch) {
    for (int batch = 0; batch < 10; ++batch) {
        // ä½¿ç”¨intoå‹æ–¹æ³•å¤ç”¨é¢„åˆ†é…çš„å¼ é‡
        model.forward_into(input, output);
        // å¤„ç†output...
    }
}
```

## é”™è¯¯å¤„ç†å’Œå®‰å…¨ä¿è¯

### å¼‚å¸¸å®‰å…¨è®¾è®¡
```cpp
// ç»Ÿä¸€å¼‚å¸¸ç±»
class TRException : public std::exception {
public:
    TRException(const std::string& message) : message_(message) {}
    const char* what() const noexcept override { return message_.c_str(); }
};

// æœªå®ç°æ–¹æ³•å¼‚å¸¸
class NotImplementedError : public TRException {
public:
    NotImplementedError(const std::string& message) : TRException(message) {}
};
```

### å†…å­˜å®‰å…¨ä¿è¯
- **RAIIç®¡ç†**ï¼šæ™ºèƒ½æŒ‡é’ˆè‡ªåŠ¨å†…å­˜é‡Šæ”¾
- **å¼‚å¸¸å®‰å…¨**ï¼šå¼ºå¼‚å¸¸å®‰å…¨ä¿è¯
- **è¾¹ç•Œæ£€æŸ¥**ï¼šå½¢çŠ¶ç»´åº¦è®¿é—®è¾¹ç•Œæ£€æŸ¥
- **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶å’Œè¿è¡Œæ—¶ç±»å‹æ£€æŸ¥

## æ€»ç»“

æŠ€æœ¯è§‰é†’æ¡†æ¶çš„å¼ é‡-åç«¯ç³»ç»Ÿé€šè¿‡åˆ›æ–°çš„è®¾è®¡å®ç°äº†ï¼š

1. **é«˜æ€§èƒ½**ï¼šæ¯ä¸ªåç«¯é€‰æ‹©æœ€ä¼˜å†…å­˜å¸ƒå±€ï¼Œintoå‹æ–¹æ³•é¿å…ä¸å¿…è¦åˆ†é…
2. **ç”¨æˆ·å‹å¥½**ï¼šè½¬æ¢å±‚é€æ˜å¤„ç†æ ¼å¼è½¬æ¢ï¼Œç”¨æˆ·æ— éœ€å…³å¿ƒåº•å±‚å®ç°
3. **ç±»å‹å®‰å…¨**ï¼šå¼ºç±»å‹å’Œå…¨é¢é”™è¯¯æ£€æŸ¥æœºåˆ¶
4. **è®¾å¤‡æ— å…³**ï¼šç»Ÿä¸€APIæ”¯æŒå¤šè®¾å¤‡å’Œè·¨è®¾å¤‡æ•°æ®ä¼ è¾“
5. **æ¢¯åº¦ä¼˜åŒ–**ï¼šV1.45.0çš„å»¶è¿Ÿåˆ†é…æœºåˆ¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨
6. **Moduleé›†æˆ**ï¼šä¸Moduleç³»ç»Ÿå®Œç¾é›†æˆï¼Œæ”¯æŒè‡ªåŠ¨æ¢¯åº¦ç®¡ç†

**æ ¸å¿ƒåˆ›æ–°**ï¼š
- **åç«¯ç®¡ç†å­˜å‚¨åŸåˆ™**ï¼šæ¯ä¸ªåç«¯é€‰æ‹©æœ€ä¼˜å†…å­˜å¸ƒå±€
- **é€æ˜è½¬æ¢å±‚**ï¼šè‡ªåŠ¨å¤„ç†ä¸åŒå­˜å‚¨æ ¼å¼é—´çš„è½¬æ¢
- **å»¶è¿Ÿæ¢¯åº¦åˆ†é…**ï¼šV1.45.0å®ç°çš„å†…å­˜ä¼˜åŒ–ç­–ç•¥
- **intoå‹æ–¹æ³•**ï¼šV1.45.0å®Œå–„çš„é›¶åˆ†é…è®¡ç®—æ¨¡å¼

## ç‰ˆæœ¬ä¿¡æ¯

- **ç‰ˆæœ¬**: V1.45.0
- **æ—¥æœŸ**: 2025-11-17
- **ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
- **ä¸»è¦æ›´æ–°**ï¼š
  - ğŸ†• æ¢¯åº¦ç®¡ç†ç³»ç»Ÿï¼šå»¶è¿Ÿåˆ†é…çš„æ¢¯åº¦å¼ é‡ç®¡ç†
  - ğŸ†• Moduleé›†æˆï¼šä¸Moduleç³»ç»Ÿæ— ç¼é›†æˆ
  - âœ… å®Œå–„çš„intoå‹æ–¹æ³•ï¼šé›¶å†…å­˜åˆ†é…çš„é«˜æ€§èƒ½è®¡ç®—
  - âœ… ç«¯åˆ°ç«¯éªŒè¯ï¼š3å±‚MLPç½‘ç»œä¸PyTorchå®Œå…¨ä¸€è‡´
  - ğŸ†• Tanhæ¿€æ´»å‡½æ•°ï¼šå®Œæ•´çš„æ¿€æ´»å‡½æ•°å®ç°