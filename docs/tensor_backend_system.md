# å¼ é‡-åç«¯ç³»ç»Ÿæ–‡æ¡£

## æ¦‚è¿°

å¼ é‡-åç«¯ç³»ç»Ÿæ˜¯æŠ€æœ¯è§‰é†’æ¡†æ¶çš„æ ¸å¿ƒæ¶æ„ä¹‹ä¸€ï¼Œé‡‡ç”¨**åç«¯ç®¡ç†å­˜å‚¨**çš„è®¾è®¡ç†å¿µã€‚è¿™ä¸ªç³»ç»Ÿå½»åº•åˆ†ç¦»äº†**å¼ é‡å…ƒæ•°æ®**ä¸**å®é™…æ•°æ®å­˜å‚¨**ï¼Œæä¾›äº†é«˜åº¦çµæ´»çš„å¤šåç«¯æ”¯æŒã€‚

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼š
- **å¼ é‡ç±»**ï¼šçº¯å…ƒæ•°æ®å®¹å™¨ï¼Œä¸æŒæœ‰å®é™…æ•°æ®
- **åç«¯ç±»**ï¼šç®¡ç†å†…å­˜åˆ†é…ã€æ•°æ®è®¿é—®å’Œè®¡ç®—æ“ä½œ
- **å­˜å‚¨ç±»**ï¼šRAIIå†…å­˜ç®¡ç†ï¼Œä¸ç‰¹å®šåç«¯ç»‘å®š
- **BackendManager**ï¼šå•ä¾‹æ¨¡å¼ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰åç«¯å®ä¾‹

**ç‰ˆæœ¬**: V1.43.0
**æ›´æ–°æ—¥æœŸ**: 2025-11-16
**ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ

## ğŸ†• V1.43.0é‡å¤§æ›´æ–°ï¼šåç«¯åŸºç±»é‡æ„

### ğŸ¯ é‡æ„ç›®æ ‡
åœ¨V1.43.0ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å¯¹BackendåŸºç±»è¿›è¡Œäº†é‡å¤§é‡æ„ï¼Œå®ç°äº†ä»¥ä¸‹ç›®æ ‡ï¼š
1. **ä»æŠ½è±¡ç±»æ”¹ä¸ºå¯å®ä¾‹åŒ–ç±»**ï¼šBackendåŸºç±»ä¸å†æ˜¯æŠ½è±¡ç±»ï¼Œè€Œæ˜¯å¯ä»¥å®ä¾‹åŒ–ä½†æŠ›å‡ºå¼‚å¸¸çš„ç±»
2. **ç»Ÿä¸€æ–¹æ³•å£°æ˜æœºåˆ¶**ï¼šå¼•å…¥å®ç³»ç»Ÿï¼Œä¸€è¡Œä»£ç å³å¯å£°æ˜æ–°æ–¹æ³•å¹¶å®ç°é»˜è®¤NotImplementedErrorè¡Œä¸º
3. **ç®€åŒ–åç«¯æ‰©å±•**ï¼šæ–°å¢æ–¹æ³•æ—¶ï¼Œæ— éœ€ä¿®æ”¹æ‰€æœ‰åç«¯ç±»ï¼Œåªéœ€åœ¨BackendåŸºç±»æ·»åŠ å®å®šä¹‰
4. **100%å‘åå…¼å®¹**ï¼šæ‰€æœ‰ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯æ­£å¸¸å·¥ä½œ

### ğŸ”§ åç«¯åŸºç±»å®ä¾‹åŒ–æœºåˆ¶

#### æ„é€ å‡½æ•°è®¾è®¡
```cpp
class Backend {
public:
    /**
     * @brief å…¬å…±æ„é€ å‡½æ•° - é˜²æ­¢ç›´æ¥å®ä¾‹åŒ–
     * @throws TRException ç›´æ¥å®ä¾‹åŒ–æ—¶æŠ›å‡ºå¼‚å¸¸
     */
    Backend() {
        throw TRException("Backend class cannot be instantiated directly! Use specific backend implementations instead.");
    }

protected:
    /**
     * @brief å—ä¿æŠ¤çš„æ„é€ å‡½æ•° - å…è®¸æ´¾ç”Ÿç±»æ„é€ 
     * @param allow_construction æ˜¯å¦å…è®¸æ„é€ ï¼ˆæ´¾ç”Ÿç±»ä¼ trueï¼‰
     */
    Backend(bool allow_construction) {
        if (!allow_construction) {
            throw TRException("Backend class cannot be instantiated directly! Use specific backend implementations instead.");
        }
    }
};
```

#### åç«¯ç±»æ„é€ ç¤ºä¾‹
```cpp
// CPUåç«¯æ„é€ å‡½æ•°
CpuBackend::CpuBackend() : Backend(true) {
    // CPUåç«¯åˆå§‹åŒ–ä»£ç 
}

// CUDAåç«¯æ„é€ å‡½æ•°
CudaBackend::CudaBackend(int device_id) : Backend(true), device_id_(device_id) {
    // CUDAåç«¯åˆå§‹åŒ–ä»£ç 
}
```

### ğŸ“ å®å®šä¹‰ç³»ç»Ÿ

#### å®å®šä¹‰è¯­æ³•
```cpp
/**
 * @brief å®šä¹‰æœªå®ç°æ–¹æ³•çš„å®
 * @param method_name æ–¹æ³•å
 * @param return_type è¿”å›ç±»å‹
 * @param params å‚æ•°åˆ—è¡¨ï¼ˆå¸¦æ‹¬å·ï¼‰
 * @param const_qualifier consté™å®šç¬¦ï¼ˆå¦‚æœæ–¹æ³•ä¸æ˜¯conståˆ™ä¸ºç©ºï¼‰
 * @details ç”Ÿæˆé»˜è®¤æŠ›å‡ºNotImplementedErrorå¼‚å¸¸çš„æ–¹æ³•å®ç°
 */
#define DEFINE_NOT_IMPLEMENTED_METHOD(method_name, return_type, params, const_qualifier) \
    return_type Backend::method_name params const_qualifier { \
        throw NotImplementedError("[" + name() + " " #method_name "] Operation NOT implemented!"); \
    }

/**
 * @brief å®šä¹‰voidè¿”å›ç±»å‹æœªå®ç°æ–¹æ³•çš„å®
 * @param method_name æ–¹æ³•å
 * @param params å‚æ•°åˆ—è¡¨ï¼ˆå¸¦æ‹¬å·ï¼‰
 * @param const_qualifier consté™å®šç¬¦ï¼ˆå¦‚æœæ–¹æ³•ä¸æ˜¯conståˆ™ä¸ºç©ºï¼‰
 */
#define DEFINE_NOT_IMPLEMENTED_VOID_METHOD(method_name, params, const_qualifier) \
    void Backend::method_name params const_qualifier { \
        throw NotImplementedError("[" + name() + " " #method_name "] Operation NOT implemented!"); \
    }
```

#### å®ä½¿ç”¨ç¤ºä¾‹
```cpp
// åœ¨backend.cppä¸­ä½¿ç”¨å®å®šä¹‰æ–°æ–¹æ³•
DEFINE_NOT_IMPLEMENTED_METHOD(crossentropy, float, (const Tensor& pred, const Tensor& label, std::string reduction), )
DEFINE_NOT_IMPLEMENTED_VOID_METHOD(reshape_inplace, (Tensor& tensor_a, const Shape& shape), )
DEFINE_NOT_IMPLEMENTED_METHOD(minus, Tensor, (float scalar, const Tensor& input), const)
```

#### å¼‚å¸¸ä¿¡æ¯æ ¼å¼
æ‰€æœ‰æœªå®ç°çš„æ–¹æ³•éƒ½ä¼šæŠ›å‡ºç»Ÿä¸€æ ¼å¼çš„å¼‚å¸¸ï¼š
```
[BackendName method_name] Operation NOT implemented!
```

ç¤ºä¾‹ï¼š
```
[CudaBackend crossentropy] Operation NOT implemented!
[CPUBackend reshape] Operation NOT implemented!
```

### ğŸš€ æ–°æ–¹æ³•æ·»åŠ æµç¨‹

#### æ­¥éª¤1ï¼šåœ¨BackendåŸºç±»ä¸­å£°æ˜æ–¹æ³•
```cpp
// åœ¨backend.hä¸­
class Backend {
    // ... ç°æœ‰æ–¹æ³•
    virtual Tensor new_method(const Tensor& input, float param) const;
};
```

#### æ­¥éª¤2ï¼šåœ¨backend.cppä¸­ä½¿ç”¨å®å®ç°
```cpp
// åœ¨backend.cppä¸­ä½¿ç”¨å®
DEFINE_NOT_IMPLEMENTED_METHOD(new_method, Tensor, (const Tensor& input, float param), const)
```

#### æ­¥éª¤3ï¼šåœ¨éœ€è¦çš„åç«¯ä¸­é‡å†™
```cpp
// åœ¨cpu_backend.hä¸­é‡å†™
class CpuBackend : public Backend {
    Tensor new_method(const Tensor& input, float param) const override;
};

// åœ¨cpu_backend.cppä¸­å®ç°
Tensor CpuBackend::new_method(const Tensor& input, float param) const {
    // CPUåç«¯å…·ä½“å®ç°
}
```

### âœ… é‡æ„ä¼˜åŠ¿

1. **æ‰©å±•æ€§æå¼º**ï¼šæ–°å¢æ–¹æ³•åªéœ€è¦åœ¨BackendåŸºç±»æ·»åŠ ä¸€è¡Œå®å®šä¹‰
2. **ç»´æŠ¤æˆæœ¬ä½**ï¼šæ— éœ€ä¿®æ”¹æ‰€æœ‰åç«¯ç±»çš„å¤´æ–‡ä»¶
3. **å¼‚å¸¸ä¿¡æ¯ç»Ÿä¸€**ï¼šæ‰€æœ‰æœªå®ç°æ–¹æ³•éƒ½æœ‰æ¸…æ™°çš„é”™è¯¯æç¤º
4. **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶æ£€æŸ¥ï¼Œé¿å…è¿è¡Œæ—¶é”™è¯¯
5. **å‘åå…¼å®¹**ï¼šç°æœ‰ä»£ç æ— éœ€ä»»ä½•ä¿®æ”¹

## # é‡è¦è­¦å‘Šï¼šä¸è¦ç›´æ¥ä½¿ç”¨Tensoræ„é€ å‡½æ•°ï¼

**è­¦å‘Šï¼šTensorç±»çš„æ„é€ å‡½æ•°ä¸ä¼šåˆ†é…å†…å­˜ï¼**

åœ¨Tech Renaissanceæ¡†æ¶ä¸­ï¼ŒTensoræ„é€ å‡½æ•°åªåˆ›å»ºå…ƒæ•°æ®ï¼Œä¸åˆ†é…å®é™…å†…å­˜ã€‚æ‰€æœ‰å¼ é‡å¿…é¡»é€šè¿‡Backendç±»çš„æ–¹æ³•æ¥åˆ›å»ºï¼Œå› ä¸ºBackendä¼šåœ¨åˆ›å»ºåç«‹å³åˆ†é…å†…å­˜ã€‚

**é‡è¦åŒºåˆ«**ï¼š
- **Tensoræ„é€ å‡½æ•°**ï¼šåˆ›å»ºTensorå¯¹è±¡ä½†**ä¸åˆ†é…å†…å­˜**ï¼ˆæ®µé”™è¯¯ï¼ï¼‰
- **Backend::empty()**ï¼š**åˆ†é…å†…å­˜ä½†æœªåˆå§‹åŒ–æ•°æ®**
- **Backend::null_tensor()**ï¼šçœŸæ­£çš„ç©ºå¼ é‡ï¼Œ**ä¸å ç”¨å†…å­˜**

**æ­£ç¡®çš„å¼ é‡åˆ›å»ºæµç¨‹ï¼š**
1. è·å–Backendå­ç±»å®ä¾‹ï¼š`BackendManager::instance().get_backend(CPU)`
2. è½¬æ¢ä¸ºå…·ä½“çš„Backendå­ç±»ï¼š`std::dynamic_pointer_cast<CpuBackend>(backend)`
3. ä½¿ç”¨Backendå­ç±»æ–¹æ³•åˆ›å»ºï¼š`cpu_backend->zeros(shape, dtype)`
4. Backendå­ç±»è‡ªåŠ¨åˆ†é…å†…å­˜å¹¶è¿”å›å¯ç”¨å¼ é‡

**é”™è¯¯çš„æ“ä½œï¼ˆä¼šå¯¼è‡´æ®µé”™è¯¯ï¼‰ï¼š**
- ç›´æ¥è°ƒç”¨`Tensor(shape, dtype, device)`æ„é€ å‡½æ•°
- ä½¿ç”¨Tensorç±»çš„é™æ€å·¥å‚æ–¹æ³•ï¼ˆä¸æ¨èï¼‰
- è¯•å›¾è®¿é—®æœªåˆ†é…å†…å­˜çš„å¼ é‡
- è¯¯è®¤ä¸ºBackendåŸºç±»ç›´æ¥åŒ…å«åˆ›å»ºæ–¹æ³•

## Overview

The Tensor-Backend system in Tech Renaissance framework adopts a layered decoupled design, implementing efficient and safe tensor data management through five core classes. The system follows the "backend manages storage" principle, providing a unified data abstraction layer for deep learning computations.

## Design Philosophy

### Core Design Principles

1. **Separation of Concerns**: Tensor manages metadata, Storage manages memory, Backend handles computation and storage formats
2. **Backend-Managed Storage**: Each backend manages its own tensor storage format, with conversion layers handling format changes
3. **Type Safety**: Strong typing prevents data type errors with compile-time error detection
4. **Device Agnostic**: Supports CPU, CUDA and other devices with transparent device-to-device data transfer
5. **RAII Management**: Smart pointer automatic memory management prevents memory leaks

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User Code/Algorithms        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Tensor Class                â”‚  â† Metadata and device management
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Conversion Layer (Backend Ops)   â”‚  â† Computation and shape manipulation
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Storage Class                â”‚  â† Device-agnostic memory abstraction
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Backend Classes              â”‚  â† Specific computation implementations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Design: Backend-Managed Storage

### Multi-Backend Storage Principle

The core design philosophy of Tech Renaissance framework is **"Backend-Managed Storage"**:

1. **CPU Backend**: Uses **row-major (Row-major)** storage for tensor data
2. **CUDA Backend**: Uses **column-major (Column-major)** storage for tensor data
3. **Transparent Conversion**: Users don't need to care about underlying storage format; conversion layers handle it automatically

### Operation Delegation

The framework delegates computational operations to backend implementations:

- **Arithmetic Operations**: `add`, `subtract`, `multiply`, etc.
- **Shape Operations**: `expand`, `unsqueeze`, `squeeze`, etc.
- **Memory Operations**: `copy`, `fill`, etc.
- **Device Transfers**: `to_cpu`, `from_cpu`, etc.

## Core Components Details

### 1. Tensor Class - Metadata and Device Management

**Design Position**: Tensor class is the core user interface, responsible for metadata management and device coordination.

**Core Data Structure**:

```cpp
class Tensor {
    Shape shape_;                          // Shape information
    DType dtype_;                          // Data type
    Device device_;                        // Device information
    std::shared_ptr<Storage> storage_;     // Memory handle (delegated management)
    size_t offset_;                        // Offset (reserved for future view support)
};
```

**Key Features**:

#### a) Multi-Type Support
- **FP32**: 32-bit floating point for training and inference
- **INT8**: 8-bit signed integers for quantized inference
- **INT32**: 32-bit signed integers for labels and index operations
- All tensor operations support the three data types

#### b) Cross-Backend Conversion Interface

```cpp
// CPU to CUDA conversion (row-major â†’ column-major)
Tensor CudaBackend::from_cpu(const Tensor& tensor);

// CUDA to CPU conversion (column-major â†’ row-major)
Tensor CudaBackend::to_cpu(const Tensor& tensor);
```

**Design Philosophy**: Device-to-device data transfer is implemented entirely through backend interfaces. The Tensor class itself contains no device transfer logic, maintaining lightweight design.

#### b) Type-Safe Scalar Access

```cpp
template<typename T>
T item() const {
    auto backend = get_backend();
    if constexpr (std::is_same_v<T, float>) {
        return backend->get_scalar_float(*this);
    } else if constexpr (std::is_same_v<T, int32_t>) {
        return backend->get_scalar_int32(*this);
    } else if constexpr (std::is_same_v<T, int8_t>) {
        return backend->get_scalar_int8(*this);
    }
    // Compile-time type checking
}
```

#### c) Metadata Access Interface

```cpp
// Shape information
const Shape& shape() const noexcept;
int32_t ndim() const noexcept;
int64_t numel() const noexcept;
int32_t dim_size(int32_t dim) const;

// Matrix dimension aliases
int32_t batch() const noexcept;    // N dimension
int32_t channel() const noexcept;  // C dimension
int32_t height() const noexcept;    // H dimension
int32_t width() const noexcept;     // W dimension

// Raw data access
void* data_ptr() noexcept;
const void* data_ptr() const noexcept;
```

#### d) Removed Methods (V1.29.2)

The following methods have been removed from the Tensor class and are now provided by backend implementations:

- `reshape()`: Shape changing operations
- `squeeze_dim()`: Dimension removal operations
- `unsqueeze_dim()`: Dimension insertion operations

These operations are now accessed through backend APIs:

```cpp
auto cpu_backend = BackendManager::get_cpu_backend();

// Instead of: tensor.squeeze_dim(0)
Tensor squeezed = cpu_backend->squeeze(tensor, 0);

// Instead of: tensor.unsqueeze_dim(1)
Tensor unsqueezed = cpu_backend->unsqueeze(tensor, 1);

// Instead of: tensor.reshape(Shape(2, 3, 4))
Tensor reshaped = cpu_backend->reshape(tensor, Shape(2, 3, 4));
```

### 2. Storage Class - Device-Agnostic Memory Abstraction

**Design Position**: Encapsulates raw memory, provides RAII management, and serves as a bridge between Tensor and Backend.

**Core Data Structure**:

```cpp
class Storage {
    std::shared_ptr<void> data_ptr_;  // Smart-pointer managed memory block
    size_t size_;                     // Actual used size
    size_t capacity_;                 // Allocated capacity
    Device device_;                   // Memory location device
    DType dtype_;                     // Data type
};
```

**Key Features**:

#### a) Device-Agnostic Memory Management

```cpp
// Storage itself doesn't care about memory layout format
Storage(size_t size, const Device& device, DType dtype)
    : size_(size), capacity_(size), device_(device), dtype_(dtype) {
    // Delegate to Backend for device-specific memory allocation
    auto backend = BackendManager::get_backend(device);
    // Memory format is determined by Backend
}
```

#### b) Backend Interface Support

```cpp
// Provide raw memory access for Backend use
void* data_ptr() noexcept { return data_ptr_.get(); }
const void* data_ptr() const noexcept { return data_ptr_.get(); }
```

### 3. Backend Base Class - Computation and Storage Implementation

**Design Position**: Defines unified computation interfaces, with specific implementations handled by each backend.

**Core Interface**:

```cpp
class Backend {
public:
    // Memory management interfaces
    virtual std::shared_ptr<void> allocate(size_t size) = 0;
    virtual void deallocate(void* ptr) = 0;
    virtual void copy_data(void* dst, const void* src, size_t size,
                       const Device& dst_device, const Device& src_device) = 0;

    // Cross-backend conversion interfaces
    virtual Tensor from_cpu(const Tensor& tensor) = 0;
    virtual Tensor to_cpu(const Tensor& tensor) = 0;
    virtual Tensor to(const Tensor& tensor, const Device& device) = 0;

    // Computation operation interfaces
    virtual void mm(Tensor& result, const Tensor& a, const Tensor& b) = 0;
    virtual void fill(Tensor& dst, float value) = 0;
    virtual void fill(Tensor& dst, int8_t value) = 0;
    virtual void add(Tensor& result, const Tensor& a, const Tensor& b) = 0;
    virtual void mul(Tensor& result, const Tensor& a, const Tensor& b) = 0;

    // Advanced operation interfaces (V1.29.2)
    // Scalar operations
    virtual Tensor mul(const Tensor& input, float scalar) const = 0;
    virtual Tensor add(const Tensor& input, float scalar) const = 0;
    virtual Tensor minus(const Tensor& input, float scalar) const = 0;
    virtual Tensor minus(float scalar, const Tensor& input) const = 0;
    virtual Tensor mac(const Tensor& input, float scalar_x, float scalar_y) const = 0;

    // Broadcast operations
    virtual Tensor add_broadcast(const Tensor& tensor_a, const Tensor& tensor_b) const = 0;
    virtual Tensor minus_broadcast(const Tensor& tensor_a, const Tensor& tensor_b) const = 0;
    virtual Tensor mul_broadcast(const Tensor& tensor_a, const Tensor& tensor_b) const = 0;

    // Expansion operations
    virtual Tensor expand(const Tensor& tensor_a, const Shape& shape_b) const = 0;

    // Dimension operations
    virtual Tensor unsqueeze(const Tensor& tensor_a, int32_t dim) const = 0;
    virtual Tensor squeeze(const Tensor& tensor_a, int32_t dim) const = 0;

    // Data access interfaces
    virtual float get_scalar_float(const Tensor& tensor) = 0;
    virtual int32_t get_scalar_int32(const Tensor& tensor) = 0;
    virtual int8_t get_scalar_int8(const Tensor& tensor) = 0;

    // Tensor comparison
    virtual bool is_close(const Tensor& tensor_a, const Tensor& tensor_b, float eps = 5e-5f) const = 0;

    // ğŸ†• V1.43.0æ–°å¢æ–¹æ³• (é€šè¿‡å®å®šä¹‰å®ç°ï¼Œé»˜è®¤æŠ›å‡ºNotImplementedError)
    virtual Tensor reshape(const Tensor& tensor_a, const Shape& shape);
    virtual void reshape_inplace(Tensor& tensor_a, const Shape& shape);
    virtual void reshape_into(const Tensor& tensor_a, Tensor& result, const Shape& shape);
    virtual Tensor tanh(const Tensor& tensor_a);
    virtual void tanh_inplace(Tensor& tensor_a);
    virtual void tanh_into(const Tensor& tensor_a, Tensor& result);
    virtual Tensor dtanh(const Tensor& tensor_a);
    virtual void dtanh_inplace(Tensor& tensor_a);
    virtual void dtanh_into(const Tensor& tensor_a, Tensor& result);
    virtual float crossentropy(const Tensor& pred, const Tensor& label, std::string reduction);
    virtual Tensor one_hot(const Tensor& label, int32_t num_classes, float label_smoothing);
    virtual void one_hot_into(const Tensor& label, Tensor& result, int32_t num_classes, float label_smoothing);
    // ... ä»¥åŠå…¶ä»–æ ‡é‡è¿ç®—å’Œå¹¿æ’­è¿ç®—æ–¹æ³•
};
```

### 4. BackendManager Backend Manager

**Design Features**:

- **Meyers Singleton**: Thread-safe singleton implementation
- **Static Convenience Methods**: Provide type-safe backend access
- **Auto-Registration**: Support compile-time configuration and runtime discovery

**Core Implementation**:

```cpp
class BackendManager {
public:
    // Meyers singleton, C++11 thread-safe
    static BackendManager& instance() {
        static BackendManager instance;
        return instance;
    }

    // Static convenience methods
    static std::shared_ptr<CudaBackend> get_cuda_backend(int device_id = 0) {
        return std::dynamic_pointer_cast<CudaBackend>(
            instance().get_backend(tr::CUDA(device_id))
        );
    }

    static std::shared_ptr<CpuBackend> get_cpu_backend() {
        return std::dynamic_pointer_cast<CpuBackend>(
            instance().get_backend(tr::CPU)
        );
    }

    std::shared_ptr<Backend> get_backend(const Device& device);
    void register_backend(const Device& device, std::shared_ptr<Backend> backend);
};
```

### 5. Specific Backend Implementations

#### CpuBackend - Row-Major Storage Implementation

**Storage Characteristics**:
- **Memory Layout**: Row-major (Row-major) storage
- **Memory Alignment**: 64-byte aligned, optimized for SIMD access
- **Computation Optimization**: Integrated Eigen3 library for vectorized computation

**Matrix Multiplication Implementation**:

```cpp
void CpuBackend::mm(Tensor& result, const Tensor& a, const Tensor& b) {
    // CPU tensors use row-major storage
    const float* a_data = static_cast<const float*>(a.data_ptr());
    const float* b_data = static_cast<const float*>(b.data_ptr());
    float* result_data = static_cast<float*>(result.data_ptr());

    int32_t M = a.height();  // Row count
    int32_t K = a.width();   // Column count
    int32_t N = b.width();   // B's column count

    // Row-major matrix multiplication: C[M,N] = A[M,K] Ã— B[K,N]
    for (int32_t i = 0; i < M; ++i) {
        for (int32_t j = 0; j < N; ++j) {
            float sum = 0.0f;
            for (int32_t k = 0; k < K; ++k) {
                sum += a_data[i * K + k] * b_data[k * N + j];
            }
            result_data[i * N + j] = sum;
        }
    }
}
```

#### CudaBackend - Column-Major Storage Implementation

**Storage Characteristics**:
- **Memory Layout**: Column-major (Column-major) storage
- **Computation Libraries**: Based on cuBLAS and cuDNN
- **Performance Optimization**: Automatic algorithm selection, GPU performance near hardware limits

**Matrix Multiplication Implementation**:

```cpp
void CudaBackend::mm(Tensor& result, const Tensor& a, const Tensor& b) {
    // CUDA tensors use column-major storage
    const float* a_data = static_cast<const float*>(a.data_ptr());
    const float* b_data = static_cast<const float*>(b.data_ptr());
    float* result_data = static_cast<float*>(result.data_ptr());

    int32_t M = a.height();  // Row count
    int32_t K = a.width();   // Column count
    int32_t N = b.width();   // B's column count

    // cuBLAS standard column-major matrix multiplication: C[M,N] = A[M,K] Ã— B[K,N]
    const float alpha = 1.0f;
    const float beta = 0.0f;

    CUBLAS_CHECK(cublasSgemm(
        cublas_handle_,
        CUBLAS_OP_N, CUBLAS_OP_N,  // No transpose
        N, M, K,                   // Result dimensions
        &alpha,
        b_data, N,                 // B matrix, leading dimension = N
        a_data, K,                 // A matrix, leading dimension = K
        &beta,
        result_data, N             // Result matrix, leading dimension = N
    ));
}
```

## Data Flow and Interaction Mechanisms

### Backend-Based Tensor Creation Flow (V1.31.1)

```cpp
// Backend-based tensor creation with type support
auto cpu_backend = BackendManager::get_cpu_backend();

// Create tensors with different data types
Tensor fp32_tensor = cpu_backend->randint(Shape(2, 3), 0, 10, DType::FP32, 42);
Tensor int8_tensor = cpu_backend->randint(Shape(2, 3), 0, 100, DType::INT8, 123);
Tensor int32_tensor = cpu_backend->randint(Shape(2, 3), 0, 1000, DType::INT32, 456);

// Cross-backend conversion preserves data types
auto cuda_backend = BackendManager::get_cuda_backend();
Tensor cuda_fp32 = cuda_backend->from_cpu(fp32_tensor);
Tensor cuda_int8 = cuda_backend->from_cpu(int8_tensor);
```

### Cross-Backend Computation Flow

```cpp
// 1. Create CPU tensor (row-major storage)
Tensor cpu_a = Tensor::randn(Shape(1024, 2048), 42, DType::FP32, tr::CPU);
Tensor cpu_b = Tensor::randn(Shape(2048, 512), 42, DType::FP32, tr::CPU);

// 2. Convert to CUDA (automatically converted to column-major)
auto cuda_backend = BackendManager::get_cuda_backend();
Tensor cuda_a = cuda_backend->from_cpu(cpu_a);  // Row-major â†’ Column-major
Tensor cuda_b = cuda_backend->from_cpu(cpu_b);

// 3. CUDA matrix multiplication (column-major computation)
Tensor cuda_result = Tensor::empty(Shape(1024, 512), DType::FP32, tr::CUDA(0));
cuda_backend->mm(cuda_result, cuda_a, cuda_b);

// 4. Convert back to CPU (automatically converted back to row-major)
Tensor cpu_result = cuda_backend->to_cpu(cuda_result);  // Column-major â†’ Row-major

// 5. Result verification: CPU and CUDA results should be consistent in row-major view
bool is_close = BackendManager::get_cpu_backend()->is_close(
    cpu_result, cpu_result, 1e-4f);
```

### Memory Layout Conversion Example

**Row-major to Column-major Conversion**:

```cpp
// Original row-major data (CPU)
// A[M,K] = [[1, 2, 3],
//           [4, 5, 6]]
// Memory layout: [1, 2, 3, 4, 5, 6]

// Convert to column-major data (CUDA)
// A^T[K,M] = [[1, 4],
//            [2, 5],
//            [3, 6]]
// Memory layout: [1, 4, 2, 5, 3, 6]

for (int32_t i = 0; i < M; ++i) {        // i = 0,1
    for (int32_t j = 0; j < K; ++j) {    // j = 0,1,2
        cuda_data[j * M + i] = cpu_data[i * K + j];
        // cuda_data[0*2+0] = cpu_data[0*3+0] = 1
        // cuda_data[0*2+1] = cpu_data[1*3+0] = 4
        // cuda_data[1*2+0] = cpu_data[0*3+1] = 2
        // cuda_data[1*2+1] = cpu_data[1*3+1] = 5
        // cuda_data[2*2+0] = cpu_data[0*3+2] = 3
        // cuda_data[2*2+1] = cpu_data[1*3+2] = 6
    }
}
```

## Backend Operations (V1.29.2)

### Available Operation Categories

The backend system provides comprehensive tensor operations:

#### 1. Basic Arithmetic Operations
```cpp
// Element-wise operations
Tensor add_result = backend->add(tensor_a, tensor_b);
Tensor mul_result = backend->mul(tensor_a, tensor_b);
```

#### 2. Scalar Operations (New in V1.29.2)
```cpp
// Scalar arithmetic
Tensor scalar_mul = backend->mul(tensor, 2.0f);
Tensor scalar_add = backend->add(tensor, 1.0f);
Tensor scalar_mac = backend->mac(tensor, 2.0f, 1.0f);  // tensor * 2 + 1
```

#### 3. Broadcast Operations (New in V1.29.2)
```cpp
// Broadcasting tensor operations
Tensor broadcast_add = backend->add_broadcast(tensor_a, tensor_b);
Tensor broadcast_mul = backend->mul_broadcast(tensor_a, tensor_b);
```

#### 4. Shape Manipulation Operations
```cpp
// Shape expansion
Tensor expanded = backend->expand(tensor, Shape(2, 1, 3));

// Dimension manipulation (New in V1.29.2)
Tensor unsqueezed = backend->unsqueeze(tensor, 1);  // Insert dimension at position 1
Tensor squeezed = backend->squeeze(tensor, 0);     // Remove dimension at position 0
```

#### 5. Device Transfer Operations
```cpp
// Device conversions
Tensor cpu_tensor = backend->to_cpu(cuda_tensor);
Tensor cuda_tensor = backend->from_cpu(cpu_tensor);
```

## Performance Characteristics and Benchmarks

### Measured Performance (V1.43.0)

**CUDA Backend Performance**:
- **Matrix Multiplication**: 6602.77 GFLOPS (1024Ã—2048 Ã— 2048Ã—512)
- **3x3 Convolution**: 11917.52 GFLOPS
- **1x1 Convolution**: 6076.90 GFLOPS
- **3x3 Transposed Convolution**: 12789.55 GFLOPS

**CPU Backend Performance**:
- **Matrix Multiplication**: 126.78 GFLOPS
- **3x3 Convolution**: 342.72 GFLOPS
- **1x1 Convolution**: 162.88 GFLOPS
- **3x3 Transposed Convolution**: 194.82 GFLOPS

**Performance Acceleration Ratios**:
- **Matrix Multiplication**: 52x speedup (CUDA vs CPU)
- **3x3 Convolution**: 35x speedup (CUDA vs CPU)
- **1x1 Convolution**: 37x speedup (CUDA vs CPU)
- **3x3 Transposed Convolution**: 66x speedup (CUDA vs CPU)

### Performance Optimization Strategies

1. **Memory Layout Optimization**: Each backend selects optimal memory layout format
2. **Zero-Copy Design**: Conversion layers execute format transformation only when necessary
3. **Cache-Friendly**: Contiguous memory layout and alignment optimization
4. **Algorithm Selection**: CUDA automatically selects optimal cuBLAS algorithms
5. **Vectorization**: CPU backend uses Eigen for SIMD optimization

## Usage Examples

### Basic Cross-Backend Operations

```cpp
#include "tech_renaissance.h"
using namespace tr;

int main() {
    // Get backend instances
    auto cuda_backend = BackendManager::get_cuda_backend();
    auto cpu_backend = BackendManager::get_cpu_backend();

    // Create random tensors (CPU, row-major)
    Tensor cpu_a = Tensor::randn(Shape(1024, 2048), 42);
    Tensor cpu_b = Tensor::randn(Shape(2048, 512), 42);

    // Convert to CUDA (automatically converted to column-major)
    Tensor cuda_a = cuda_backend->from_cpu(cpu_a);
    Tensor cuda_b = cuda_backend->from_cpu(cpu_b);

    // CUDA matrix multiplication (column-major computation)
    Tensor cuda_result = Tensor::empty(Shape(1024, 512), DType::FP32, tr::CUDA(0));
    cuda_backend->mm(cuda_result, cuda_a, cuda_b);

    // Convert back to CPU (automatically converted back to row-major)
    Tensor cpu_result = cuda_backend->to_cpu(cuda_result);

    // Result verification
    bool is_close = cpu_backend->is_close(cpu_result, cpu_result, 1e-4f);
    std::cout << "Results are close: " << (is_close ? "YES" : "NO") << std::endl;

    return 0;
}
```

### New Backend Operations (V1.43.0)

```cpp
// Shape operations
Tensor reshaped = cpu_backend->reshape(input_tensor, Shape(2, 3, 4));
Tensor tanh_result = cpu_backend->tanh(input_tensor);

// Scalar operations
Tensor scalar_result = cpu_backend->mul(input_tensor, 2.0f);
Tensor mac_result = cpu_backend->mac(input_tensor, 2.0f, 1.0f);

// Broadcast operations
Tensor broadcast_result = cpu_backend->add_broadcast(tensor_a, tensor_b);

// Shape operations
Tensor expanded_result = cpu_backend->expand(input_tensor, Shape(2, 1, 3));

// Dimension operations
Tensor unsqueezed_result = cpu_backend->unsqueeze(input_tensor, 1);
Tensor squeezed_result = cpu_backend->squeeze(unsqueezed_result, 1);

// Loss functions
float loss = cpu_backend->crossentropy(pred, label, "mean");

// One-hot encoding
Tensor one_hot = cpu_backend->one_hot(label, 10, 0.1f);
```

### Advanced API Usage (V1.43.0)

```cpp
// Use static convenience methods
auto cuda_backend = BackendManager::get_cuda_backend();
auto cpu_backend = BackendManager::get_cpu_backend();

// Use new matrix dimension alias methods
int32_t M = cpu_a.height();  // 1024
int32_t K = cpu_a.width();   // 2048
int32_t N = cpu_b.width();   // 512

// Shape compatibility checking
if (cpu_a.shape().is_matmul_compatible(cpu_b.shape())) {
    std::cout << "Matrices are compatible for multiplication" << std::endl;
}

// Chain operations with backend delegation
Tensor result = cpu_backend->add(
    cpu_backend->expand(tensor_a, Shape(2, 1, 3)),
    cpu_backend->squeeze(tensor_b, 1)
);
```

## Error Handling and Safety Guarantees

### Exception Safety Design

```cpp
// Unified exception class
class TRException : public std::exception {
public:
    TRException(const std::string& message) : message_(message) {}
    const char* what() const noexcept override { return message_.c_str(); }
private:
    std::string message_;
};

// NotImplementedError for unimplemented backend methods
class NotImplementedError : public TRException {
public:
    NotImplementedError(const std::string& message) : TRException(message) {}
};
```

### Memory Safety Guarantees

- **RAII Management**: Smart pointer automatic memory deallocation
- **Exception Safety**: Strong exception safety guarantees
- **Bounds Checking**: Shape dimension access bounds checking
- **Type Safety**: Compile-time and runtime type checking

## Extensibility Design

### Adding New Backends

1. **Inherit Backend Base Class** and implement all virtual functions
2. **Define Storage Format** (row-major, column-major, or other)
3. **Implement Conversion Methods** (`from_cpu`, `to_cpu`, `to`)
4. **Register with BackendManager**

### ğŸ†• V1.43.0æ–°æ–¹æ³•æ‰©å±•

ä½¿ç”¨æ–°çš„å®ç³»ç»Ÿï¼Œæ·»åŠ æ–°æ–¹æ³•å˜å¾—æå…¶ç®€å•ï¼š

```cpp
// æ­¥éª¤1ï¼šåœ¨backend.hä¸­å£°æ˜
class Backend {
    virtual Tensor new_advanced_op(const Tensor& input, float param) const;
};

// æ­¥éª¤2ï¼šåœ¨backend.cppä¸­ä½¿ç”¨å®å®ç°
DEFINE_NOT_IMPLEMENTED_METHOD(new_advanced_op, Tensor, (const Tensor& input, float param), const)

// æ­¥éª¤3ï¼šåœ¨éœ€è¦åç«¯ä¸­é‡å†™ï¼ˆå¦‚CPUåç«¯ï¼‰
class CpuBackend : public Backend {
    Tensor new_advanced_op(const Tensor& input, float param) const override;
};
```

### New Memory Format Support

The framework supports future memory format extensions:
- Sparse tensor storage formats
- Compressed storage formats
- Hardware-specific optimization formats

## Summary

The Tech Renaissance framework's Tensor-Backend system through the innovative "Backend-Managed Storage" design achieves:

1. **High Performance**: Each backend selects optimal memory layout, GPU performance reaches hardware limits
2. **User-Friendly**: Conversion layers transparently handle format conversions, users don't need to care about underlying implementation
3. **Type Safety**: Strong typing and comprehensive error checking mechanisms
4. **Device-Agnostic**: Unified API supports multiple devices and cross-device data transfer
5. **Extensibility**: Modular design supports new backends and new storage formats
6. **ğŸ†• æå¼ºæ‰©å±•æ€§**: V1.43.0å®ç³»ç»Ÿä½¿å¾—æ–°æ–¹æ³•æ·»åŠ åªéœ€ä¸€è¡Œä»£ç 

**Key Innovations**:
- **Backend-Managed Storage Principle**: Each backend selects optimal memory layout
- **Transparent Conversion Layers**: Automatically handle conversions between different storage formats
- **ğŸ†• å®å®šä¹‰ç³»ç»Ÿ**: V1.43.0å®ç°çš„ç»Ÿä¸€æ–¹æ³•å£°æ˜æœºåˆ¶ï¼Œæå¤§æå‡å¼€å‘æ•ˆç‡

## å¼ é‡é”€æ¯æœ€ä½³å®è·µ

### # æ¨èçš„å¼ é‡é”€æ¯æ–¹æ³•

åœ¨Tech Renaissanceæ¡†æ¶ä¸­ï¼Œå¯¹äºå¤§å‹å¼ é‡çš„é”€æ¯ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ç»“åˆä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š

#### æ–¹æ³•1ï¼šRAIIä½œç”¨åŸŸç®¡ç†ï¼ˆæ¨èç”¨äºå±€éƒ¨å¼ é‡ï¼‰

```cpp
auto cpu_backend = std::dynamic_pointer_cast<CpuBackend>(
    BackendManager::instance().get_backend(CPU));

{
    // åœ¨å¤§æ‹¬å·å†…åˆ›å»ºå¤§å‹å¼ é‡
    Tensor temp_tensor = cpu_backend->zeros(Shape(1000, 1000, 1000), DType::FP32);

    // ä½¿ç”¨temp_tensorè¿›è¡Œè®¡ç®—
    // ...

}  // temp_tensoråœ¨è¿™é‡Œè‡ªåŠ¨ææ„ï¼Œå†…å­˜ç«‹å³é‡Šæ”¾
```

#### æ–¹æ³•2ï¼šæ˜¾å¼åç«¯null_tensor()æ–¹æ³•ï¼ˆæ¨èç”¨äºéœ€è¦çµæ´»æ§åˆ¶çš„åœºæ™¯ï¼‰

```cpp
auto cpu_backend = std::dynamic_pointer_cast<CpuBackend>(
    BackendManager::instance().get_backend(CPU));

// åˆ›å»ºå¤§å‹å¼ é‡
Tensor large_tensor = cpu_backend->zeros(Shape(1000, 1000, 1000), DType::FP32);

// ä½¿ç”¨large_tensorè¿›è¡Œè®¡ç®—
// ...

// æ˜¾å¼é”€æ¯ï¼Œç«‹å³é‡Šæ”¾å†…å­˜
large_tensor = cpu_backend->null_tensor();  // æ˜ç¡®å‘ŠçŸ¥ï¼šè¿™æ˜¯ä¸€ä¸ªnullå¼ é‡
```

### å†…å­˜åˆ†é…çš„é‡è¦åŒºåˆ«

**å…³é”®ç†è§£ä¸åŒæ–¹æ³•çš„å†…å­˜è¡Œä¸ºï¼š**

1. **Tensoræ„é€ å‡½æ•°**ï¼šåªåˆ›å»ºå…ƒæ•°æ®ï¼Œ**ä¸åˆ†é…å†…å­˜**ï¼ˆæ®µé”™è¯¯ï¼ï¼‰
2. **Backend::empty()**ï¼š**åˆ†é…å†…å­˜ä½†æœªåˆå§‹åŒ–æ•°æ®**
3. **Backend::null_tensor()**ï¼šçœŸæ­£çš„ç©ºå¼ é‡ï¼Œ**ä¸å ç”¨å†…å­˜**

### ä¸ºä»€ä¹ˆæ¨èè¿™ä¸¤ç§æ–¹æ³•ï¼Ÿ

1. **é¿å…æ„é€ å‡½æ•°è¯¯ç”¨**ï¼šé˜²æ­¢ç”¨æˆ·ç›´æ¥è°ƒç”¨`Tensor()`æ„é€ å‡½æ•°
2. **APIæ˜ç¡®æ€§**ï¼š`null_tensor()`æ¯”`empty()`æ›´æ— æ­§ä¹‰
3. **ç¬¦åˆæ¡†æ¶è®¾è®¡**ï¼šæ‰€æœ‰æ“ä½œéƒ½é€šè¿‡åç«¯ï¼Œä¿æŒä¸€è‡´æ€§

### å®é™…æ¡ˆä¾‹å‚è€ƒ

å‚è§ `tests/unit_tests/test_memory_occupation.cpp` ä¸­çš„å®Œæ•´æµ‹è¯•æ¡ˆä¾‹ï¼Œè¯¥æµ‹è¯•éªŒè¯äº†ï¼š
- RAIIä½œç”¨åŸŸç®¡ç†çš„æœ‰æ•ˆæ€§
- `null_tensor()`æ–¹æ³•çš„æ­£ç¡®æ€§
- ä¸åŒé”€æ¯æ–¹å¼çš„å†…å­˜é‡Šæ”¾æ•ˆæœ

**æ ¸å¿ƒåŸåˆ™**ï¼šæ— è®ºä½¿ç”¨å“ªç§æ–¹æ³•ï¼Œéƒ½è¦é¿å…ç›´æ¥è°ƒç”¨Tensorç±»çš„æ„é€ å‡½æ•°è¿›è¡Œé”€æ¯æ“ä½œã€‚
- **Consistent Access Interface**: Users always see row-major data access
- **Operation Delegation**: Computational and shape operations delegated to specialized backend implementations

---

## Version Information

- **Version**: V1.43.0
- **Date**: 2025-11-16
- **Author**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
- **Major Updates**:
  - ğŸ†• BackendåŸºç±»é‡æ„ï¼šä»æŠ½è±¡ç±»æ”¹ä¸ºå¯å®ä¾‹åŒ–ç±»
  - ğŸ†• å®å®šä¹‰ç³»ç»Ÿï¼šç»Ÿä¸€æ–¹æ³•å£°æ˜å’Œé»˜è®¤å®ç°æœºåˆ¶
  - ğŸ†• æ–°å¢æ–¹æ³•ï¼šreshapeã€tanhã€crossentropyã€one_hotç­‰é«˜çº§æ“ä½œ
  - ğŸ†• æ‰©å±•æ€§å¤§å¹…æå‡ï¼šæ–°å¢æ–¹æ³•åªéœ€ä¸€è¡Œå®å®šä¹‰
  - âœ… 100%å‘åå…¼å®¹ï¼šç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹