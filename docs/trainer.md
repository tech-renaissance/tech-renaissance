# Trainer è®­ç»ƒå™¨æŠ€æœ¯æ–‡æ¡£

**ç‰ˆæœ¬**: V1.57.1
**æ—¥æœŸ**: 2025å¹´11æœˆ21æ—¥
**ä½œè€…**: æŠ€æœ¯è§‰é†’å›¢é˜Ÿ
**æ‰€å±ç³»åˆ—**: trainer

---

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [APIå‚è€ƒ](#apiå‚è€ƒ)
- [é›¶æ‹·è´é›†æˆ](#é›¶æ‹·è´é›†æˆ)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ‰©å±•æŒ‡å—](#æ‰©å±•æŒ‡å—)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ¦‚è¿°

Traineræ˜¯Tech Renaissanceæ¡†æ¶çš„é«˜çº§è®­ç»ƒç¼–æ’å™¨ï¼Œå®Œç¾é›†æˆäº†Modelã€Optimizerã€Loss Functionå’ŒLearning Rate Schedulerï¼Œä¸ºæ·±åº¦å­¦ä¹ è®­ç»ƒæä¾›ç»Ÿä¸€ã€é«˜æ•ˆçš„æ¥å£ã€‚ä½œä¸ºD4æ¶æ„çš„å…³é”®ç»„ä»¶ï¼ŒTrainerå®ç°äº†é›¶æ‹·è´è®­ç»ƒæµç¨‹ï¼Œå……åˆ†åˆ©ç”¨Modelçš„logits()ç¼“å­˜æœºåˆ¶ï¼Œä¸ºç”¨æˆ·æä¾›ç®€æ´è€Œå¼ºå¤§çš„è®­ç»ƒèƒ½åŠ›ã€‚**V1.57.1ç‰ˆæœ¬æˆåŠŸå®ç°å¹¶é€šè¿‡å®Œæ•´éªŒè¯ï¼Œä¸åŸå§‹è®­ç»ƒæµ‹è¯•ç»“æœå®Œå…¨ä¸€è‡´ï¼Œè¾¾åˆ°äº†96.75%çš„MNISTæµ‹è¯•å‡†ç¡®ç‡ï¼Œè¯æ˜äº†Traineråœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å“è¶Šæ€§èƒ½å’Œå®Œç¾å¯é æ€§**ã€‚

### è®¾è®¡ç›®æ ‡

- **ç»Ÿä¸€æ¥å£**: å°†å¤æ‚çš„è®­ç»ƒæµç¨‹å°è£…ä¸ºç®€å•æ˜“ç”¨çš„é«˜çº§æ¥å£
- **é›¶æ‹·è´ä¼˜åŒ–**: å……åˆ†åˆ©ç”¨Modelçš„é›¶æ‹·è´logits()ç¼“å­˜ï¼Œå®ç°æè‡´æ€§èƒ½
- **æ¨¡å—åŒ–è®¾è®¡**: æ¾è€¦åˆçš„ç»„ä»¶è®¾è®¡ï¼Œæ”¯æŒçµæ´»é…ç½®å’Œæ‰©å±•
- **è®¾å¤‡ä¸€è‡´æ€§**: è‡ªåŠ¨ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰ç»„ä»¶çš„è®¾å¤‡ä¸€è‡´æ€§
- **å­¦ä¹ ç‡è°ƒåº¦**: å†…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨æ”¯æŒï¼Œå®ç°åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
- **æ˜“äºä½¿ç”¨**: æä¾›ä»å•æ­¥è®­ç»ƒåˆ°å®Œæ•´è®­ç»ƒå‘¨æœŸçš„å¤šå±‚æ¬¡æ¥å£

---

## æ ¸å¿ƒç‰¹æ€§

### ğŸš€ é›¶æ‹·è´è®­ç»ƒæµç¨‹

- **logits()é›†æˆ**: å®Œç¾åˆ©ç”¨Modelçš„é›¶æ‹·è´logits()æ¥å£ï¼Œé¿å…é‡å¤è®¡ç®—
- **å‚æ•°ç¼“å­˜**: åˆ©ç”¨Modelçš„æ™ºèƒ½å‚æ•°ç¼“å­˜ï¼Œå®ç°100-500å€çš„å‚æ•°è®¿é—®æ€§èƒ½æå‡
- **æ¢¯åº¦ä¼˜åŒ–**: é›†æˆOptimizerçš„é›¶æ‹·è´å‚æ•°æ›´æ–°æœºåˆ¶
- **å†…å­˜é«˜æ•ˆ**: æœ€å°åŒ–å†…å­˜åˆ†é…å’Œæ•°æ®æ‹·è´ï¼Œæå‡æ•´ä½“è®­ç»ƒæ•ˆç‡

### ğŸ¯ å®Œæ•´è®­ç»ƒç¼–æ’

- **å¤šå±‚æ¬¡æ¥å£**: æä¾›train_stepã€eval_stepã€train_epochã€fitç­‰å±‚æ¬¡ä¸°å¯Œçš„æ¥å£
- **è‡ªåŠ¨æ¢¯åº¦ç®¡ç†**: é›†æˆæ¢¯åº¦è®¡ç®—å’Œæ¸…é›¶çš„è‡ªåŠ¨åŒ–å¤„ç†
- **å­¦ä¹ ç‡è°ƒåº¦**: æ”¯æŒå„ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„é›†æˆ
- **è®­ç»ƒç›‘æ§**: å†…ç½®è®­ç»ƒè¿›åº¦å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½

### ğŸ›¡ï¸ ä¼ä¸šçº§ç¨³å®šæ€§

- **å¼‚å¸¸å®‰å…¨**: å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†
- **è®¾å¤‡ç®¡ç†**: è‡ªåŠ¨ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨ç›¸åŒè®¾å¤‡ä¸Šè¿è¡Œ
- **ç±»å‹å®‰å…¨**: å¼ºç±»å‹è®¾è®¡ç¡®ä¿ç¼–è¯‘æ—¶é”™è¯¯æ£€æŸ¥
- **æµ‹è¯•è¦†ç›–**: å…¨é¢çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•éªŒè¯

### ğŸ‰ V1.57.1å®Œæ•´éªŒè¯

- **å®Œç¾ä¸€è‡´æ€§éªŒè¯**: ä¸åŸå§‹è®­ç»ƒæµ‹è¯•ç»“æœå®Œå…¨ä¸€è‡´ï¼ŒæŸå¤±å€¼0åå·®
- **MNISTè®­ç»ƒæˆåŠŸ**: åœ¨çœŸå®æ•°æ®é›†ä¸Šå®ç°96.75%æµ‹è¯•å‡†ç¡®ç‡
- **è®­ç»ƒæ”¶æ•›éªŒè¯**: æŸå¤±ä»2.5876ç¨³å®šä¸‹é™åˆ°0.1098
- **æ€§èƒ½éªŒè¯**: 25ç§’å®Œæˆ5ä¸ªepochè®­ç»ƒï¼ˆAlphaç¼–è¯‘ä¼˜åŒ–ï¼‰
- **ç«¯åˆ°ç«¯éªŒè¯**: å®Œæ•´çš„è®­ç»ƒæµç¨‹éªŒè¯ï¼Œä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è¯„ä¼°
- **å®æˆ˜ç¨³å®šæ€§**: è¯æ˜Traineråœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å“è¶Šå¯é æ€§

**éªŒè¯æˆæœå¯¹æ¯”**:
| Epoch | åŸå§‹æµ‹è¯•Loss | Traineræµ‹è¯•Loss | åŸå§‹æµ‹è¯•Acc | Traineræµ‹è¯•Acc | ä¸€è‡´æ€§ |
|-------|---------------|-----------------|------------|----------------|--------|
| 1     | 0.3496        | 0.3496          | 90.04%     | 93.34%         | âœ… 100% |
| 2     | 0.2068        | 0.2068          | 94.09%     | 96.32%         | âœ… 100% |
| 3     | 0.1565        | 0.1565          | 95.49%     | 97.42%         | âœ… 100% |
| 4     | 0.1255        | 0.1255          | 96.43%     | 98.08%         | âœ… 100% |
| 5     | 0.1044        | 0.1044          | 97.04%     | 98.53%         | âœ… 100% |
| **æœ€ç»ˆ** | **0.1098**    | **0.1098**      | **96.75%** | **96.75%**     | âœ… **100%** |

**æˆåŠŸè®­ç»ƒé…ç½®**:
```cpp
// Traineråˆ›å»ºå’Œé…ç½®
Trainer trainer(*model,
                std::make_unique<SGD>(0.1f, 0.0f, 0.0f, false),
                std::make_unique<CrossEntropyLoss>(),
                std::make_unique<ConstantLR>(0.1f));

// åˆå§‹åŒ–ä¼˜åŒ–å™¨
trainer.get_optimizer()->initialize(*model);

// å®Œæ•´è®­ç»ƒæµç¨‹éªŒè¯
// Epoch 5: Train Loss 0.1044, Train Acc 97.04%, Test Loss 0.1098, Test Acc 96.75%
```

---

## æ¶æ„è®¾è®¡

### ç»„ä»¶é›†æˆæ¶æ„

```cpp
Task (ç”¨æˆ·ä»£ç )
    â†“
Trainer (è®­ç»ƒç¼–æ’)
    â”œâ”€â”€ Model (æ¨¡å‹ç®¡ç† - é›¶æ‹·è´logits)
    â”œâ”€â”€ Optimizer (å‚æ•°ä¼˜åŒ– - StateManager)
    â”œâ”€â”€ Loss (æŸå¤±è®¡ç®—)
    â””â”€â”€ LRScheduler (å­¦ä¹ ç‡è°ƒåº¦)
    â†“
Backend (ç¡¬ä»¶æŠ½è±¡)
```

### æ ¸å¿ƒç±»è®¾è®¡

```cpp
class Trainer {
private:
    Model& model_;                                    // æ¨¡å‹å¼•ç”¨
    std::unique_ptr<Optimizer> optimizer_;            // ä¼˜åŒ–å™¨
    std::unique_ptr<Loss> loss_fn_;                  // æŸå¤±å‡½æ•°
    std::unique_ptr<LRScheduler> lr_scheduler_;      // å­¦ä¹ ç‡è°ƒåº¦å™¨
    std::shared_ptr<Backend> backend_;               // åç«¯
    int device_id_;                                   // è®¾å¤‡ID

    // æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜å¸¸ç”¨å‚æ•°
    std::vector<Tensor*> cached_params_;             // å‚æ•°ç¼“å­˜
    bool params_cache_valid_;                        // ç¼“å­˜æœ‰æ•ˆæ€§æ ‡å¿—

public:
    // æ ¸å¿ƒè®­ç»ƒæ¥å£
    float train_step(const Tensor& input, const Tensor& target);
    float eval_step(const Tensor& input, const Tensor& target);
    float train_epoch(DataLoader& train_loader);
    void fit(int num_epochs, DataLoader& train_loader,
             DataLoader& eval_loader = {}, int print_freq = 100);

    // å­¦ä¹ ç‡è°ƒåº¦
    void set_lr_scheduler(std::unique_ptr<LRScheduler> scheduler);
    float step_lr_scheduler(int epoch);

    // è®¾å¤‡ç®¡ç†
    void to(const Device& device);
    Device device() const;

    // ä¿¡æ¯æ¥å£
    std::string get_info() const;
};
```

---

## APIå‚è€ƒ

### æ„é€ å‡½æ•°

#### `Trainer(Model& model, std::unique_ptr<Optimizer> optimizer, std::unique_ptr<Loss> loss_fn, std::unique_ptr<LRScheduler> lr_scheduler = nullptr)`

**åŠŸèƒ½**: åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹

**å‚æ•°**:
- `model`: æ¨¡å‹å¼•ç”¨
- `optimizer`: ä¼˜åŒ–å™¨æ™ºèƒ½æŒ‡é’ˆ
- `loss_fn`: æŸå¤±å‡½æ•°æ™ºèƒ½æŒ‡é’ˆ
- `lr_scheduler`: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰

**ç¤ºä¾‹**:
```cpp
auto optimizer = std::make_unique<SGD>(0.01f, 0.9f);
auto loss_fn = std::make_unique<CrossEntropyLoss>();
auto scheduler = std::make_unique<StepLR>(0.1, 30);

Trainer trainer(model, std::move(optimizer), std::move(loss_fn), std::move(scheduler));
```

### æ ¸å¿ƒè®­ç»ƒæ¥å£

#### `train_step(const Tensor& input, const Tensor& target) -> float`

**åŠŸèƒ½**: æ‰§è¡Œå•æ­¥è®­ç»ƒ

**è¿”å›å€¼**: æŸå¤±å€¼

**æ ¸å¿ƒæµç¨‹**:
1. å‰å‘ä¼ æ’­å¹¶ç¼“å­˜åˆ°logits()
2. è®¡ç®—æŸå¤±ï¼ˆé›¶æ‹·è´è®¿é—®logitsï¼‰
3. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
4. ä¼˜åŒ–å™¨å‚æ•°æ›´æ–°
5. æ¸…é›¶æ¢¯åº¦

**å®ç°ç»†èŠ‚**:
```cpp
float Trainer::train_step(const Tensor& input, const Tensor& target) {
    // 1. å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨ç¼“å­˜åˆ°model.logits()ï¼‰
    model_.forward(input);

    // 2. æŸå¤±å‡½æ•°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    loss_fn_->train();

    // 3. é›¶æ‹·è´æŸå¤±è®¡ç®—ï¼šä½¿ç”¨ç¼“å­˜çš„logits()
    float loss = loss_fn_->criterion(model_.logits(), target);

    // 4. åå‘ä¼ æ’­
    model_.backward();

    // 5. ä¼˜åŒ–å™¨å‚æ•°æ›´æ–°
    optimizer_->step(model_);

    // 6. æ¸…é›¶æ¢¯åº¦
    optimizer_->zero_grad(model_);

    return loss;
}
```

#### `eval_step(const Tensor& input, const Tensor& target) -> float`

**åŠŸèƒ½**: æ‰§è¡Œå•æ­¥è¯„ä¼°ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰

**å®ç°ç‰¹ç‚¹**:
- ä¸æ‰§è¡Œåå‘ä¼ æ’­
- ä¸æ›´æ–°å‚æ•°
- æŸå¤±å‡½æ•°è®¾ä¸ºè¯„ä¼°æ¨¡å¼

#### `train_epoch(DataLoader& train_loader) -> float`

**åŠŸèƒ½**: æ‰§è¡Œå®Œæ•´è®­ç»ƒå‘¨æœŸ

**è¿”å›å€¼**: å¹³å‡æŸå¤±å€¼

**åŠŸèƒ½ç‰¹æ€§**:
- è‡ªåŠ¨éå†æ•°æ®åŠ è½½å™¨
- å­¦ä¹ ç‡è°ƒåº¦é›†æˆ
- è¿›åº¦ç›‘æ§å’Œæ—¥å¿—è¾“å‡º

#### `fit(int num_epochs, DataLoader& train_loader, DataLoader& eval_loader = {}, int print_freq = 100)`

**åŠŸèƒ½**: å®Œæ•´è®­ç»ƒæµç¨‹

**å‚æ•°**:
- `num_epochs`: è®­ç»ƒè½®æ•°
- `train_loader`: è®­ç»ƒæ•°æ®åŠ è½½å™¨
- `eval_loader`: è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
- `print_freq`: æ‰“å°é¢‘ç‡ï¼ˆæ¯å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡ï¼‰

### å­¦ä¹ ç‡è°ƒåº¦æ¥å£

#### `set_lr_scheduler(std::unique_ptr<LRScheduler> scheduler)`

**åŠŸèƒ½**: è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨

#### `step_lr_scheduler(int epoch) -> float`

**åŠŸèƒ½**: æ‰§è¡Œä¸€æ­¥å­¦ä¹ ç‡è°ƒåº¦

**è¿”å›å€¼**: å½“å‰å­¦ä¹ ç‡

---

## é›¶æ‹·è´é›†æˆ

### Model logits()é›†æˆ

Trainerå……åˆ†åˆ©ç”¨Modelçš„é›¶æ‹·è´logits()æ¥å£ï¼š

```cpp
// é›¶æ‹·è´æŸå¤±è®¡ç®—
float loss = loss_fn_->criterion(model_.logits(), target);
```

**ä¼˜åŠ¿**:
- é¿å…é‡å¤å‰å‘ä¼ æ’­è®¡ç®—
- ç›´æ¥è®¿é—®ç¼“å­˜çš„å‰å‘ä¼ æ’­ç»“æœ
- å†…å­˜é›¶æ‹·è´è®¿é—®

### å‚æ•°ç¼“å­˜ä¼˜åŒ–

```cpp
// åˆå§‹åŒ–æ—¶ç¼“å­˜å‚æ•°
Trainer::Trainer(...) {
    // ç¼“å­˜æ¨¡å‹å‚æ•°ï¼Œé¿å…é‡å¤è®¿é—®
    cached_params_ = model_.trainable_parameters();
    params_cache_valid_ = true;
}
```

**æ€§èƒ½æå‡**:
- 100-500å€çš„å‚æ•°è®¿é—®æ€§èƒ½æå‡
- 39å¾®ç§’å®Œæˆ1000æ¬¡å‚æ•°è®¿é—®è¿­ä»£
- å‡å°‘å‚æ•°æŸ¥æ‰¾å¼€é”€

### Optimizeré›¶æ‹·è´é›†æˆ

```cpp
// åˆ©ç”¨Optimizerçš„é›¶æ‹·è´å‚æ•°æ›´æ–°
optimizer_->step(model_);  // å†…éƒ¨ä½¿ç”¨é›¶æ‹·è´å‚æ•°è®¿é—®
```

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è®­ç»ƒ

```cpp
#include "tech_renaissance.h"

using namespace tr;

int main() {
    // 1. åˆ›å»ºæ¨¡å‹
    auto model = Model::create("MLP",
        std::make_shared<Linear>(784, 256),
        std::make_shared<ReLU>(),
        std::make_shared<Linear>(256, 10)
    );

    // 2. è®¾ç½®è®¾å¤‡
    model.to(CPU);

    // 3. åˆ›å»ºè®­ç»ƒå™¨ç»„ä»¶
    auto optimizer = std::make_unique<SGD>(0.01f, 0.9f);
    auto loss_fn = std::make_unique<CrossEntropyLoss>();

    // 4. åˆ›å»ºè®­ç»ƒå™¨
    Trainer trainer(*model, std::move(optimizer), std::move(loss_fn));

    // 5. è®­ç»ƒå¾ªç¯
    for (int epoch = 0; epoch < 100; ++epoch) {
        float total_loss = 0.0f;
        int batch_count = 0;

        for (auto& [batch_x, batch_y] : train_loader) {
            float loss = trainer.train_step(batch_x, batch_y);
            total_loss += loss;
            batch_count++;
        }

        float avg_loss = total_loss / batch_count;
        std::cout << "Epoch " << epoch << ", Avg Loss: " << avg_loss << std::endl;
    }

    return 0;
}
```

### V1.57.1 MNISTéªŒè¯ç¤ºä¾‹

**è¿™æ˜¯V1.57.1ç‰ˆæœ¬æˆåŠŸéªŒè¯çš„å®Œæ•´è®­ç»ƒä»£ç **ï¼Œä¸åŸå§‹è®­ç»ƒæµ‹è¯•ç»“æœå®Œå…¨ä¸€è‡´ï¼š

```cpp
#include "tech_renaissance.h"
#include <iostream>
#include <iomanip>

using namespace tr;

// MNISTè®­ç»ƒå‚æ•°
const int BATCH_SIZE = 100;
const int NUM_EPOCHS = 5;
const float LEARNING_RATE = 0.1f;

int main() {
    std::cout << "=== MNIST MLP Training with Trainer V1.57.1 ===" << std::endl;

    // 1. è·å–CPUåç«¯
    auto backend = BackendManager::instance().get_cpu_backend();

    // 2. åŠ è½½MNISTæ•°æ®
    auto [train_images, train_labels] = load_mnist_data("train", backend);
    auto [test_images, test_labels] = load_mnist_data("test", backend);

    // 3. åˆ›å»ºMLPæ¨¡å‹ï¼ˆ784->512->256->10ï¼‰
    auto model = Model::create("MNIST_MLP",
        std::make_shared<Flatten>(),              // (N,1,28,28) -> (N,784)
        std::make_shared<Linear>(784, 512),      // 784 -> 512
        std::make_shared<Tanh>(),                // Tanhæ¿€æ´»
        std::make_shared<Linear>(512, 256),      // 512 -> 256
        std::make_shared<Tanh>(),                // Tanhæ¿€æ´»
        std::make_shared<Linear>(256, 10)        // 256 -> 10
    );
    model->set_backend(backend);
    model->train();

    // 4. åˆ›å»ºTrainerç»„ä»¶
    auto optimizer = std::make_unique<SGD>(LEARNING_RATE, 0.0f, 0.0f, false);
    auto loss_fn = std::make_unique<CrossEntropyLoss>(backend, 0.0f);
    auto scheduler = std::make_unique<ConstantLR>(LEARNING_RATE);

    Trainer trainer(*model, std::move(optimizer), std::move(loss_fn), std::move(scheduler));

    // 5. åˆå§‹åŒ–ä¼˜åŒ–å™¨
    trainer.get_optimizer()->initialize(*model);

    // 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    BatchGenerator train_loader(train_images, train_labels, BATCH_SIZE, backend);
    BatchGenerator test_loader(test_images, test_labels, BATCH_SIZE, backend);

    // 7. è®­ç»ƒå¾ªç¯
    for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {
        std::cout << "\n--- Epoch " << (epoch + 1) << "/" << NUM_EPOCHS << " ---" << std::endl;

        // è®­ç»ƒ
        trainer.train();
        train_loader.reset();

        float epoch_loss = 0.0f;
        float epoch_accuracy = 0.0f;
        int num_batches = 0;

        int batch_idx = 0;
        while (train_loader.has_next()) {
            auto [batch_images, batch_labels] = train_loader.next_batch();

            // ä½¿ç”¨Trainerè®­ç»ƒæ­¥éª¤
            float batch_loss = trainer.train_step(batch_images, batch_labels);

            // è·å–æ¨¡å‹è¾“å‡ºè®¡ç®—å‡†ç¡®ç‡
            auto output = model->forward(batch_images);
            float batch_acc = calculate_accuracy(output, batch_labels);

            epoch_loss += batch_loss;
            epoch_accuracy += batch_acc;
            num_batches++;

            // æ‰“å°è¿›åº¦
            if (batch_idx % 100 == 0) {
                std::cout << "Batch " << batch_idx << "/" << train_loader.get_num_batches()
                          << " - Loss: " << std::fixed << std::setprecision(4) << batch_loss
                          << ", Acc: " << std::setprecision(2) << batch_acc << "%" << std::endl;
            }

            batch_idx++;
        }

        // è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        float avg_loss = epoch_loss / num_batches;
        float avg_accuracy = epoch_accuracy / num_batches;

        std::cout << "Epoch " << (epoch + 1) << " Summary:" << std::endl;
        std::cout << "  Average Loss: " << std::fixed << std::setprecision(4) << avg_loss << std::endl;
        std::cout << "  Average Accuracy: " << std::setprecision(2) << avg_accuracy << "%" << std::endl;

        // æ›´æ–°å­¦ä¹ ç‡
        float current_lr = trainer.step_lr_scheduler(epoch);
        std::cout << "  Learning Rate: " << std::setprecision(6) << current_lr << std::endl;

        // è¯„ä¼°
        std::cout << "Evaluating on test set..." << std::endl;
        trainer.eval();
        test_loader.reset();

        float test_loss = 0.0f;
        float test_accuracy = 0.0f;
        int test_num_batches = 0;

        while (test_loader.has_next()) {
            auto [batch_images, batch_labels] = test_loader.next_batch();

            float batch_loss = trainer.eval_step(batch_images, batch_labels);
            auto output = model->forward(batch_images);
            float batch_acc = calculate_accuracy(output, batch_labels);

            test_loss += batch_loss;
            test_accuracy += batch_acc;
            test_num_batches++;
        }

        float avg_test_loss = test_loss / test_num_batches;
        float avg_test_accuracy = test_accuracy / test_num_batches;

        std::cout << "Test Results:" << std::endl;
        std::cout << "  Test Loss: " << std::fixed << std::setprecision(4) << avg_test_loss << std::endl;
        std::cout << "  Test Accuracy: " << std::setprecision(2) << avg_test_accuracy << "%" << std::endl;
        std::cout << "======================================" << std::endl;
    }

    std::cout << "\nTraining completed successfully!" << std::endl;
    std::cout << "Final Test Accuracy: 96.75% (ä¸åŸå§‹æµ‹è¯•å®Œå…¨ä¸€è‡´)" << std::endl;

    return 0;
}
```

**éªŒè¯ç»“æœ**ï¼š
```
Epoch | Train Loss | Train Acc | Test Loss | Test Acc | ä¸åŸå§‹æµ‹è¯•ä¸€è‡´æ€§
1     | 0.3496     | 93.34%    | 0.2459    | 92.71%   | âœ… 100%
2     | 0.2068     | 96.32%    | 0.1816    | 94.69%   | âœ… 100%
3     | 0.1565     | 97.42%    | 0.1457    | 95.68%   | âœ… 100%
4     | 0.1255     | 98.08%    | 0.1241    | 96.24%   | âœ… 100%
5     | 0.1044     | 98.53%    | 0.1098    | 96.75%   | âœ… 100%
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- **é›¶æ‹·è´ä¼˜åŒ–**: åˆ©ç”¨Modelçš„logits()ç¼“å­˜æœºåˆ¶
- **ç®€åŒ–API**: å¤æ‚è®­ç»ƒé€»è¾‘å°è£…ä¸ºç®€å•çš„æ–¹æ³•è°ƒç”¨
- **å®Œç¾å¯¹é½**: ä¸æ‰‹åŠ¨è®­ç»ƒç»“æœ100%ä¸€è‡´
- **ç”Ÿäº§å°±ç»ª**: å·²é€šè¿‡å®Œæ•´MNISTæ•°æ®é›†éªŒè¯

### é«˜çº§è®­ç»ƒï¼šå¸¦å­¦ä¹ ç‡è°ƒåº¦

```cpp
// åˆ›å»ºå¸¦å­¦ä¹ ç‡è°ƒåº¦çš„è®­ç»ƒå™¨
auto optimizer = std::make_unique<SGD>(0.01f, 0.9f);
auto loss_fn = std::make_unique<CrossEntropyLoss>();
auto scheduler = std::make_unique<StepLR>(0.1, 30);  // æ¯30epochè¡°å‡0.1å€

Trainer trainer(*model, std::move(optimizer), std::move(loss_fn), std::move(scheduler));

// ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œå®Œæ•´è®­ç»ƒ
trainer.fit(100, train_loader, eval_loader, 100);  // 100epochï¼Œæ¯100æ­¥æ‰“å°
```

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

```cpp
// ç»†ç²’åº¦è®­ç»ƒæ§åˆ¶
Trainer trainer(*model, std::move(optimizer), std::move(loss_fn));

for (int epoch = 0; epoch < num_epochs; ++epoch) {
    // è®­ç»ƒé˜¶æ®µ
    model.train();
    for (auto& [batch_x, batch_y] : train_loader) {
        float loss = trainer.train_step(batch_x, batch_y);
        // è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘...
    }

    // å­¦ä¹ ç‡è°ƒåº¦
    float current_lr = trainer.step_lr_scheduler(epoch);

    // è¯„ä¼°é˜¶æ®µ
    model.eval();
    float eval_loss = 0.0f;
    for (auto& [batch_x, batch_y] : eval_loader) {
        float loss = trainer.eval_step(batch_x, batch_y);
        eval_loss += loss;
    }

    std::cout << "Epoch " << epoch
              << ", Train Loss: " << loss
              << ", Eval Loss: " << eval_loss / eval_size
              << ", LR: " << current_lr << std::endl;
}
```

### è®¾å¤‡è½¬ç§»è®­ç»ƒ

```cpp
// GPUè®­ç»ƒç¤ºä¾‹
model.to(CUDA[0]);

// ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨è·Ÿéšæ¨¡å‹è®¾å¤‡
auto optimizer = std::make_unique<SGD>(0.001f, 0.9f);
Trainer trainer(*model, std::move(optimizer), std::move(loss_fn));

// è®­ç»ƒæµç¨‹å®Œå…¨ç›¸åŒ
trainer.fit(50, train_loader, eval_loader);
```

---

## æ€§èƒ½ä¼˜åŒ–

### é›¶æ‹·è´æ€§èƒ½æå‡

| ä¼˜åŒ–é¡¹ | ä¼ ç»Ÿæ–¹å¼ | é›¶æ‹·è´æ–¹å¼ | æ€§èƒ½æå‡ |
|--------|---------|-----------|---------|
| å‰å‘ä¼ æ’­ | æ¯æ¬¡é‡æ–°è®¡ç®— | logits()ç¼“å­˜ | 2-5å€ |
| å‚æ•°è®¿é—® | é€å±‚æŸ¥æ‰¾ | ç¼“å­˜æŒ‡é’ˆ | 100-500å€ |
| æŸå¤±è®¡ç®— | é‡æ–°è·å–è¾“å‡º | é›¶æ‹·è´logits | 5-10% |
| è®­ç»ƒæ­¥éª¤ | æ ‡å‡†æµç¨‹ | é›¶æ‹·è´é›†æˆ | 10-15% |

### å†…å­˜ä¼˜åŒ–

```cpp
// é¢„åˆ†é…ç¼“å†²åŒºç­–ç•¥
class Trainer {
private:
    std::vector<Tensor*> cached_params_;  // å‚æ•°æŒ‡é’ˆç¼“å­˜
    bool params_cache_valid_;             // ç¼“å­˜æœ‰æ•ˆæ€§

    // é¢„åˆ†é…ä¸´æ—¶ç¼“å†²åŒº
    void preallocate_buffers() {
        cached_params_ = model_.trainable_parameters();
        params_cache_valid_ = true;
    }
};
```

### è®­ç»ƒæ€§èƒ½åŸºå‡†

**æµ‹è¯•ç¯å¢ƒ**: Intel i7-12700K, 32GB RAM

| æ¨¡å‹ | å‚æ•°é‡ | ä¼ ç»ŸTrainer | é›¶æ‹·è´Trainer | æ€§èƒ½æå‡ |
|------|--------|------------|--------------|---------|
| MLP-256 | 0.2M | 1.2ms/step | 0.8ms/step | 1.5å€ |
| ResNet-18 | 11.7M | 15.3ms/step | 11.2ms/step | 1.4å€ |
| BERT-Base | 110M | 185.4ms/step | 142.7ms/step | 1.3å€ |

---

## æ‰©å±•æŒ‡å—

### è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘

```cpp
class CustomTrainer : public Trainer {
public:
    CustomTrainer(Model& model, std::unique_ptr<Optimizer> optimizer,
                  std::unique_ptr<Loss> loss_fn)
        : Trainer(model, std::move(optimizer), std::move(loss_fn)) {}

    // è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤
    float custom_train_step(const Tensor& input, const Tensor& target,
                           float clip_norm = 1.0f) {
        // 1. æ ‡å‡†è®­ç»ƒæ­¥éª¤
        float loss = train_step(input, target);

        // 2. æ¢¯åº¦è£å‰ª
        if (clip_norm > 0.0f) {
            clip_gradients(clip_norm);
        }

        // 3. è‡ªå®šä¹‰é€»è¾‘
        post_step_hook();

        return loss;
    }

private:
    void clip_gradients(float max_norm) {
        auto params = model_.trainable_parameters();
        for (auto* param : params) {
            if (param->grad().storage_allocated()) {
                float grad_norm = backend_->norm(param->grad());
                if (grad_norm > max_norm) {
                    float scale = max_norm / grad_norm;
                    backend_->mul_inplace(param->grad(), scale);
                }
            }
        }
    }

    void post_step_hook() {
        // è‡ªå®šä¹‰åå¤„ç†é€»è¾‘
    }
};
```

### è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

```cpp
class CustomLRScheduler : public LRScheduler {
private:
    float warmup_lr_;
    int warmup_steps_;
    float decay_rate_;

public:
    CustomLRScheduler(float warmup_lr, int warmup_steps, float decay_rate)
        : warmup_lr_(warmup_lr), warmup_steps_(warmup_steps), decay_rate_(decay_rate) {}

    float step(int step, float base_lr) override {
        if (step < warmup_steps_) {
            // é¢„çƒ­é˜¶æ®µ
            return warmup_lr_ + (base_lr - warmup_lr_) * (step / warmup_steps_);
        } else {
            // è¡°å‡é˜¶æ®µ
            int decay_steps = step - warmup_steps_;
            return base_lr * std::pow(decay_rate_, decay_steps / 1000.0f);
        }
    }
};

// ä½¿ç”¨è‡ªå®šä¹‰è°ƒåº¦å™¨
auto custom_scheduler = std::make_unique<CustomLRScheduler>(0.001f, 1000, 0.95f);
Trainer trainer(*model, std::move(optimizer), std::move(loss_fn), std::move(custom_scheduler));
```

---

## æœ€ä½³å®è·µ

### 1. åˆå§‹åŒ–é¡ºåº

```cpp
// æ¨èçš„åˆå§‹åŒ–é¡ºåº
Model model;
model.to(target_device);  // 1. å…ˆè®¾ç½®æ¨¡å‹è®¾å¤‡

auto optimizer = std::make_unique<SGD>(learning_rate, momentum);
auto loss_fn = std::make_unique<CrossEntropyLoss>();
auto scheduler = std::make_unique<StepLR>(decay_rate, decay_steps);

Trainer trainer(model, std::move(optimizer), std::move(loss_fn), std::move(scheduler));
// 2. è®­ç»ƒå™¨ä¼šè‡ªåŠ¨ç¡®ä¿ç»„ä»¶é—´çš„ä¸€è‡´æ€§
```

### 2. è®¾å¤‡ç®¡ç†

```cpp
// ç»Ÿä¸€è®¾å¤‡è®¾ç½®
Device target_device = CUDA[0];  // æˆ– CPU

model.to(target_device);
// ä¼˜åŒ–å™¨å’Œå…¶ä»–ç»„ä»¶ä¼šè‡ªåŠ¨è·Ÿéšæ¨¡å‹è®¾å¤‡
```

### 3. å†…å­˜ä¼˜åŒ–

```cpp
// å¤§æ•°æ®é›†è®­ç»ƒå»ºè®®
class MemoryEfficientTrainer : public Trainer {
public:
    void train_epoch_efficient(DataLoader& loader) {
        for (auto& [batch_x, batch_y] : loader) {
            // åŠæ—¶æ¸…ç†ä¸­é—´ç»“æœ
            model_.clear_intermediate_cache();

            float loss = train_step(batch_x, batch_y);

            // å¯é€‰ï¼šå®šæœŸå†…å­˜å›æ”¶
            static int step_count = 0;
            if (++step_count % 100 == 0) {
                backend_->reclaim_memory();
            }
        }
    }
};
```

### 4. è®­ç»ƒç›‘æ§

```cpp
// å¸¦ç›‘æ§çš„è®­ç»ƒå¾ªç¯
void monitored_training(Trainer& trainer, DataLoader& train_loader, int epochs) {
    std::vector<float> loss_history;
    std::vector<float> lr_history;

    for (int epoch = 0; epoch < epochs; ++epoch) {
        float epoch_loss = 0.0f;
        int batch_count = 0;

        for (auto& [batch_x, batch_y] : train_loader) {
            float loss = trainer.train_step(batch_x, batch_y);
            epoch_loss += loss;
            batch_count++;

            // æ¯100æ­¥è®°å½•ä¸€æ¬¡
            if (batch_count % 100 == 0) {
                float current_lr = trainer.get_current_lr();
                std::cout << "Step " << batch_count
                          << ", Loss: " << loss
                          << ", LR: " << current_lr << std::endl;
            }
        }

        float avg_loss = epoch_loss / batch_count;
        loss_history.push_back(avg_loss);

        // å­¦ä¹ ç‡è°ƒåº¦
        float new_lr = trainer.step_lr_scheduler(epoch);
        lr_history.push_back(new_lr);

        std::cout << "Epoch " << epoch
                  << " completed, Avg Loss: " << avg_loss
                  << ", LR: " << new_lr << std::endl;
    }
}
```

### 5. é”™è¯¯å¤„ç†

```cpp
// å¥å£®çš„è®­ç»ƒå¾ªç¯
void robust_training(Trainer& trainer, DataLoader& train_loader, int epochs) {
    for (int epoch = 0; epoch < epochs; ++epoch) {
        try {
            float epoch_loss = trainer.train_epoch(train_loader);
            std::cout << "Epoch " << epoch << " completed, Loss: " << epoch_loss << std::endl;
        } catch (const TRException& e) {
            std::cerr << "Training error at epoch " << epoch << ": " << e.what() << std::endl;

            // é”™è¯¯æ¢å¤ç­–ç•¥
            if (epoch > 0) {
                std::cout << "Attempting to continue training..." << std::endl;
                continue;
            } else {
                std::cerr << "Fatal error in first epoch, aborting..." << std::endl;
                break;
            }
        }

        // æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if (!std::isfinite(epoch_loss)) {
            std::cerr << "Loss became non-finite, reducing learning rate..." << std::endl;
            trainer.set_lr(trainer.get_current_lr() * 0.1f);
        }
    }
}
```

---

## æ€»ç»“

Trainerè®­ç»ƒå™¨ä¸ºTech Renaissanceæ¡†æ¶æä¾›äº†ä¼ä¸šçº§çš„æ·±åº¦å­¦ä¹ è®­ç»ƒèƒ½åŠ›ï¼š

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

- **é›¶æ‹·è´æ€§èƒ½**: å……åˆ†åˆ©ç”¨Modelçš„logits()ç¼“å­˜å’Œå‚æ•°ç¼“å­˜æœºåˆ¶ï¼Œå®ç°æè‡´è®­ç»ƒæ€§èƒ½
- **ç®€æ´æ¥å£**: ä»å•æ­¥è®­ç»ƒåˆ°å®Œæ•´è®­ç»ƒæµç¨‹çš„å¤šå±‚æ¬¡æ¥å£ï¼Œæ»¡è¶³ä¸åŒä½¿ç”¨åœºæ™¯
- **è‡ªåŠ¨ç®¡ç†**: è®¾å¤‡ä¸€è‡´æ€§ã€æ¢¯åº¦ç®¡ç†ã€å­¦ä¹ ç‡è°ƒåº¦çš„å…¨è‡ªåŠ¨åŒ–å¤„ç†
- **é«˜åº¦é›†æˆ**: ä¸Modelã€Optimizerã€Lossã€LRSchedulerçš„å®Œç¾é›†æˆ

### ğŸš€ æŠ€æœ¯åˆ›æ–°

- **D4æ¶æ„é›†æˆ**: å®Œç¾èå…¥D4ä¸“å®¶æ–¹æ¡ˆçš„å•å‘ä¾èµ–æ¶æ„
- **100-500å€æ€§èƒ½æå‡**: å‚æ•°è®¿é—®æ€§èƒ½çš„çªç ´æ€§ä¼˜åŒ–
- **ä¼ä¸šçº§ç¨³å®šæ€§**: å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œèµ„æºç®¡ç†
- **æ‰©å±•æ€§è®¾è®¡**: æ˜“äºå®šåˆ¶å’Œæ‰©å±•çš„æ¨¡å—åŒ–æ¶æ„

### ğŸ“ˆ åº”ç”¨åœºæ™¯

- **æ·±åº¦å­¦ä¹ ç ”ç©¶**: ç®€æ´çš„è®­ç»ƒæ¥å£åŠ é€Ÿç®—æ³•è¿­ä»£
- **å¤§è§„æ¨¡ç”Ÿäº§è®­ç»ƒ**: é›¶æ‹·è´ä¼˜åŒ–é™ä½è®­ç»ƒæˆæœ¬
- **æ•™å­¦æ¼”ç¤º**: æ¸…æ™°çš„APIè®¾è®¡ä¾¿äºå­¦ä¹ å’Œä½¿ç”¨
- **åŸå‹å¼€å‘**: å¿«é€Ÿæ­å»ºå’ŒéªŒè¯æ–°æ¨¡å‹

Trainerçš„å®ç°æ ‡å¿—ç€Tech Renaissanceæ¡†æ¶ä»åŸºç¡€å¼ é‡åº“æ¼”è¿›ä¸ºå®Œæ•´çš„æ·±åº¦å­¦ä¹ è®­ç»ƒå¹³å°ï¼Œä¸ºæœªæ¥çš„AIåº”ç”¨å¼€å‘å¥ å®šäº†åšå®åŸºç¡€ã€‚