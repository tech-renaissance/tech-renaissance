# CudaBackend API 文档

## 概述

`CudaBackend`是技术觉醒框架的GPU计算后端实现，继承自`Backend`基类。它基于NVIDIA CUDA平台，结合cuBLAS和cuDNN库提供高性能的GPU加速计算能力，支持深度学习工作负载的大规模并行计算。

**版本**: V1.23.1
**更新日期**: 2025-10-30
**作者**: 技术觉醒团队

## 设计理念

### 核心设计原则

1. **列主序存储**：CUDA后端使用**列主序（Column-major）**存储张量数据，与cuBLAS/cuDNN库接口保持一致
2. **高性能计算**：基于cuBLAS的优化矩阵运算，GPU性能接近硬件极限
3. **透明转换**：通过`from_cpu()`和`to_cpu()`方法自动处理行主序与列主序之间的格式转换
4. **异步计算**：使用CUDA流实现异步操作，提高并发性能
5. **RAII管理**：智能指针自动内存管理，防止GPU内存泄漏

### 关键架构特性

#### **后端管理存储原则（V1.23.1核心特性）**

CUDA后端遵循"后端管理存储"的设计原则：
- **CPU后端**：使用行主序（Row-major）存储张量数据
- **CUDA后端**：使用列主序（Column-major）存储张量数据
- **转换层透明**：用户无需关心底层存储格式，`from_cpu()`和`to_cpu()`自动处理转换

#### **内存布局转换层**

```cpp
// CPU → CUDA 转换：行主序 → 列主序
Tensor CudaBackend::from_cpu(const Tensor& tensor) {
    // 1. 创建CUDA Storage（列主序存储）
    Tensor cuda_tensor = Tensor::empty(tensor.shape(), tensor.dtype(), tr::CUDA[device_id_]);

    // 2. 对于2D矩阵，执行内存布局转换
    if (tensor.shape().ndim() == 2) {
        int32_t M = tensor.shape().height();  // 行数
        int32_t N = tensor.shape().width();   // 列数

        const float* cpu_data = static_cast<const float*>(tensor.data_ptr());
        float* cuda_data = static_cast<float*>(cuda_tensor.data_ptr());

        // 行主序 → 列主序转换
        for (int32_t i = 0; i < M; ++i) {
            for (int32_t j = 0; j < N; ++j) {
                cuda_data[j * M + i] = cpu_data[i * N + j];
            }
        }
    } else {
        // 非2D张量直接复制
        copy(cuda_tensor.data_ptr(), tensor.data_ptr(),
             tensor.memory_size(), tr::CUDA[device_id_], tr::CPU);
    }

    return cuda_tensor;
}

// CUDA → CPU 转换：列主序 → 行主序
Tensor CudaBackend::to_cpu(const Tensor& tensor) {
    // 1. 创建CPU Storage（行主序存储）
    Tensor cpu_tensor = Tensor::empty(tensor.shape(), tensor.dtype(), tr::CPU);

    // 2. 对于2D矩阵，执行内存布局转换
    if (tensor.shape().ndim() == 2) {
        int32_t M = tensor.shape().height();  // 行数
        int32_t N = tensor.shape().width();   // 列数

        const float* cuda_data = static_cast<const float*>(tensor.data_ptr());
        float* cpu_data = static_cast<float*>(cpu_tensor.data_ptr());

        // 列主序 → 行主序转换
        for (int32_t i = 0; i < M; ++i) {
            for (int32_t j = 0; j < N; ++j) {
                cpu_data[i * N + j] = cuda_data[j * M + i];
            }
        }
    } else {
        // 非2D张量直接复制
        copy(cpu_tensor.data_ptr(), tensor.data_ptr(),
             tensor.memory_size(), tr::CPU, tensor.device());
    }

    return cpu_tensor;
}
```

## 头文件

```cpp
#include "tech_renaissance/backend/cuda_backend.h"
```

## 编译要求

- **CUDA Toolkit**：12.0或更高版本
- **cuBLAS**：CUDA基础线性代数子程序库
- **cuDNN**：CUDA深度神经网络库
- **编译宏**：需要定义`TR_USE_CUDA`宏
- **GPU硬件**：支持CUDA架构的NVIDIA GPU

## 主要特性

- **列主序存储**：使用列主序存储格式，与cuBLAS/cuDNN库接口一致
- **高性能矩阵运算**：基于cuBLAS的优化矩阵乘法，性能达到6673+ GFLOPS
- **自动格式转换**：透明处理CPU行主序与CUDA列主序之间的转换
- **多GPU支持**：支持多个CUDA设备，每个设备独立后端实例
- **异步计算**：使用CUDA流实现异步操作
- **RAII内存管理**：智能指针自动GPU内存管理

## 核心API

### 构造函数

#### `CudaBackend(int device_id)`

构造指定CUDA设备的后端实例。

**参数**：
- `device_id` - CUDA设备ID，范围0-7

**异常**：
- `TRException` - 当device_id无效或CUDA初始化失败时抛出

**示例**：
```cpp
auto cuda_backend = BackendManager::get_cuda_backend(0);  // 使用GPU 0
```

### 跨后端转换接口（V1.23.1核心特性）

#### `Tensor from_cpu(const Tensor& tensor) override`

将CPU张量转换为CUDA张量，自动处理行主序到列主序的格式转换。

**参数**：
- `tensor` - CPU张量（行主序存储）

**返回值**：
- `Tensor` - CUDA张量（列主序存储）

**关键特性**：
- **2D矩阵转换**：执行行主序到列主序的转置操作
- **非2D张量**：直接复制，不进行格式转换
- **内存分配**：自动在GPU上分配列主序存储空间

**转换示例**：
```cpp
// CPU张量（行主序）：A[M,K] = [[1, 2, 3], [4, 5, 6]]
// 内存布局：[1, 2, 3, 4, 5, 6]

Tensor cpu_tensor = Tensor::randn(Shape(2, 3), 42, DType::FP32, tr::CPU);
Tensor cuda_tensor = cuda_backend->from_cpu(cpu_tensor);

// CUDA张量（列主序）：A^T[K,M] = [[1, 4], [2, 5], [3, 6]]
// 内存布局：[1, 4, 2, 5, 3, 6]
```

#### `Tensor to_cpu(const Tensor& tensor) override`

将CUDA张量转换为CPU张量，自动处理列主序到行主序的格式转换。

**参数**：
- `tensor` - CUDA张量（列主序存储）

**返回值**：
- `Tensor` - CPU张量（行主序存储）

**关键特性**：
- **2D矩阵转换**：执行列主序到行主序的转置操作
- **非2D张量**：直接复制，不进行格式转换
- **内存分配**：自动在CPU上分配行主序存储空间

#### `Tensor to(const Tensor& tensor, const Device& target_device) override`

通用设备转换接口，支持CUDA到其他设备的转换。

**参数**：
- `tensor` - 源张量
- `target_device` - 目标设备

**返回值**：
- `Tensor` - 目标设备上的张量

### 内存管理接口

#### `std::shared_ptr<void> allocate(size_t size) override`

在GPU显存中分配指定大小的内存块。

**参数**：
- `size` - 要分配的内存大小（字节）

**返回值**：
- `std::shared_ptr<void>` - GPU内存块的智能指针，自动管理生命周期

**实现特点**：
- 使用`cudaMalloc()`分配GPU内存
- 智能指针析构时自动调用`cudaFree()`
- GPU内存按后端特定格式（列主序）组织

#### `void copy(void* dst, const void* src, size_t size, const Device& dst_device, const Device& src_device) override`

在GPU内存或GPU与其他设备间复制数据。

**支持的复制方向**：
- CUDA → CUDA：设备间复制，使用`cudaMemcpyDeviceToDevice`
- CPU → CUDA：主机到设备，使用`cudaMemcpyHostToDevice`
- CUDA → CPU：设备到主机，使用`cudaMemcpyDeviceToHost`

**注意**：此方法只复制数据，不进行格式转换。格式转换由`from_cpu()`和`to_cpu()`处理。

### 矩阵乘法操作（V1.23.1核心实现）

#### `void mm(Tensor& result, const Tensor& a, const Tensor& b) override`

执行GPU张量的矩阵乘法运算：result = a × b。使用标准的cuBLAS实现，配合列主序存储格式。

**参数**：
- `result` - 结果GPU张量（列主序存储）
- `a` - 第一个操作数GPU张量（列主序存储）
- `b` - 第二个操作数GPU张量（列主序存储）

**异常**：
- `TRException` - 当张量形状、数据类型或设备不匹配时抛出

**关键实现**：
```cpp
void CudaBackend::mm(Tensor& result, const Tensor& a, const Tensor& b) {
    // CUDA张量使用列主序存储
    const float* a_data = static_cast<const float*>(a.data_ptr());
    const float* b_data = static_cast<const float*>(b.data_ptr());
    float* result_data = static_cast<float*>(result.data_ptr());

    int32_t M = a.height();  // 行数
    int32_t K = a.width();   // 列数
    int32_t N = b.width();   // B的列数

    // cuBLAS标准的列主序矩阵乘法：C[M,N] = A[M,K] × B[K,N]
    const float alpha = 1.0f;
    const float beta = 0.0f;

    CUBLAS_CHECK(cublasSgemm(
        cublas_handle_,
        CUBLAS_OP_N, CUBLAS_OP_N,  // 不转置
        N, M, K,                   // 结果维度
        &alpha,
        b_data, N,                 // B矩阵，leading dimension = N
        a_data, K,                 // A矩阵，leading dimension = K
        &beta,
        result_data, N             // 结果矩阵，leading dimension = N
    ));
}
```

**性能特征**：
- **实测性能**：6673.76 GFLOPS（1024×2048 × 2048×512矩阵）
- **执行时间**：平均0.3218毫秒
- **精度**：平均相对误差 4.590400e-07
- **效率**：52.4%的理论峰值（RTX 3060级别）

### 张量操作

#### `void fill(Tensor& dst, float value) override`

用浮点数值填充GPU张量（FP32类型）。

**参数**：
- `dst` - 目标GPU张量，必须是FP32类型
- `value` - 填充值

**实现特点**：
- 对于0值，使用高效的`cudaMemset()`
- 对于非0值，创建CPU临时数组并复制到GPU
- 在列主序存储上进行填充

#### `void fill(Tensor& dst, int8_t value) override`

用8位整数值填充GPU张量（INT8类型）。

**参数**：
- `dst` - 目标GPU张量，必须是INT8类型
- `value` - 填充值

**实现特点**：
- 使用`cudaMemset()`进行高效字节级填充

#### `void add(Tensor& result, const Tensor& a, const Tensor& b) override`

执行GPU张量的逐元素加法运算：result = a + b。

**参数**：
- `result` - 结果GPU张量
- `a` - 第一个操作数GPU张量
- `b` - 第二个操作数GPU张量

**实现特点**：
- 使用cuBLAS的`cublasSaxpy()`函数实现高效加法
- 在列主序存储上执行逐元素操作

#### `void mul(Tensor& result, const Tensor& a, const Tensor& b) override`

执行GPU张量的逐元素乘法运算：result = a * b。

**参数**：
- `result` - 结果GPU张量
- `a` - 第一个操作数GPU张量
- `b` - 第二个操作数GPU张量

**实现特点**：
- 使用cuDNN的`cudnnOpTensor()`函数实现高效逐元素乘法
- 支持4D张量的NCHW格式

### 设备信息

#### `Device device() const override`

获取CUDA后端的设备信息。

**返回值**：
- `Device` - 返回对应CUDA设备的Device对象（如`tr::CUDA[0]`）

#### `std::string name() const override`

获取CUDA后端的名称。

**返回值**：
- `std::string` - 返回"CudaBackend"

## 使用示例

### 跨后端矩阵乘法（V1.23.1推荐用法）

```cpp
#include "tech_renaissance.h"
using namespace tr;

void cross_backend_matrix_multiplication() {
    try {
        // 获取后端实例（V1.23.1新API）
        auto cuda_backend = BackendManager::get_cuda_backend();
        auto cpu_backend = BackendManager::get_cpu_backend();

        // 创建CPU随机张量（行主序存储）
        Tensor cpu_a = Tensor::randn(Shape(1024, 2048), 42);
        Tensor cpu_b = Tensor::randn(Shape(2048, 512), 42);

        // 转换到CUDA（自动转换为列主序）
        Tensor cuda_a = cuda_backend->from_cpu(cpu_a);  // 行主序 → 列主序
        Tensor cuda_b = cuda_backend->from_cpu(cpu_b);

        // 创建CUDA结果张量
        Tensor cuda_result = Tensor::empty(Shape(1024, 512), DType::FP32, tr::CUDA[0]);

        // CUDA矩阵乘法（列主序计算）
        cuda_backend->mm(cuda_result, cuda_a, cuda_b);

        // 转换回CPU（自动转换回行主序）
        Tensor cpu_result = cuda_backend->to_cpu(cuda_result);  // 列主序 → 行主序

        // 结果验证：CPU和CUDA结果在行主序下应该一致
        bool is_close = cpu_backend->is_close(cpu_a_result, cpu_b_result, 1e-4f);
        std::cout << "Results are close: " << (is_close ? "YES" : "NO") << std::endl;

    } catch (const TRException& e) {
        std::cerr << "CUDA error: " << e.what() << std::endl;
    }
}
```

### 内存布局转换示例

```cpp
void memory_layout_conversion_example() {
    auto cuda_backend = BackendManager::get_cuda_backend();

    // 创建CPU张量（行主序）
    Tensor cpu_matrix = Tensor::full(Shape(2, 3), 0.0f);
    float* cpu_data = static_cast<float*>(cpu_matrix.data_ptr());

    // 手动设置数据以便观察转换
    // 矩阵：[[1, 2, 3], [4, 5, 6]]
    // 行主序内存：[1, 2, 3, 4, 5, 6]
    cpu_data[0] = 1.0f; cpu_data[1] = 2.0f; cpu_data[2] = 3.0f;
    cpu_data[3] = 4.0f; cpu_data[4] = 5.0f; cpu_data[5] = 6.0f;

    // 转换到CUDA（自动转置为列主序）
    Tensor cuda_matrix = cuda_backend->from_cpu(cpu_matrix);
    const float* cuda_data = static_cast<const float*>(cuda_matrix.data_ptr());

    // 列主序内存布局：[1, 4, 2, 5, 3, 6]
    // 对应转置矩阵：[[1, 4], [2, 5], [3, 6]]

    // 转换回CPU（验证转换的正确性）
    Tensor cpu_result = cuda_backend->to_cpu(cuda_matrix);
    // cpu_result的内存布局应该与原始cpu_matrix一致
}
```

### 基本GPU张量操作

```cpp
void basic_gpu_operations() {
    try {
        auto cuda_backend = BackendManager::get_cuda_backend();

        // 创建GPU张量（列主序存储）
        Shape shape(3, 4);
        Tensor a(shape, DType::FP32, tr::CUDA[0]);
        Tensor b(shape, DType::FP32, tr::CUDA[0]);
        Tensor result(shape, DType::FP32, tr::CUDA[0]);

        // 填充张量
        cuda_backend->fill(a, 1.5f);
        cuda_backend->fill(b, 2.5f);

        // 执行加法
        cuda_backend->add(result, a, b);

        std::cout << "GPU computation completed successfully!" << std::endl;

    } catch (const TRException& e) {
        std::cerr << "CUDA error: " << e.what() << std::endl;
    }
}
```

### 高性能矩阵乘法基准测试

```cpp
void performance_benchmark() {
    auto cuda_backend = BackendManager::get_cuda_backend();
    auto cpu_backend = BackendManager::get_cpu_backend();

    // 创建测试矩阵
    const int32_t M = 1024, K = 2048, N = 512;
    Tensor cpu_a = Tensor::randn(Shape(M, K), 42);
    Tensor cpu_b = Tensor::randn(Shape(K, N), 42);

    // 转换到CUDA
    Tensor cuda_a = cuda_backend->from_cpu(cpu_a);
    Tensor cuda_b = cuda_backend->from_cpu(cpu_b);
    Tensor cuda_result = Tensor::empty(Shape(M, N), DType::FP32, tr::CUDA[0]);

    // 预热
    for (int i = 0; i < 10; ++i) {
        cuda_backend->mm(cuda_result, cuda_a, cuda_b);
    }

    // 性能测试
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < iterations; ++i) {
        cuda_backend->mm(cuda_result, cuda_a, cuda_b);
    }

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

    // 计算GFLOPS
    double flops = 2.0 * M * K * N;  // 矩阵乘法的浮点运算次数
    double avg_time_ms = duration.count() / 1000.0 / iterations;
    double gflops = flops / (avg_time_ms * 1e6);

    std::cout << "CUDA GEMM Performance:" << std::endl;
    std::cout << "  Matrix size: " << M << "x" << K << " x " << K << "x" << N << std::endl;
    std::cout << "  Average time: " << avg_time_ms << " ms" << std::endl;
    std::cout << "  GFLOPS: " << gflops << std::endl;
}
```

## 编译配置

### CMake配置
```cmake
option(TR_ENABLE_CUDA "Enable CUDA backend" ON)

if(TR_ENABLE_CUDA AND CUDAToolkit_FOUND)
    # 添加CUDA后端目标
    target_compile_definitions(cuda_backend PUBLIC TR_USE_CUDA)
    target_link_libraries(cuda_backend PUBLIC
        CUDA::cudart
        CUDA::cublas
        cudnn_static
    )
endif()
```

### 条件编译
```cpp
#ifdef TR_USE_CUDA
    // CUDA相关代码
    auto cuda_backend = BackendManager::get_cuda_backend();
#endif
```

## 性能特征（V1.23.1实测）

### 矩阵乘法性能
- **测试矩阵**：A(1024×2048) × B(2048×512) = C(1024×512)
- **实测性能**：6673.76 GFLOPS
- **平均执行时间**：0.3218毫秒
- **理论峰值效率**：52.4%（RTX 3060级别）
- **精度**：平均相对误差 4.590400e-07

### 内存布局转换开销
- **2D矩阵转换**：O(M×N)转置开销，一次转换约0.05ms
- **非2D张量**：直接复制，O(N)开销
- **转换优化**：临时缓冲区避免多次内存分配

### 性能优化建议

#### 内存访问优化
- 使用列主序存储格式，与cuBLAS接口完全匹配
- 避免频繁的CPU-GPU数据传输
- 使用异步传输和计算重叠

#### 矩阵乘法优化
- 确保输入矩阵已经是列主序格式
- 使用`from_cpu()`进行一次性转换，避免重复转置
- 大矩阵运算可获得更好的GPU利用率

#### 设备选择
- 根据矩阵大小选择合适的GPU设备
- 考虑GPU内存容量和计算能力
- 对于小型矩阵，CPU可能更高效

## 错误处理

### 常见CUDA错误

```cpp
try {
    auto backend = BackendManager::get_cuda_backend(99);  // 无效设备ID
} catch (const TRException& e) {
    std::cerr << "Error: " << e.what() << std::endl;
    // "Invalid CUDA device ID: 99"
}

try {
    Tensor tensor(Shape(2, 3), DType::FP32, tr::CPU);
    auto cuda_backend = BackendManager::get_cuda_backend();
    cuda_backend->fill(tensor, 3.14f);  // 设备不匹配
} catch (const TRException& e) {
    std::cerr << "Error: " << e.what() << std::endl;
    // "CudaBackend: tensor device mismatch"
}
```

### 内存布局错误
```cpp
// 错误示例：直接在CUDA张量上使用行主序访问模式
Tensor cuda_tensor = BackendManager::get_cuda_backend()->from_cpu(cpu_tensor);
float* data = static_cast<float*>(cuda_tensor.data_ptr());

// 错误：这样访问会得到列主序的数据
// float value = data[i * width + j];  // 错误！

// 正确：使用转换后的CPU张量或正确的列主序访问
Tensor cpu_result = BackendManager::get_cuda_backend()->to_cpu(cuda_tensor);
float correct_value = static_cast<float*>(cpu_result.data_ptr())[i * width + j];
```

### CUDA错误检查
CudaBackend使用宏定义进行全面的CUDA错误检查：
- `CUDA_CHECK()` - 检查CUDA运行时API错误
- `CUBLAS_CHECK()` - 检查cuBLAS库错误
- `CUDNN_CHECK()` - 检查cuDNN库错误

## 关键设计原则总结（V1.23.1）

### 后端管理存储
- **CUDA后端**：使用列主序存储，与cuBLAS/cuDNN库接口一致
- **CPU后端**：使用行主序存储，符合C/C++惯例
- **转换透明**：`from_cpu()`和`to_cpu()`自动处理格式转换

### 数据访问一致性
- **用户视角**：所有张量都是行主序访问，无论在哪个后端
- **后端内部**：各自选择最优的内存布局进行计算
- **转换保证**：跨后端转换保证数学结果的正确性

### 最佳实践
1. **使用转换方法**：始终使用`from_cpu()`和`to_cpu()`进行跨后端转换
2. **避免直接访问**：不要直接在不同后端间访问原始内存
3. **批量转换**：尽量一次性转换，避免频繁的小块转换
4. **性能监控**：监控转换开销，优化计算流程

## 注意事项

1. **设备一致性**：所有操作的张量必须位于同一CUDA设备上
2. **内存管理**：GPU内存有限，注意内存使用量
3. **异步操作**：CUDA操作是异步的，必要时需要同步
4. **线程安全**：单个CudaBackend实例不是线程安全的
5. **错误检查**：CUDA错误可能导致程序异常，需要妥善处理
6. **条件编译**：CUDA相关代码受`TR_USE_CUDA`宏控制
7. **内存布局**：CUDA张量使用列主序存储，访问时需注意格式差异

## 版本信息

- **版本**：V1.23.1
- **更新日期**：2025-10-30
- **作者**：技术觉醒团队
- **主要特性**：列主序存储、自动格式转换、cuBLAS矩阵乘法、高性能计算
- **依赖库**：CUDA Toolkit 12.0+、cuBLAS、cuDNN